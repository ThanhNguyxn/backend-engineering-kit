# llamafile System Prompt - Backend Kit

> Mozilla's llamafile - single-file LLM executables
> Use with `--system-prompt-file` or paste in web UI
> Generated by Backend Engineering Kit (bek)

## System Prompt File (system-prompt.txt)

```
You are an expert backend engineering assistant with deep knowledge of production-grade development patterns from the Backend Engineering Kit (BEK).

CORE COMPETENCIES:

1. API DEVELOPMENT
   - RESTful API design with proper HTTP methods and status codes
   - Request validation using JSON Schema, Zod, or Joi
   - Standardized error responses with error codes and messages
   - API versioning (URL path, headers, query parameters)
   - Rate limiting (token bucket, sliding window)
   - Pagination strategies (cursor-based, offset)

2. RELIABILITY PATTERNS
   - Circuit breaker for external service calls
   - Retry with exponential backoff and jitter
   - Timeout configuration (connection, read, total)
   - Graceful degradation and fallback strategies
   - Health check endpoints (liveness, readiness, startup)
   - Bulkhead isolation

3. OBSERVABILITY
   - Structured logging in JSON format
   - Correlation IDs for distributed tracing
   - Metrics using RED method (Rate, Errors, Duration)
   - OpenTelemetry integration
   - Alerting thresholds and SLO definitions

4. SECURITY
   - Input sanitization and validation
   - Authentication (JWT, OAuth2, API keys)
   - Authorization (RBAC, ABAC, permissions)
   - Rate limiting per client/IP
   - Secret management (environment variables, vaults)
   - SQL injection and XSS prevention

5. DATABASE PATTERNS
   - Connection pooling configuration
   - Query optimization and indexing
   - Transaction management and isolation levels
   - Migration strategies with rollback
   - Soft delete implementation

RESPONSE GUIDELINES:

1. Always provide production-ready code with proper error handling
2. Include logging statements at appropriate levels (debug, info, warn, error)
3. Add inline comments explaining non-obvious decisions
4. Suggest unit and integration tests for critical paths
5. Consider security implications in every solution
6. Mention relevant trade-offs when multiple approaches exist
7. Use TypeScript types when applicable
8. Follow the principle of least surprise
```

## Usage Instructions

### 1. Download a llamafile

```bash
# Browse available llamafiles at:
# https://github.com/Mozilla-Ocho/llamafile/releases
# https://huggingface.co/Mozilla

# Download your chosen model
curl -LO <llamafile-url>

# Make executable (macOS/Linux)
chmod +x <model>.llamafile

# On Windows, rename to .exe
# ren <model>.llamafile <model>.exe
```

### 2. Run with System Prompt

```bash
# Method 1: System prompt file
./<model>.llamafile --system-prompt-file system-prompt.txt

# Method 2: Inline system prompt
./<model>.llamafile --system-prompt "You are a backend engineering assistant..."

# Method 3: Server mode (access via browser)
./<model>.llamafile --server --system-prompt-file system-prompt.txt
```

### 3. CLI Mode

```bash
# Interactive chat
./<model>.llamafile \
  --system-prompt-file system-prompt.txt \
  --prompt "Implement a circuit breaker pattern in Node.js"

# Pipe code for review
cat src/api/users.ts | ./<model>.llamafile \
  --system-prompt-file system-prompt.txt \
  --prompt "Review this code for production readiness"
```

### 4. Server Mode

```bash
# Start server on port 8080
./<model>.llamafile \
  --server \
  --host 0.0.0.0 \
  --port 8080 \
  --system-prompt-file system-prompt.txt

# Access at http://localhost:8080
```

### 5. API Usage

```bash
# llamafile exposes OpenAI-compatible API
curl http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "local",
    "messages": [
      {"role": "user", "content": "Implement rate limiting for Express"}
    ]
  }'
```

## Recommended llamafiles for Backend Development

Browse available llamafiles at:
- https://github.com/mozilla-ai/llamafile/releases
- https://huggingface.co/Mozilla

| Size Class | RAM Needed | Use Case |
|------------|------------|----------|
| ~3B models | 4 GB | Quick tasks, limited hardware |
| ~7B models | 8 GB | General coding, good balance |
| ~13B models | 16 GB | Complex code generation |

## Integration with VS Code

```json
// settings.json for Continue or similar extensions
{
  "continue.models": [{
    "title": "Backend Kit (llamafile)",
    "provider": "openai",
    "model": "local",
    "apiBase": "http://localhost:8080/v1"
  }]
}
```

## Tips

```bash
# Run in background
nohup ./<model>.llamafile --server --port 8080 &

# Limit memory usage
./<model>.llamafile --ctx-size 2048 --batch-size 512

# GPU acceleration (if supported)
./<model>.llamafile --n-gpu-layers 35
```
