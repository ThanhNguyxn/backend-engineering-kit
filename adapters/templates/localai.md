# LocalAI Model Configuration - Backend Kit

> Place in LocalAI models directory
> Location: `models/backend-kit.yaml`
> Generated by Backend Engineering Kit (bek)

```yaml
# LocalAI Model Configuration for Backend Engineering Kit
# Documentation: https://localai.io/docs/getting-started/customize-model/

name: backend-kit
parameters:
  # Model to use as base (adjust based on your downloaded model)
  # Examples: llama3, mistral, codellama, deepseek-coder
  model: llama3

# System prompt for backend engineering context
system_prompt: |
  You are an expert backend engineering assistant with deep knowledge of production-grade development patterns from the Backend Engineering Kit (BEK).

  ## Your Capabilities

  ### API Development
  - RESTful API design with proper error handling
  - Request validation and sanitization
  - API versioning strategies
  - Rate limiting implementation

  ### Database Patterns
  - Connection pooling configuration
  - Query optimization
  - Transaction management
  - Migration strategies

  ### Reliability Engineering
  - Circuit breaker patterns
  - Retry with exponential backoff
  - Timeout configuration
  - Graceful degradation

  ### Observability
  - Structured logging (JSON format)
  - Distributed tracing with correlation IDs
  - Health check endpoints
  - Metrics collection (RED method)

  ### Security
  - Input sanitization
  - Authentication/Authorization patterns
  - Rate limiting
  - Secret management

  ## Response Guidelines

  1. Always provide production-ready code with error handling
  2. Include logging statements at appropriate levels
  3. Add inline comments explaining critical decisions
  4. Suggest relevant tests for the implementation
  5. Consider security implications in every solution

# Template configuration (optional)
template:
  chat: |
    {{.System}}
    
    {{range .Messages}}{{if eq .Role "user"}}User: {{.Content}}
    {{else if eq .Role "assistant"}}Assistant: {{.Content}}
    {{end}}{{end}}
    Assistant:

# Generation parameters
parameters:
  temperature: 0.7
  top_p: 0.9
  top_k: 40
  repeat_penalty: 1.1
  
# Context size (adjust based on your hardware)
context_size: 4096

# Stop sequences
stopwords:
  - "User:"
  - "\n\n\n"
```

## Setup Instructions

### 1. Install LocalAI

```bash
# Using Docker
docker run -p 8080:8080 --name local-ai -ti localai/localai:latest

# Or install locally
curl https://localai.io/install.sh | sh
```

### 2. Download a Model

```bash
# Download a model (check https://models.localai.io for options)
local-ai run <model-name>

# Examples:
local-ai run llama3
local-ai run mistral
local-ai run codellama
```

### 3. Use Backend Kit Configuration

```bash
# Copy the configuration
cp backend-kit.yaml ~/.local/share/localai/models/

# Or mount in Docker
docker run -p 8080:8080 -v $(pwd)/models:/models localai/localai:latest
```

### 4. Query the Model

```bash
# Using curl
curl http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "backend-kit",
    "messages": [
      {"role": "user", "content": "Implement a circuit breaker pattern in Node.js"}
    ]
  }'

# Using OpenAI-compatible client
from openai import OpenAI

client = OpenAI(base_url="http://localhost:8080/v1", api_key="not-needed")
response = client.chat.completions.create(
    model="backend-kit",
    messages=[{"role": "user", "content": "Implement rate limiting for my API"}]
)
```

## Integration with VS Code

LocalAI can be used with Continue, Cody, or any OpenAI-compatible extension:

```json
{
  "continue.models": [{
    "title": "Backend Kit (LocalAI)",
    "provider": "openai",
    "model": "backend-kit",
    "apiBase": "http://localhost:8080/v1"
  }]
}
```
