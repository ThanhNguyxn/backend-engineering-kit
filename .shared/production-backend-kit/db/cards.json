[
  {
    "id": "api-error-model",
    "title": "API Error Model (RFC 7807)",
    "tags": [
      "api",
      "error-handling",
      "rest",
      "http",
      "rfc7807"
    ],
    "level": "beginner",
    "stacks": [
      "all"
    ],
    "scope": "api",
    "maturity": "stable",
    "works_with": [
      "all"
    ],
    "path": "D:/Code/backend-engineering-kit/.shared/production-backend-kit/patterns/api.error-model.md",
    "type": "pattern",
    "sections": [],
    "checklist": [],
    "sources": [],
    "rawContent": "\r\n# API Error Model (RFC 7807)\r\n\r\n## Problem\r\n\r\nAPIs return errors in inconsistent formats across services, making client-side error handling fragile, debugging difficult, and documentation incomplete. Teams waste time parsing different error shapes and writing custom error handlers.\r\n\r\n## When to use\r\n\r\n- **Always** for REST/HTTP APIs (public or internal)\r\n- GraphQL APIs (adapt to GraphQL errors extension)\r\n- When multiple teams/clients consume your API\r\n- Any service that needs consistent error contracts\r\n\r\n## Solution: RFC 7807 Problem Details\r\n\r\nUse the **RFC 7807 Problem Details** standard - widely adopted by Google, Zalando, and Microsoft.\r\n\r\n### 1. Standard Error Structure\r\n\r\n```json\r\n{\r\n  \"type\": \"https://api.example.com/errors/validation-error\",\r\n  \"title\": \"Validation Error\",\r\n  \"status\": 400,\r\n  \"detail\": \"The 'email' field is not a valid email address\",\r\n  \"instance\": \"/users/registration\",\r\n  \"traceId\": \"req_abc123xyz\",\r\n  \"timestamp\": \"2026-01-19T12:00:00Z\",\r\n  \"errors\": [\r\n    {\r\n      \"field\": \"email\",\r\n      \"code\": \"INVALID_FORMAT\",\r\n      \"message\": \"Must be a valid email address\",\r\n      \"rejectedValue\": \"not-an-email\"\r\n    },\r\n    {\r\n      \"field\": \"age\",\r\n      \"code\": \"MIN_VALUE\",\r\n      \"message\": \"Must be at least 18\",\r\n      \"rejectedValue\": 15\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\n### 2. Required Fields (RFC 7807)\r\n\r\n| Field | Type | Description |\r\n|-------|------|-------------|\r\n| `type` | URI | Machine-readable error type (use docs URL) |\r\n| `title` | string | Short, human-readable summary |\r\n| `status` | integer | HTTP status code (redundant but useful) |\r\n| `detail` | string | Human-readable explanation specific to this occurrence |\r\n| `instance` | string | URI reference to the specific occurrence |\r\n\r\n### 3. Extended Fields (Recommended)\r\n\r\n| Field | Type | Description |\r\n|-------|------|-------------|\r\n| `traceId` | string | Correlation/request ID for debugging |\r\n| `timestamp` | ISO8601 | When the error occurred |\r\n| `errors` | array | Field-level validation errors |\r\n| `code` | string | Application-specific error code |\r\n\r\n### 4. HTTP Status Code Mapping\r\n\r\n```\r\n┌─────────────────────────────────────────────────────────┐\r\n│ Client Errors (4xx)                                     │\r\n├─────────────────────────────────────────────────────────┤\r\n│ 400 Bad Request      - Malformed syntax, validation     │\r\n│ 401 Unauthorized     - Missing/invalid authentication   │\r\n│ 403 Forbidden        - Valid auth, insufficient perms   │\r\n│ 404 Not Found        - Resource doesn't exist           │\r\n│ 405 Method Not Allow - HTTP method not supported        │\r\n│ 409 Conflict         - State conflict (duplicate, etc)  │\r\n│ 410 Gone             - Resource permanently deleted     │\r\n│ 422 Unprocessable    - Semantic validation failure      │\r\n│ 429 Too Many Request - Rate limit exceeded              │\r\n├─────────────────────────────────────────────────────────┤\r\n│ Server Errors (5xx)                                     │\r\n├─────────────────────────────────────────────────────────┤\r\n│ 500 Internal Error   - Unexpected server failure        │\r\n│ 502 Bad Gateway      - Upstream service failure         │\r\n│ 503 Service Unavail  - Temporarily unavailable          │\r\n│ 504 Gateway Timeout  - Upstream timeout                 │\r\n└─────────────────────────────────────────────────────────┘\r\n```\r\n\r\n### 5. Error Code Conventions\r\n\r\nUse `SCREAMING_SNAKE_CASE` for machine-readable codes:\r\n\r\n```\r\nVALIDATION_ERROR          RESOURCE_NOT_FOUND\r\nAUTHENTICATION_REQUIRED   PERMISSION_DENIED\r\nRATE_LIMIT_EXCEEDED       CONFLICT_DETECTED\r\nINVALID_PARAMETER         MISSING_REQUIRED_FIELD\r\nRESOURCE_EXHAUSTED        PRECONDITION_FAILED\r\n```\r\n\r\n## Pitfalls\r\n\r\n| Pitfall | Impact | How to Avoid |\r\n|---------|--------|--------------|\r\n| Exposing stack traces | Security vulnerability, info leak | Log server-side only, return generic message |\r\n| 200 OK with error body | Breaks HTTP semantics, caching | Always use appropriate 4xx/5xx |\r\n| Inconsistent structure | Client fragility, maintenance hell | Use shared error middleware/factory |\r\n| Missing trace ID | Impossible to debug in prod | Generate at edge, propagate everywhere |\r\n| Leaking internal paths | Security risk | Sanitize file paths, DB details |\r\n| Generic \"Error occurred\" | Poor UX, no actionable info | Provide specific, helpful messages |\r\n| No error documentation | API consumers confused | Document every error type in OpenAPI |\r\n\r\n## Checklist\r\n\r\n- [ ] Using RFC 7807 Problem Details format\r\n- [ ] `type` URIs resolve to error documentation\r\n- [ ] HTTP status codes correctly mapped\r\n- [ ] `traceId` present in every error response\r\n- [ ] Field-level validation errors in `errors` array\r\n- [ ] Error codes are documented in OpenAPI spec\r\n- [ ] Stack traces never exposed to clients\r\n- [ ] Error messages are helpful and actionable\r\n- [ ] Consistent error handling middleware\r\n- [ ] Server logs include full context (stack, request body)\r\n- [ ] Sensitive data redacted from error details\r\n- [ ] Error responses set `Content-Type: application/problem+json`\r\n\r\n## Code Examples\r\n\r\n### TypeScript/Node.js\r\n\r\n```typescript\r\n// error-types.ts\r\nexport class ApiError extends Error {\r\n  constructor(\r\n    public readonly type: string,\r\n    public readonly title: string,\r\n    public readonly status: number,\r\n    public readonly detail: string,\r\n    public readonly errors?: FieldError[]\r\n  ) {\r\n    super(detail);\r\n    this.name = 'ApiError';\r\n  }\r\n\r\n  static badRequest(detail: string, errors?: FieldError[]) {\r\n    return new ApiError(\r\n      '/errors/bad-request',\r\n      'Bad Request',\r\n      400,\r\n      detail,\r\n      errors\r\n    );\r\n  }\r\n\r\n  static notFound(resource: string, id: string) {\r\n    return new ApiError(\r\n      '/errors/not-found',\r\n      'Not Found',\r\n      404,\r\n      `${resource} with ID '${id}' not found`\r\n    );\r\n  }\r\n\r\n  static unauthorized(detail = 'Authentication required') {\r\n    return new ApiError('/errors/unauthorized', 'Unauthorized', 401, detail);\r\n  }\r\n\r\n  toJSON(traceId: string) {\r\n    return {\r\n      type: `https://api.example.com${this.type}`,\r\n      title: this.title,\r\n      status: this.status,\r\n      detail: this.detail,\r\n      traceId,\r\n      timestamp: new Date().toISOString(),\r\n      ...(this.errors && { errors: this.errors }),\r\n    };\r\n  }\r\n}\r\n\r\n// error-middleware.ts (Express)\r\nexport function errorHandler(err: Error, req: Request, res: Response, next: NextFunction) {\r\n  const traceId = req.headers['x-trace-id'] || crypto.randomUUID();\r\n  \r\n  // Log full error server-side\r\n  logger.error({ err, traceId, path: req.path, method: req.method });\r\n\r\n  if (err instanceof ApiError) {\r\n    return res\r\n      .status(err.status)\r\n      .type('application/problem+json')\r\n      .json(err.toJSON(traceId));\r\n  }\r\n\r\n  // Unknown errors - don't leak details\r\n  res.status(500).type('application/problem+json').json({\r\n    type: 'https://api.example.com/errors/internal',\r\n    title: 'Internal Server Error',\r\n    status: 500,\r\n    detail: 'An unexpected error occurred',\r\n    traceId,\r\n    timestamp: new Date().toISOString(),\r\n  });\r\n}\r\n```\r\n\r\n### Python/FastAPI\r\n\r\n```python\r\nfrom fastapi import FastAPI, Request, HTTPException\r\nfrom fastapi.responses import JSONResponse\r\nfrom pydantic import BaseModel\r\nfrom typing import Optional, List\r\nfrom datetime import datetime\r\nimport uuid\r\n\r\nclass FieldError(BaseModel):\r\n    field: str\r\n    code: str\r\n    message: str\r\n    rejected_value: Optional[any] = None\r\n\r\nclass ProblemDetail(BaseModel):\r\n    type: str\r\n    title: str\r\n    status: int\r\n    detail: str\r\n    instance: Optional[str] = None\r\n    trace_id: str\r\n    timestamp: str\r\n    errors: Optional[List[FieldError]] = None\r\n\r\nclass ApiError(Exception):\r\n    def __init__(self, type_: str, title: str, status: int, detail: str, \r\n                 errors: List[FieldError] = None):\r\n        self.type = type_\r\n        self.title = title\r\n        self.status = status\r\n        self.detail = detail\r\n        self.errors = errors\r\n\r\n@app.exception_handler(ApiError)\r\nasync def api_error_handler(request: Request, exc: ApiError):\r\n    trace_id = request.headers.get(\"x-trace-id\", str(uuid.uuid4()))\r\n    return JSONResponse(\r\n        status_code=exc.status,\r\n        content=ProblemDetail(\r\n            type=f\"https://api.example.com{exc.type}\",\r\n            title=exc.title,\r\n            status=exc.status,\r\n            detail=exc.detail,\r\n            instance=str(request.url.path),\r\n            trace_id=trace_id,\r\n            timestamp=datetime.utcnow().isoformat() + \"Z\",\r\n            errors=exc.errors,\r\n        ).dict(exclude_none=True),\r\n        media_type=\"application/problem+json\",\r\n    )\r\n```\r\n\r\n### Go\r\n\r\n```go\r\npackage errors\r\n\r\nimport (\r\n    \"encoding/json\"\r\n    \"net/http\"\r\n    \"time\"\r\n)\r\n\r\ntype FieldError struct {\r\n    Field         string      `json:\"field\"`\r\n    Code          string      `json:\"code\"`\r\n    Message       string      `json:\"message\"`\r\n    RejectedValue interface{} `json:\"rejectedValue,omitempty\"`\r\n}\r\n\r\ntype ProblemDetail struct {\r\n    Type      string       `json:\"type\"`\r\n    Title     string       `json:\"title\"`\r\n    Status    int          `json:\"status\"`\r\n    Detail    string       `json:\"detail\"`\r\n    Instance  string       `json:\"instance,omitempty\"`\r\n    TraceID   string       `json:\"traceId\"`\r\n    Timestamp string       `json:\"timestamp\"`\r\n    Errors    []FieldError `json:\"errors,omitempty\"`\r\n}\r\n\r\nfunc WriteError(w http.ResponseWriter, r *http.Request, err *ProblemDetail) {\r\n    err.TraceID = r.Header.Get(\"X-Trace-ID\")\r\n    if err.TraceID == \"\" {\r\n        err.TraceID = generateTraceID()\r\n    }\r\n    err.Timestamp = time.Now().UTC().Format(time.RFC3339)\r\n    err.Instance = r.URL.Path\r\n    \r\n    w.Header().Set(\"Content-Type\", \"application/problem+json\")\r\n    w.WriteHeader(err.Status)\r\n    json.NewEncoder(w).Encode(err)\r\n}\r\n\r\nfunc BadRequest(detail string, errors []FieldError) *ProblemDetail {\r\n    return &ProblemDetail{\r\n        Type:   \"https://api.example.com/errors/bad-request\",\r\n        Title:  \"Bad Request\",\r\n        Status: http.StatusBadRequest,\r\n        Detail: detail,\r\n        Errors: errors,\r\n    }\r\n}\r\n```\r\n\r\n## References\r\n\r\n- [RFC 7807 - Problem Details for HTTP APIs](https://datatracker.ietf.org/doc/html/rfc7807)\r\n- [Zalando RESTful API Guidelines - Error Handling](https://opensource.zalando.com/restful-api-guidelines/#176)\r\n- [Google Cloud API Design - Errors](https://cloud.google.com/apis/design/errors)\r\n- [Stripe API - Error Handling](https://stripe.com/docs/api/errors)\r\n- [Microsoft REST API Guidelines - Error Condition Responses](https://github.com/microsoft/api-guidelines/blob/vNext/Guidelines.md#7102-error-condition-responses)\r\n\r\n## Sources\r\n\r\n- RFC 7807 - Problem Details for HTTP APIs: https://datatracker.ietf.org/doc/html/rfc7807\r\n- Google API Design Guide - Errors: https://cloud.google.com/apis/design/errors\r\n- Microsoft REST API Guidelines - Error Handling: https://github.com/microsoft/api-guidelines/blob/vNext/Guidelines.md\r\n- Zalando RESTful API Guidelines: https://opensource.zalando.com/restful-api-guidelines/\r\n"
  },
  {
    "id": "api-idempotency-keys",
    "title": "Idempotency Keys",
    "tags": [
      "api",
      "idempotency",
      "reliability",
      "payments",
      "distributed-systems"
    ],
    "level": "intermediate",
    "stacks": [
      "all"
    ],
    "scope": "api",
    "maturity": "stable",
    "works_with": [
      "all"
    ],
    "path": "D:/Code/backend-engineering-kit/.shared/production-backend-kit/patterns/api.idempotency-keys.md",
    "type": "pattern",
    "sections": [],
    "checklist": [],
    "sources": [],
    "rawContent": "\r\n# Idempotency Keys\r\n\r\n## Problem\r\n\r\nNetwork failures, timeouts, and client retries cause **duplicate operations**:\r\n- Double charges on payments\r\n- Duplicate orders created\r\n- Multiple emails sent\r\n- Inconsistent financial records\r\n\r\nWithout idempotency guarantees, clients cannot safely retry failed requests.\r\n\r\n## When to use\r\n\r\n- **Payment processing** (critical!)\r\n- Order/transaction creation\r\n- Any state-changing operation (POST, PUT, PATCH, DELETE)\r\n- Webhook delivery systems\r\n- Message queue consumers\r\n- Financial transactions\r\n- Email/notification sending\r\n\r\n## Solution\r\n\r\n### 1. Idempotency Key Flow\r\n\r\n```\r\n┌──────────┐     ┌──────────┐     ┌──────────┐     ┌──────────┐\r\n│  Client  │────▶│ Check DB │────▶│ Process  │────▶│  Store   │\r\n│          │     │ for Key  │     │ Request  │     │ Response │\r\n└──────────┘     └──────────┘     └──────────┘     └──────────┘\r\n     │                │                                  │\r\n     │                ▼                                  │\r\n     │         ┌──────────┐                             │\r\n     │         │ Key Found│                             │\r\n     │         │  Match?  │                             │\r\n     │         └──────────┘                             │\r\n     │           │      │                               │\r\n     │      Yes  │      │ No                            │\r\n     │           ▼      ▼                               │\r\n     │      ┌────────┐ ┌────────┐                       │\r\n     │      │ Return │ │  409   │                       │\r\n     │      │ Cached │ │Conflict│                       │\r\n     │      └────────┘ └────────┘                       │\r\n     │                                                  │\r\n     └──────────────────────────────────────────────────┘\r\n                    Retry with same key\r\n```\r\n\r\n### 2. Request Headers\r\n\r\n```http\r\nPOST /v1/payments HTTP/1.1\r\nHost: api.example.com\r\nIdempotency-Key: idem_8f14e45f-ceea-367a-9f10-8c4f5f3c8d8d\r\nContent-Type: application/json\r\n\r\n{\r\n  \"amount\": 10000,\r\n  \"currency\": \"USD\",\r\n  \"customer_id\": \"cus_abc123\"\r\n}\r\n```\r\n\r\n### 3. Key Requirements\r\n\r\n| Requirement | Implementation |\r\n|-------------|----------------|\r\n| Uniqueness | UUID v4 or ULID per operation |\r\n| Scope | Namespace by user/tenant ID |\r\n| Fingerprint | SHA-256 of (method + path + body) |\r\n| TTL | 24-48 hours minimum |\r\n| Storage | Redis (speed) + DB (persistence) |\r\n\r\n### 4. Processing States\r\n\r\n```\r\n┌─────────────────────────────────────────────────────────┐\r\n│ State        │ Meaning                │ Response        │\r\n├──────────────┼────────────────────────┼─────────────────┤\r\n│ NOT_FOUND    │ First request          │ Process & store │\r\n│ PROCESSING   │ In-flight (locked)     │ 409 Conflict    │\r\n│ COMPLETED    │ Previously succeeded   │ Return cached   │\r\n│ FAILED       │ Previously failed      │ Allow retry*    │\r\n│ HASH_MISMATCH│ Same key, diff request │ 422 Unprocess.  │\r\n└─────────────────────────────────────────────────────────┘\r\n\r\n* Failed requests may allow retry with same key (configurable)\r\n```\r\n\r\n### 5. Storage Schema\r\n\r\n```sql\r\nCREATE TABLE idempotency_keys (\r\n  id              UUID PRIMARY KEY DEFAULT gen_random_uuid(),\r\n  idempotency_key VARCHAR(255) NOT NULL,\r\n  user_id         UUID NOT NULL,\r\n  request_method  VARCHAR(10) NOT NULL,\r\n  request_path    VARCHAR(500) NOT NULL,\r\n  request_hash    VARCHAR(64) NOT NULL,  -- SHA-256\r\n  \r\n  status          VARCHAR(20) NOT NULL DEFAULT 'processing',\r\n  -- 'processing', 'completed', 'failed'\r\n  \r\n  response_code   INTEGER,\r\n  response_body   JSONB,\r\n  \r\n  locked_at       TIMESTAMPTZ,\r\n  created_at      TIMESTAMPTZ DEFAULT NOW(),\r\n  expires_at      TIMESTAMPTZ NOT NULL,\r\n  \r\n  UNIQUE (idempotency_key, user_id)\r\n);\r\n\r\nCREATE INDEX idx_idempotency_keys_lookup \r\n  ON idempotency_keys (idempotency_key, user_id) \r\n  WHERE expires_at > NOW();\r\n```\r\n\r\n## Pitfalls\r\n\r\n| Pitfall | Impact | How to Avoid |\r\n|---------|--------|--------------|\r\n| Not hashing request body | Different requests return wrong cached response | Always store and compare fingerprint |\r\n| Key reuse across users | Security breach, data leak | Scope key to user_id/tenant_id |\r\n| Too short TTL | Retries fail after key expires | Use 24-48 hours minimum |\r\n| No lock during processing | Race condition, double processing | Use DB row lock or Redis SETNX |\r\n| Storing only success responses | Failed requests processed twice | Store failures too (configurable retry) |\r\n| Ignoring in-flight requests | Concurrent duplicates | Return 409 if locked/processing |\r\n| No cleanup | Storage bloat | Use TTL + periodic cleanup job |\r\n\r\n## Checklist\r\n\r\n- [ ] `Idempotency-Key` header accepted (case-insensitive)\r\n- [ ] Key scoped to authenticated user/tenant\r\n- [ ] Request fingerprint stored (method + path + body hash)\r\n- [ ] Response body and status code cached\r\n- [ ] TTL set appropriately (24-48 hours)\r\n- [ ] Concurrent requests handled (distributed lock)\r\n- [ ] Mismatched request returns 422 Unprocessable\r\n- [ ] In-flight request returns 409 Conflict\r\n- [ ] Storage is highly available (Redis/PostgreSQL)\r\n- [ ] Key format and usage documented for API consumers\r\n- [ ] Expired keys cleaned up automatically\r\n- [ ] Monitoring on duplicate key hits\r\n\r\n## Code Examples\r\n\r\n### TypeScript/Node.js (Express + Redis)\r\n\r\n```typescript\r\nimport crypto from 'crypto';\r\nimport Redis from 'ioredis';\r\n\r\nconst redis = new Redis();\r\nconst IDEMPOTENCY_TTL = 48 * 60 * 60; // 48 hours in seconds\r\n\r\ninterface IdempotencyRecord {\r\n  status: 'processing' | 'completed' | 'failed';\r\n  requestHash: string;\r\n  responseCode?: number;\r\n  responseBody?: any;\r\n  createdAt: string;\r\n}\r\n\r\nfunction hashRequest(method: string, path: string, body: any): string {\r\n  const data = JSON.stringify({ method, path, body });\r\n  return crypto.createHash('sha256').update(data).digest('hex');\r\n}\r\n\r\nexport async function idempotencyMiddleware(\r\n  req: Request,\r\n  res: Response,\r\n  next: NextFunction\r\n) {\r\n  const idempotencyKey = req.headers['idempotency-key'] as string;\r\n  \r\n  // Only apply to state-changing methods\r\n  if (!['POST', 'PUT', 'PATCH', 'DELETE'].includes(req.method)) {\r\n    return next();\r\n  }\r\n  \r\n  // Key is optional but recommended\r\n  if (!idempotencyKey) {\r\n    return next();\r\n  }\r\n  \r\n  const userId = req.user.id;\r\n  const redisKey = `idempotency:${userId}:${idempotencyKey}`;\r\n  const requestHash = hashRequest(req.method, req.path, req.body);\r\n  \r\n  // Try to acquire lock (SETNX pattern)\r\n  const existingRecord = await redis.get(redisKey);\r\n  \r\n  if (existingRecord) {\r\n    const record: IdempotencyRecord = JSON.parse(existingRecord);\r\n    \r\n    // Request hash mismatch - client reused key incorrectly\r\n    if (record.requestHash !== requestHash) {\r\n      return res.status(422).json({\r\n        type: 'https://api.example.com/errors/idempotency-mismatch',\r\n        title: 'Idempotency Key Mismatch',\r\n        status: 422,\r\n        detail: 'This idempotency key was used with different request parameters',\r\n      });\r\n    }\r\n    \r\n    // Still processing - return conflict\r\n    if (record.status === 'processing') {\r\n      return res.status(409).json({\r\n        type: 'https://api.example.com/errors/request-in-progress',\r\n        title: 'Request In Progress',\r\n        status: 409,\r\n        detail: 'A request with this idempotency key is currently being processed',\r\n      });\r\n    }\r\n    \r\n    // Completed - return cached response\r\n    if (record.status === 'completed') {\r\n      return res.status(record.responseCode!).json(record.responseBody);\r\n    }\r\n    \r\n    // Failed - allow retry (optional behavior)\r\n    // Fall through to process again\r\n  }\r\n  \r\n  // Store processing state with lock\r\n  const newRecord: IdempotencyRecord = {\r\n    status: 'processing',\r\n    requestHash,\r\n    createdAt: new Date().toISOString(),\r\n  };\r\n  \r\n  // Use SETNX for atomic check-and-set\r\n  const acquired = await redis.set(\r\n    redisKey,\r\n    JSON.stringify(newRecord),\r\n    'EX', IDEMPOTENCY_TTL,\r\n    'NX'\r\n  );\r\n  \r\n  if (!acquired) {\r\n    // Race condition - another request got there first\r\n    return res.status(409).json({\r\n      type: 'https://api.example.com/errors/request-in-progress',\r\n      title: 'Request In Progress',\r\n      status: 409,\r\n      detail: 'A request with this idempotency key is currently being processed',\r\n    });\r\n  }\r\n  \r\n  // Capture response to store\r\n  const originalJson = res.json.bind(res);\r\n  res.json = function(body: any) {\r\n    // Store completed state\r\n    const completedRecord: IdempotencyRecord = {\r\n      status: res.statusCode >= 400 ? 'failed' : 'completed',\r\n      requestHash,\r\n      responseCode: res.statusCode,\r\n      responseBody: body,\r\n      createdAt: newRecord.createdAt,\r\n    };\r\n    \r\n    redis.set(redisKey, JSON.stringify(completedRecord), 'EX', IDEMPOTENCY_TTL);\r\n    \r\n    return originalJson(body);\r\n  };\r\n  \r\n  next();\r\n}\r\n```\r\n\r\n### Python/FastAPI\r\n\r\n```python\r\nimport hashlib\r\nimport json\r\nfrom datetime import datetime, timedelta\r\nfrom typing import Optional\r\nfrom fastapi import Request, Response, HTTPException\r\nfrom redis.asyncio import Redis\r\n\r\nIDEMPOTENCY_TTL = timedelta(hours=48)\r\n\r\nasync def get_idempotency_record(redis: Redis, key: str) -> Optional[dict]:\r\n    data = await redis.get(key)\r\n    return json.loads(data) if data else None\r\n\r\nasync def set_idempotency_record(redis: Redis, key: str, record: dict):\r\n    await redis.set(key, json.dumps(record), ex=int(IDEMPOTENCY_TTL.total_seconds()))\r\n\r\ndef hash_request(method: str, path: str, body: bytes) -> str:\r\n    data = f\"{method}:{path}:{body.decode()}\"\r\n    return hashlib.sha256(data.encode()).hexdigest()\r\n\r\n@app.middleware(\"http\")\r\nasync def idempotency_middleware(request: Request, call_next):\r\n    if request.method not in (\"POST\", \"PUT\", \"PATCH\", \"DELETE\"):\r\n        return await call_next(request)\r\n    \r\n    idempotency_key = request.headers.get(\"idempotency-key\")\r\n    if not idempotency_key:\r\n        return await call_next(request)\r\n    \r\n    user_id = request.state.user.id\r\n    redis_key = f\"idempotency:{user_id}:{idempotency_key}\"\r\n    body = await request.body()\r\n    request_hash = hash_request(request.method, request.url.path, body)\r\n    \r\n    existing = await get_idempotency_record(redis, redis_key)\r\n    \r\n    if existing:\r\n        if existing[\"request_hash\"] != request_hash:\r\n            raise HTTPException(\r\n                status_code=422,\r\n                detail=\"Idempotency key was used with different request parameters\"\r\n            )\r\n        \r\n        if existing[\"status\"] == \"processing\":\r\n            raise HTTPException(\r\n                status_code=409,\r\n                detail=\"Request with this idempotency key is in progress\"\r\n            )\r\n        \r\n        if existing[\"status\"] == \"completed\":\r\n            return Response(\r\n                content=json.dumps(existing[\"response_body\"]),\r\n                status_code=existing[\"response_code\"],\r\n                media_type=\"application/json\"\r\n            )\r\n    \r\n    # Acquire lock\r\n    acquired = await redis.set(\r\n        redis_key,\r\n        json.dumps({\r\n            \"status\": \"processing\",\r\n            \"request_hash\": request_hash,\r\n            \"created_at\": datetime.utcnow().isoformat()\r\n        }),\r\n        ex=int(IDEMPOTENCY_TTL.total_seconds()),\r\n        nx=True\r\n    )\r\n    \r\n    if not acquired:\r\n        raise HTTPException(status_code=409, detail=\"Request in progress\")\r\n    \r\n    # Process request\r\n    response = await call_next(request)\r\n    \r\n    # Store result\r\n    response_body = b\"\".join([chunk async for chunk in response.body_iterator])\r\n    await set_idempotency_record(redis, redis_key, {\r\n        \"status\": \"completed\" if response.status_code < 400 else \"failed\",\r\n        \"request_hash\": request_hash,\r\n        \"response_code\": response.status_code,\r\n        \"response_body\": json.loads(response_body) if response_body else None,\r\n        \"created_at\": datetime.utcnow().isoformat()\r\n    })\r\n    \r\n    return Response(\r\n        content=response_body,\r\n        status_code=response.status_code,\r\n        headers=dict(response.headers),\r\n        media_type=response.media_type\r\n    )\r\n```\r\n\r\n## References\r\n\r\n- [Stripe - Idempotent Requests](https://stripe.com/docs/api/idempotent_requests)\r\n- [Brandur - Implementing Stripe-like Idempotency Keys](https://brandur.org/idempotency-keys)\r\n- [AWS - Making Retries Safe with Idempotent APIs](https://aws.amazon.com/builders-library/making-retries-safe-with-idempotent-APIs/)\r\n- [PayPal - Idempotency](https://developer.paypal.com/docs/api/reference/api-responses/#link-idempotency)\r\n"
  },
  {
    "id": "api-idempotency",
    "title": "Idempotent API Design",
    "tags": [
      "api",
      "idempotency",
      "reliability",
      "retry",
      "consistency"
    ],
    "level": "intermediate",
    "stacks": [
      "nodejs",
      "python",
      "go"
    ],
    "scope": "api",
    "maturity": "stable",
    "works_with": [
      "nodejs",
      "python",
      "go"
    ],
    "path": "D:/Code/backend-engineering-kit/.shared/production-backend-kit/patterns/api.idempotency.md",
    "type": "pattern",
    "sections": [],
    "checklist": [],
    "sources": [],
    "rawContent": "\r\n# Idempotent API Design\r\n\r\n## Problem\r\n\r\nWithout idempotency:\r\n```\r\nClient ──POST /orders──→ Server (creates order)\r\n                        ↓\r\nClient ←───timeout/error───┘\r\n                        ↓\r\nClient ──POST /orders──→ Server (creates DUPLICATE order!)\r\n```\r\n\r\nNetwork failures, timeouts, and retries can cause:\r\n- Duplicate payments\r\n- Double order submissions\r\n- Multiple resource creations\r\n- Data inconsistencies\r\n\r\n## When to use\r\n\r\n- **All mutating operations** (POST, PUT, PATCH, DELETE)\r\n- Payment processing\r\n- Order/booking systems\r\n- Any operation that has side effects\r\n- Webhook handlers\r\n- Message queue consumers\r\n\r\n## Solution\r\n\r\n### 1. Idempotency Key Pattern\r\n\r\n```typescript\r\nimport { createHash } from 'crypto';\r\nimport { Redis } from 'ioredis';\r\n\r\ninterface IdempotencyRecord {\r\n  status: 'processing' | 'completed' | 'failed';\r\n  response?: any;\r\n  statusCode?: number;\r\n  createdAt: number;\r\n  completedAt?: number;\r\n}\r\n\r\nclass IdempotencyService {\r\n  constructor(\r\n    private redis: Redis,\r\n    private lockTTL = 60,      // Lock timeout in seconds\r\n    private recordTTL = 86400   // Store completed responses for 24h\r\n  ) {}\r\n\r\n  /**\r\n   * Execute an operation idempotently\r\n   */\r\n  async execute<T>(\r\n    idempotencyKey: string,\r\n    operation: () => Promise<{ statusCode: number; body: T }>\r\n  ): Promise<{ statusCode: number; body: T; fromCache: boolean }> {\r\n    const recordKey = `idempotency:${idempotencyKey}`;\r\n    const lockKey = `idempotency:lock:${idempotencyKey}`;\r\n    \r\n    // Check if we already have a response for this key\r\n    const existingRecord = await this.redis.get(recordKey);\r\n    \r\n    if (existingRecord) {\r\n      const record: IdempotencyRecord = JSON.parse(existingRecord);\r\n      \r\n      if (record.status === 'completed') {\r\n        // Return cached response\r\n        return {\r\n          statusCode: record.statusCode!,\r\n          body: record.response,\r\n          fromCache: true,\r\n        };\r\n      }\r\n      \r\n      if (record.status === 'processing') {\r\n        // Another request is processing - conflict\r\n        throw new IdempotencyConflictError(\r\n          'Request with this idempotency key is already being processed'\r\n        );\r\n      }\r\n      \r\n      // Failed - allow retry\r\n    }\r\n    \r\n    // Try to acquire lock\r\n    const lockAcquired = await this.redis.set(\r\n      lockKey,\r\n      '1',\r\n      'EX',\r\n      this.lockTTL,\r\n      'NX'\r\n    );\r\n    \r\n    if (!lockAcquired) {\r\n      throw new IdempotencyConflictError(\r\n        'Request with this idempotency key is being processed'\r\n      );\r\n    }\r\n    \r\n    try {\r\n      // Mark as processing\r\n      await this.redis.set(\r\n        recordKey,\r\n        JSON.stringify({\r\n          status: 'processing',\r\n          createdAt: Date.now(),\r\n        } as IdempotencyRecord),\r\n        'EX',\r\n        this.lockTTL\r\n      );\r\n      \r\n      // Execute the operation\r\n      const result = await operation();\r\n      \r\n      // Store successful response\r\n      await this.redis.set(\r\n        recordKey,\r\n        JSON.stringify({\r\n          status: 'completed',\r\n          response: result.body,\r\n          statusCode: result.statusCode,\r\n          createdAt: Date.now(),\r\n          completedAt: Date.now(),\r\n        } as IdempotencyRecord),\r\n        'EX',\r\n        this.recordTTL\r\n      );\r\n      \r\n      return { ...result, fromCache: false };\r\n      \r\n    } catch (error) {\r\n      // Mark as failed\r\n      await this.redis.set(\r\n        recordKey,\r\n        JSON.stringify({\r\n          status: 'failed',\r\n          createdAt: Date.now(),\r\n        } as IdempotencyRecord),\r\n        'EX',\r\n        300  // Allow retry after 5 minutes\r\n      );\r\n      \r\n      throw error;\r\n    } finally {\r\n      // Release lock\r\n      await this.redis.del(lockKey);\r\n    }\r\n  }\r\n\r\n  /**\r\n   * Generate idempotency key from request data\r\n   */\r\n  generateKey(components: {\r\n    userId: string;\r\n    action: string;\r\n    data: object;\r\n  }): string {\r\n    const payload = JSON.stringify({\r\n      userId: components.userId,\r\n      action: components.action,\r\n      data: this.sortObject(components.data),\r\n    });\r\n    \r\n    return createHash('sha256').update(payload).digest('hex');\r\n  }\r\n\r\n  private sortObject(obj: object): object {\r\n    return Object.keys(obj)\r\n      .sort()\r\n      .reduce((acc, key) => {\r\n        acc[key] = obj[key];\r\n        return acc;\r\n      }, {} as any);\r\n  }\r\n}\r\n\r\nclass IdempotencyConflictError extends Error {\r\n  constructor(message: string) {\r\n    super(message);\r\n    this.name = 'IdempotencyConflictError';\r\n  }\r\n}\r\n```\r\n\r\n### 2. Express Middleware\r\n\r\n```typescript\r\nimport { Request, Response, NextFunction } from 'express';\r\n\r\nconst idempotencyService = new IdempotencyService(redis);\r\n\r\n// Middleware for idempotent endpoints\r\nfunction idempotent(options: { \r\n  headerName?: string;\r\n  required?: boolean;\r\n  generateKey?: (req: Request) => string;\r\n} = {}) {\r\n  const {\r\n    headerName = 'Idempotency-Key',\r\n    required = true,\r\n    generateKey,\r\n  } = options;\r\n\r\n  return async (req: Request, res: Response, next: NextFunction) => {\r\n    // Get idempotency key from header or generate\r\n    let idempotencyKey = req.get(headerName);\r\n    \r\n    if (!idempotencyKey && generateKey) {\r\n      idempotencyKey = generateKey(req);\r\n    }\r\n    \r\n    if (!idempotencyKey) {\r\n      if (required) {\r\n        return res.status(400).json({\r\n          error: 'Idempotency-Key header is required',\r\n          code: 'MISSING_IDEMPOTENCY_KEY',\r\n        });\r\n      }\r\n      return next();\r\n    }\r\n    \r\n    // Validate key format (UUID v4)\r\n    if (!isValidUUID(idempotencyKey)) {\r\n      return res.status(400).json({\r\n        error: 'Invalid Idempotency-Key format. Use UUID v4.',\r\n        code: 'INVALID_IDEMPOTENCY_KEY',\r\n      });\r\n    }\r\n    \r\n    // Include user/tenant in key to prevent cross-user conflicts\r\n    const scopedKey = `${req.user?.id || 'anonymous'}:${idempotencyKey}`;\r\n    \r\n    try {\r\n      const result = await idempotencyService.execute(scopedKey, async () => {\r\n        // Capture the response\r\n        const originalJson = res.json.bind(res);\r\n        let capturedBody: any;\r\n        let capturedStatus: number = 200;\r\n        \r\n        res.json = (body: any) => {\r\n          capturedBody = body;\r\n          return originalJson(body);\r\n        };\r\n        \r\n        res.status = ((code: number) => {\r\n          capturedStatus = code;\r\n          return res;\r\n        }) as any;\r\n        \r\n        // Call next middleware/handler\r\n        await new Promise<void>((resolve, reject) => {\r\n          res.on('finish', resolve);\r\n          res.on('error', reject);\r\n          next();\r\n        });\r\n        \r\n        return { statusCode: capturedStatus, body: capturedBody };\r\n      });\r\n      \r\n      // If from cache, set header and return cached response\r\n      if (result.fromCache) {\r\n        res.setHeader('Idempotent-Replayed', 'true');\r\n        return res.status(result.statusCode).json(result.body);\r\n      }\r\n      \r\n    } catch (error) {\r\n      if (error instanceof IdempotencyConflictError) {\r\n        return res.status(409).json({\r\n          error: error.message,\r\n          code: 'IDEMPOTENCY_CONFLICT',\r\n        });\r\n      }\r\n      throw error;\r\n    }\r\n  };\r\n}\r\n\r\n// Usage\r\napp.post('/api/payments', \r\n  authenticate,\r\n  idempotent({ required: true }),\r\n  async (req, res) => {\r\n    const payment = await paymentService.createPayment(req.body);\r\n    res.status(201).json(payment);\r\n  }\r\n);\r\n```\r\n\r\n### 3. Database-Level Idempotency\r\n\r\n```typescript\r\nimport { Prisma, PrismaClient } from '@prisma/client';\r\n\r\n// Using database unique constraints for idempotency\r\n\r\ninterface CreateOrderInput {\r\n  idempotencyKey: string;\r\n  customerId: string;\r\n  items: Array<{ productId: string; quantity: number }>;\r\n  totalAmount: number;\r\n}\r\n\r\nclass OrderService {\r\n  constructor(private prisma: PrismaClient) {}\r\n\r\n  async createOrder(input: CreateOrderInput) {\r\n    const { idempotencyKey, ...orderData } = input;\r\n    \r\n    // Use upsert with idempotency key\r\n    // If key exists, return existing order\r\n    // If key doesn't exist, create new order\r\n    \r\n    try {\r\n      const order = await this.prisma.$transaction(async (tx) => {\r\n        // Check for existing order with this idempotency key\r\n        const existing = await tx.order.findUnique({\r\n          where: { idempotencyKey },\r\n          include: { items: true },\r\n        });\r\n        \r\n        if (existing) {\r\n          // Verify the request data matches\r\n          if (existing.customerId !== orderData.customerId) {\r\n            throw new Error('Idempotency key reused with different request body');\r\n          }\r\n          return existing;\r\n        }\r\n        \r\n        // Create new order\r\n        const newOrder = await tx.order.create({\r\n          data: {\r\n            idempotencyKey,\r\n            customerId: orderData.customerId,\r\n            totalAmount: orderData.totalAmount,\r\n            status: 'pending',\r\n            items: {\r\n              create: orderData.items.map(item => ({\r\n                productId: item.productId,\r\n                quantity: item.quantity,\r\n              })),\r\n            },\r\n          },\r\n          include: { items: true },\r\n        });\r\n        \r\n        // Trigger downstream processes only for new orders\r\n        await this.queueOrderProcessing(newOrder.id);\r\n        \r\n        return newOrder;\r\n      });\r\n      \r\n      return order;\r\n      \r\n    } catch (error) {\r\n      if (error instanceof Prisma.PrismaClientKnownRequestError) {\r\n        if (error.code === 'P2002') {\r\n          // Unique constraint violation - race condition\r\n          // Fetch and return the existing order\r\n          const existing = await this.prisma.order.findUnique({\r\n            where: { idempotencyKey },\r\n            include: { items: true },\r\n          });\r\n          return existing;\r\n        }\r\n      }\r\n      throw error;\r\n    }\r\n  }\r\n}\r\n\r\n// Schema\r\n// model Order {\r\n//   id              String    @id @default(uuid())\r\n//   idempotencyKey  String    @unique\r\n//   customerId      String\r\n//   totalAmount     Decimal\r\n//   status          String\r\n//   items           OrderItem[]\r\n//   createdAt       DateTime  @default(now())\r\n// }\r\n```\r\n\r\n### 4. Idempotent Message Processing\r\n\r\n```typescript\r\n// For message queues / event handlers\r\n\r\ninterface Message<T> {\r\n  id: string;           // Unique message ID\r\n  type: string;\r\n  payload: T;\r\n  timestamp: number;\r\n  retryCount?: number;\r\n}\r\n\r\nclass IdempotentMessageHandler<T> {\r\n  constructor(\r\n    private redis: Redis,\r\n    private ttl: number = 86400 * 7  // 7 days\r\n  ) {}\r\n\r\n  async process(\r\n    message: Message<T>,\r\n    handler: (payload: T) => Promise<void>\r\n  ): Promise<{ processed: boolean; duplicate: boolean }> {\r\n    const processedKey = `msg:processed:${message.id}`;\r\n    const lockKey = `msg:lock:${message.id}`;\r\n    \r\n    // Check if already processed\r\n    const alreadyProcessed = await this.redis.exists(processedKey);\r\n    if (alreadyProcessed) {\r\n      return { processed: true, duplicate: true };\r\n    }\r\n    \r\n    // Acquire lock\r\n    const lockAcquired = await this.redis.set(lockKey, '1', 'EX', 60, 'NX');\r\n    if (!lockAcquired) {\r\n      // Another worker is processing this message\r\n      throw new Error('Message is being processed by another worker');\r\n    }\r\n    \r\n    try {\r\n      // Double-check after acquiring lock\r\n      const stillNotProcessed = !(await this.redis.exists(processedKey));\r\n      if (!stillNotProcessed) {\r\n        return { processed: true, duplicate: true };\r\n      }\r\n      \r\n      // Process the message\r\n      await handler(message.payload);\r\n      \r\n      // Mark as processed\r\n      await this.redis.set(processedKey, JSON.stringify({\r\n        processedAt: Date.now(),\r\n        messageId: message.id,\r\n      }), 'EX', this.ttl);\r\n      \r\n      return { processed: true, duplicate: false };\r\n      \r\n    } finally {\r\n      await this.redis.del(lockKey);\r\n    }\r\n  }\r\n}\r\n\r\n// Usage with SQS/SNS\r\nclass OrderEventHandler {\r\n  private idempotentHandler = new IdempotentMessageHandler(redis);\r\n\r\n  async handleOrderCreated(message: Message<OrderCreatedEvent>) {\r\n    const result = await this.idempotentHandler.process(message, async (event) => {\r\n      // This will only run once per message ID\r\n      await notificationService.sendOrderConfirmation(event.orderId);\r\n      await inventoryService.reserveStock(event.items);\r\n      await analyticsService.trackOrder(event);\r\n    });\r\n    \r\n    if (result.duplicate) {\r\n      logger.info({ messageId: message.id }, 'Skipped duplicate message');\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n### 5. Idempotent State Machines\r\n\r\n```typescript\r\n// Using state machines for complex operations\r\n\r\nenum PaymentState {\r\n  CREATED = 'created',\r\n  AUTHORIZED = 'authorized',\r\n  CAPTURED = 'captured',\r\n  REFUNDED = 'refunded',\r\n  FAILED = 'failed',\r\n}\r\n\r\ninterface PaymentTransition {\r\n  from: PaymentState[];\r\n  to: PaymentState;\r\n  action: string;\r\n}\r\n\r\nconst paymentTransitions: PaymentTransition[] = [\r\n  { from: [PaymentState.CREATED], to: PaymentState.AUTHORIZED, action: 'authorize' },\r\n  { from: [PaymentState.AUTHORIZED], to: PaymentState.CAPTURED, action: 'capture' },\r\n  { from: [PaymentState.CAPTURED], to: PaymentState.REFUNDED, action: 'refund' },\r\n  { from: [PaymentState.CREATED, PaymentState.AUTHORIZED], to: PaymentState.FAILED, action: 'fail' },\r\n];\r\n\r\nclass IdempotentPaymentService {\r\n  constructor(private prisma: PrismaClient) {}\r\n\r\n  async authorize(paymentId: string, idempotencyKey: string): Promise<Payment> {\r\n    return this.transition(paymentId, 'authorize', idempotencyKey, async (payment) => {\r\n      // Call payment gateway\r\n      const authResult = await paymentGateway.authorize({\r\n        amount: payment.amount,\r\n        paymentMethodId: payment.paymentMethodId,\r\n      });\r\n      \r\n      return {\r\n        gatewayAuthId: authResult.authorizationId,\r\n        authorizedAt: new Date(),\r\n      };\r\n    });\r\n  }\r\n\r\n  async capture(paymentId: string, idempotencyKey: string): Promise<Payment> {\r\n    return this.transition(paymentId, 'capture', idempotencyKey, async (payment) => {\r\n      // Only capture if authorized\r\n      const captureResult = await paymentGateway.capture({\r\n        authorizationId: payment.gatewayAuthId,\r\n      });\r\n      \r\n      return {\r\n        gatewayCaptureId: captureResult.captureId,\r\n        capturedAt: new Date(),\r\n      };\r\n    });\r\n  }\r\n\r\n  private async transition(\r\n    paymentId: string,\r\n    action: string,\r\n    idempotencyKey: string,\r\n    execute: (payment: Payment) => Promise<Partial<Payment>>\r\n  ): Promise<Payment> {\r\n    const transition = paymentTransitions.find(t => t.action === action);\r\n    if (!transition) {\r\n      throw new Error(`Unknown action: ${action}`);\r\n    }\r\n\r\n    return await this.prisma.$transaction(async (tx) => {\r\n      // Lock the payment record\r\n      const payment = await tx.payment.findUnique({\r\n        where: { id: paymentId },\r\n      });\r\n\r\n      if (!payment) {\r\n        throw new Error('Payment not found');\r\n      }\r\n\r\n      // Check for duplicate idempotency key\r\n      const existingOperation = await tx.paymentOperation.findUnique({\r\n        where: { idempotencyKey },\r\n      });\r\n\r\n      if (existingOperation) {\r\n        if (existingOperation.paymentId !== paymentId) {\r\n          throw new Error('Idempotency key used for different payment');\r\n        }\r\n        // Already processed - return current state\r\n        return payment;\r\n      }\r\n\r\n      // Validate state transition\r\n      if (!transition.from.includes(payment.status as PaymentState)) {\r\n        // Invalid transition - but this might be a retry after success\r\n        if (payment.status === transition.to) {\r\n          // Already in target state - idempotent success\r\n          return payment;\r\n        }\r\n        throw new Error(\r\n          `Cannot ${action} payment in ${payment.status} state`\r\n        );\r\n      }\r\n\r\n      // Execute the action\r\n      const updates = await execute(payment);\r\n\r\n      // Update payment and record operation atomically\r\n      const updatedPayment = await tx.payment.update({\r\n        where: { id: paymentId },\r\n        data: {\r\n          ...updates,\r\n          status: transition.to,\r\n        },\r\n      });\r\n\r\n      await tx.paymentOperation.create({\r\n        data: {\r\n          idempotencyKey,\r\n          paymentId,\r\n          action,\r\n          previousState: payment.status,\r\n          newState: transition.to,\r\n        },\r\n      });\r\n\r\n      return updatedPayment;\r\n    });\r\n  }\r\n}\r\n```\r\n\r\n### 6. Client-Side Idempotency\r\n\r\n```typescript\r\n// Client implementation for idempotent requests\r\n\r\nclass IdempotentClient {\r\n  private pendingKeys = new Map<string, Promise<Response>>();\r\n\r\n  async request(\r\n    url: string,\r\n    options: RequestInit & { idempotencyKey?: string }\r\n  ): Promise<Response> {\r\n    const idempotencyKey = options.idempotencyKey || crypto.randomUUID();\r\n    \r\n    // Check if we already have a pending request with this key\r\n    const pending = this.pendingKeys.get(idempotencyKey);\r\n    if (pending) {\r\n      return pending;\r\n    }\r\n    \r\n    const requestPromise = this.executeWithRetry(url, {\r\n      ...options,\r\n      headers: {\r\n        ...options.headers,\r\n        'Idempotency-Key': idempotencyKey,\r\n      },\r\n    });\r\n    \r\n    // Store pending promise\r\n    this.pendingKeys.set(idempotencyKey, requestPromise);\r\n    \r\n    try {\r\n      const response = await requestPromise;\r\n      return response;\r\n    } finally {\r\n      this.pendingKeys.delete(idempotencyKey);\r\n    }\r\n  }\r\n\r\n  private async executeWithRetry(\r\n    url: string,\r\n    options: RequestInit,\r\n    maxRetries = 3\r\n  ): Promise<Response> {\r\n    let lastError: Error | null = null;\r\n    \r\n    for (let attempt = 0; attempt < maxRetries; attempt++) {\r\n      try {\r\n        const response = await fetch(url, options);\r\n        \r\n        // 409 Conflict - another request with same key in progress\r\n        if (response.status === 409) {\r\n          await this.delay(Math.pow(2, attempt) * 100);\r\n          continue;\r\n        }\r\n        \r\n        // Check if response was replayed\r\n        const replayed = response.headers.get('Idempotent-Replayed');\r\n        if (replayed === 'true') {\r\n          console.log('Received cached idempotent response');\r\n        }\r\n        \r\n        return response;\r\n        \r\n      } catch (error) {\r\n        lastError = error as Error;\r\n        \r\n        // Retry on network errors (with same idempotency key)\r\n        if (attempt < maxRetries - 1) {\r\n          await this.delay(Math.pow(2, attempt) * 100);\r\n          continue;\r\n        }\r\n      }\r\n    }\r\n    \r\n    throw lastError || new Error('Request failed');\r\n  }\r\n\r\n  private delay(ms: number): Promise<void> {\r\n    return new Promise(resolve => setTimeout(resolve, ms));\r\n  }\r\n}\r\n\r\n// Usage\r\nconst client = new IdempotentClient();\r\n\r\n// Same idempotency key ensures same result\r\nconst idempotencyKey = crypto.randomUUID();\r\n\r\nconst result1 = await client.request('/api/payments', {\r\n  method: 'POST',\r\n  body: JSON.stringify({ amount: 100 }),\r\n  idempotencyKey,\r\n});\r\n\r\n// If first request timed out, retry with SAME key\r\nconst result2 = await client.request('/api/payments', {\r\n  method: 'POST',\r\n  body: JSON.stringify({ amount: 100 }),\r\n  idempotencyKey,  // Same key!\r\n});\r\n\r\n// result1 and result2 will be the same payment\r\n```\r\n\r\n## Pitfalls\r\n\r\n| Pitfall | Impact | How to Avoid |\r\n|---------|--------|--------------|\r\n| No scope isolation | Cross-user conflicts | Include user/tenant in key |\r\n| Key reuse with different body | Data inconsistency | Validate request body matches |\r\n| Too short TTL | Lost idempotency | Keep records for 24h+ |\r\n| No conflict handling | Race conditions | Use locks + 409 responses |\r\n| Client key generation only | Missing coverage | Generate keys server-side for some cases |\r\n| Not handling partial failures | Inconsistent state | Use transactions + state machines |\r\n\r\n## Checklist\r\n\r\n- [ ] Idempotency-Key header accepted on mutating endpoints\r\n- [ ] Keys scoped to user/tenant\r\n- [ ] Request body validated against stored key\r\n- [ ] Lock mechanism prevents concurrent processing\r\n- [ ] Completed responses cached for replay\r\n- [ ] 409 Conflict returned for in-progress duplicates\r\n- [ ] Idempotent-Replayed header on cached responses\r\n- [ ] TTL appropriate for use case (24h+)\r\n- [ ] Client retries use same idempotency key\r\n- [ ] Database constraints as backup\r\n\r\n## References\r\n\r\n- [Stripe: Idempotent Requests](https://stripe.com/docs/api/idempotent_requests)\r\n- [AWS: Making Retries Safe](https://aws.amazon.com/builders-library/making-retries-safe-with-idempotent-APIs/)\r\n- [RFC 7231: HTTP Semantics](https://datatracker.ietf.org/doc/html/rfc7231#section-4.2.2)\r\n- [Designing Robust APIs](https://brandur.org/idempotency-keys)\r\n"
  },
  {
    "id": "api-pagination-filter-sort",
    "title": "API Pagination, Filtering & Sorting",
    "tags": [
      "api",
      "pagination",
      "filtering",
      "sorting",
      "rest",
      "cursor"
    ],
    "level": "intermediate",
    "stacks": [
      "all"
    ],
    "scope": "api",
    "maturity": "stable",
    "works_with": [
      "all"
    ],
    "path": "D:/Code/backend-engineering-kit/.shared/production-backend-kit/patterns/api.pagination-filter-sort.md",
    "type": "pattern",
    "sections": [],
    "checklist": [],
    "sources": [],
    "rawContent": "\r\n# API Pagination, Filtering & Sorting\r\n\r\n## Problem\r\n\r\nList endpoints returning unbounded results cause:\r\n- Memory exhaustion (server & client)\r\n- Timeout failures on large datasets\r\n- Poor user experience (long load times)\r\n- Inconsistent results with concurrent modifications\r\n- Database performance degradation\r\n\r\n## When to use\r\n\r\n- **Always** for collection endpoints\r\n- Any endpoint that could return >50 items\r\n- Real-time feeds and timelines\r\n- Admin dashboards and data tables\r\n- Search results\r\n\r\n## Solution\r\n\r\n### 1. Pagination Strategies Comparison\r\n\r\n```\r\n┌─────────────────────────────────────────────────────────────────────┐\r\n│ Strategy        │ Pros                    │ Cons                    │\r\n├─────────────────┼─────────────────────────┼─────────────────────────┤\r\n│ Offset/Page     │ Simple, random access   │ Drift with inserts,     │\r\n│ ?page=2&size=20 │ Jump to any page        │ slow on large offsets   │\r\n├─────────────────┼─────────────────────────┼─────────────────────────┤\r\n│ Cursor-based    │ Stable with writes,     │ No random access,       │\r\n│ ?cursor=xyz     │ Consistent pagination   │ Can't jump to page N    │\r\n├─────────────────┼─────────────────────────┼─────────────────────────┤\r\n│ Keyset/Seek     │ Best performance,       │ Requires sortable key,  │\r\n│ ?after_id=100   │ Works with billions     │ Complex with multi-sort │\r\n└─────────────────────────────────────────────────────────────────────┘\r\n\r\nRECOMMENDATION: Use cursor-based for most APIs (Stripe, Slack, GitHub style)\r\n```\r\n\r\n### 2. Cursor-Based Pagination (Recommended)\r\n\r\n```json\r\n// Request\r\nGET /api/orders?limit=20&cursor=eyJjcmVhdGVkX2F0IjoiMjAyNi0wMS0xOVQxMDowMDowMFoiLCJpZCI6MTAwfQ\r\n\r\n// Response\r\n{\r\n  \"data\": [\r\n    { \"id\": 101, \"created_at\": \"2026-01-19T10:01:00Z\", ... },\r\n    { \"id\": 102, \"created_at\": \"2026-01-19T10:02:00Z\", ... }\r\n  ],\r\n  \"pagination\": {\r\n    \"has_more\": true,\r\n    \"next_cursor\": \"eyJjcmVhdGVkX2F0IjoiMjAyNi0wMS0xOVQxMDoyMDowMFoiLCJpZCI6MTIwfQ\",\r\n    \"prev_cursor\": \"eyJjcmVhdGVkX2F0IjoiMjAyNi0wMS0xOVQxMDowMTowMFoiLCJpZCI6MTAxfQ\"\r\n  },\r\n  \"links\": {\r\n    \"self\": \"/api/orders?limit=20&cursor=...\",\r\n    \"next\": \"/api/orders?limit=20&cursor=eyJ...\",\r\n    \"prev\": \"/api/orders?limit=20&cursor=eyJ...\"\r\n  }\r\n}\r\n```\r\n\r\n**Cursor encoding**: Base64 of `{ field: value, id: uniqueId }` - always include unique ID for tie-breaker.\r\n\r\n### 3. Filtering Best Practices\r\n\r\n```bash\r\n# Simple equality\r\nGET /api/products?status=active&category=electronics\r\n\r\n# Comparison operators (LHS brackets)\r\nGET /api/products?price[gte]=100&price[lte]=500\r\n\r\n# IN operator (comma-separated)\r\nGET /api/products?status=active,pending\r\n\r\n# Full-text search\r\nGET /api/products?q=wireless+headphones\r\n\r\n# Date ranges (ISO 8601)\r\nGET /api/orders?created_at[gte]=2026-01-01T00:00:00Z\r\n```\r\n\r\n**Security**: ALWAYS validate filter fields against an allowlist!\r\n\r\n```typescript\r\nconst ALLOWED_FILTERS = ['status', 'category', 'price', 'created_at'];\r\nconst ALLOWED_OPERATORS = ['eq', 'ne', 'gt', 'gte', 'lt', 'lte', 'in'];\r\n\r\nfunction validateFilters(filters: Record<string, any>) {\r\n  for (const [field, value] of Object.entries(filters)) {\r\n    const [fieldName, operator = 'eq'] = field.split('[').map(s => s.replace(']', ''));\r\n    \r\n    if (!ALLOWED_FILTERS.includes(fieldName)) {\r\n      throw new ApiError(400, `Invalid filter field: ${fieldName}`);\r\n    }\r\n    if (!ALLOWED_OPERATORS.includes(operator)) {\r\n      throw new ApiError(400, `Invalid operator: ${operator}`);\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n### 4. Sorting Best Practices\r\n\r\n```bash\r\n# Single field (- prefix for descending)\r\nGET /api/products?sort=-created_at\r\n\r\n# Multiple fields (comma-separated)\r\nGET /api/products?sort=-featured,price,-created_at\r\n\r\n# Alternative syntax (explicit direction)\r\nGET /api/products?sort_by=created_at&sort_order=desc\r\n```\r\n\r\n**Critical**: Only allow sorting on indexed columns!\r\n\r\n```typescript\r\nconst SORTABLE_FIELDS = new Map([\r\n  ['created_at', { index: 'idx_products_created_at' }],\r\n  ['price', { index: 'idx_products_price' }],\r\n  ['name', { index: 'idx_products_name' }],\r\n]);\r\n\r\nfunction validateSort(sortFields: string[]) {\r\n  for (const field of sortFields) {\r\n    const fieldName = field.replace(/^-/, '');\r\n    if (!SORTABLE_FIELDS.has(fieldName)) {\r\n      throw new ApiError(400, `Cannot sort by: ${fieldName}. Allowed: ${[...SORTABLE_FIELDS.keys()]}`);\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n### 5. Response Envelope\r\n\r\n```json\r\n{\r\n  \"data\": [...],\r\n  \"pagination\": {\r\n    \"limit\": 20,\r\n    \"has_more\": true,\r\n    \"total\": 1547,          // Optional - expensive for large datasets\r\n    \"next_cursor\": \"eyJ...\",\r\n    \"prev_cursor\": \"eyJ...\"\r\n  },\r\n  \"links\": {\r\n    \"self\": \"/api/products?limit=20\",\r\n    \"next\": \"/api/products?limit=20&cursor=eyJ...\",\r\n    \"prev\": null,\r\n    \"first\": \"/api/products?limit=20\"\r\n  },\r\n  \"meta\": {\r\n    \"filters_applied\": { \"status\": \"active\" },\r\n    \"sort_applied\": [\"-created_at\"]\r\n  }\r\n}\r\n```\r\n\r\n## Pitfalls\r\n\r\n| Pitfall | Impact | How to Avoid |\r\n|---------|--------|--------------|\r\n| No max page size | Memory exhaustion, DoS | Enforce hard limit (100 max) |\r\n| Offset on huge tables | Slow queries (OFFSET 1000000) | Use cursor/keyset pagination |\r\n| Filter on non-indexed field | Full table scan, timeout | Allowlist indexed fields only |\r\n| Sort on non-indexed field | Full table sort, slow | Validate against index list |\r\n| COUNT(*) on millions | Query timeout | Make total optional or use estimates |\r\n| Offset drift with inserts | Missing/duplicate items | Use cursor-based pagination |\r\n| Exposing internal IDs | Security concern | Use opaque cursors |\r\n| Null vs empty array | Client confusion | Always return `[]` for empty |\r\n\r\n## Checklist\r\n\r\n- [ ] Max page size enforced (≤100)\r\n- [ ] Default page size set (20-25)\r\n- [ ] Cursor-based pagination for mutable data\r\n- [ ] Filter fields validated against allowlist\r\n- [ ] Sort fields validated against indexed columns\r\n- [ ] Pagination metadata in response\r\n- [ ] HATEOAS links provided (self, next, prev)\r\n- [ ] `has_more` flag included (not just total)\r\n- [ ] Empty results return `[]`, not null\r\n- [ ] Cursors are opaque (base64 encoded)\r\n- [ ] Total count is optional or estimated\r\n- [ ] Documented in OpenAPI spec\r\n\r\n## Code Examples\r\n\r\n### TypeScript/Node.js\r\n\r\n```typescript\r\ninterface PaginationParams {\r\n  limit: number;\r\n  cursor?: string;\r\n  sort?: string[];\r\n  filters?: Record<string, any>;\r\n}\r\n\r\ninterface PaginatedResponse<T> {\r\n  data: T[];\r\n  pagination: {\r\n    limit: number;\r\n    has_more: boolean;\r\n    next_cursor: string | null;\r\n    prev_cursor: string | null;\r\n    total?: number;\r\n  };\r\n  links: {\r\n    self: string;\r\n    next: string | null;\r\n    prev: string | null;\r\n  };\r\n}\r\n\r\n// Cursor utilities\r\nfunction encodeCursor(data: Record<string, any>): string {\r\n  return Buffer.from(JSON.stringify(data)).toString('base64url');\r\n}\r\n\r\nfunction decodeCursor(cursor: string): Record<string, any> {\r\n  return JSON.parse(Buffer.from(cursor, 'base64url').toString());\r\n}\r\n\r\n// Pagination service\r\nasync function paginateQuery<T>(\r\n  query: QueryBuilder,\r\n  params: PaginationParams,\r\n  buildCursor: (item: T) => Record<string, any>\r\n): Promise<PaginatedResponse<T>> {\r\n  const limit = Math.min(params.limit || 20, 100); // Max 100\r\n  \r\n  // Fetch limit + 1 to determine has_more\r\n  const items = await query.limit(limit + 1).execute();\r\n  const hasMore = items.length > limit;\r\n  const data = hasMore ? items.slice(0, -1) : items;\r\n  \r\n  return {\r\n    data,\r\n    pagination: {\r\n      limit,\r\n      has_more: hasMore,\r\n      next_cursor: hasMore ? encodeCursor(buildCursor(data[data.length - 1])) : null,\r\n      prev_cursor: params.cursor ? encodeCursor(buildCursor(data[0])) : null,\r\n    },\r\n    links: {\r\n      self: buildLink(params),\r\n      next: hasMore ? buildLink({ ...params, cursor: encodeCursor(buildCursor(data[data.length - 1])) }) : null,\r\n      prev: params.cursor ? buildLink({ ...params, cursor: null }) : null,\r\n    },\r\n  };\r\n}\r\n\r\n// Usage in controller\r\napp.get('/api/orders', async (req, res) => {\r\n  const params = parsePaginationParams(req.query);\r\n  \r\n  validateFilters(params.filters, ALLOWED_ORDER_FILTERS);\r\n  validateSort(params.sort, SORTABLE_ORDER_FIELDS);\r\n  \r\n  let query = db('orders').select('*');\r\n  \r\n  // Apply cursor\r\n  if (params.cursor) {\r\n    const { created_at, id } = decodeCursor(params.cursor);\r\n    query = query.where(function() {\r\n      this.where('created_at', '<', created_at)\r\n        .orWhere(function() {\r\n          this.where('created_at', '=', created_at).where('id', '<', id);\r\n        });\r\n    });\r\n  }\r\n  \r\n  // Apply filters\r\n  if (params.filters?.status) {\r\n    query = query.where('status', params.filters.status);\r\n  }\r\n  \r\n  // Apply sort\r\n  query = query.orderBy('created_at', 'desc').orderBy('id', 'desc');\r\n  \r\n  const result = await paginateQuery(query, params, (order) => ({\r\n    created_at: order.created_at,\r\n    id: order.id,\r\n  }));\r\n  \r\n  res.json(result);\r\n});\r\n```\r\n\r\n### Python/FastAPI\r\n\r\n```python\r\nfrom typing import TypeVar, Generic, List, Optional, Any\r\nfrom pydantic import BaseModel\r\nfrom fastapi import Query\r\nimport base64\r\nimport json\r\n\r\nT = TypeVar('T')\r\n\r\nclass PaginationMeta(BaseModel):\r\n    limit: int\r\n    has_more: bool\r\n    next_cursor: Optional[str]\r\n    prev_cursor: Optional[str]\r\n    total: Optional[int] = None\r\n\r\nclass PaginatedResponse(BaseModel, Generic[T]):\r\n    data: List[T]\r\n    pagination: PaginationMeta\r\n    links: dict\r\n\r\ndef encode_cursor(data: dict) -> str:\r\n    return base64.urlsafe_b64encode(json.dumps(data).encode()).decode()\r\n\r\ndef decode_cursor(cursor: str) -> dict:\r\n    return json.loads(base64.urlsafe_b64decode(cursor.encode()).decode())\r\n\r\nALLOWED_FILTERS = {'status', 'category', 'created_at'}\r\nSORTABLE_FIELDS = {'created_at', 'price', 'name'}\r\n\r\n@app.get(\"/api/products\")\r\nasync def list_products(\r\n    limit: int = Query(default=20, le=100, ge=1),\r\n    cursor: Optional[str] = None,\r\n    sort: Optional[str] = Query(default=\"-created_at\"),\r\n    status: Optional[str] = None,\r\n    db: Session = Depends(get_db)\r\n):\r\n    # Validate sort\r\n    sort_fields = sort.split(',') if sort else ['-created_at']\r\n    for field in sort_fields:\r\n        field_name = field.lstrip('-')\r\n        if field_name not in SORTABLE_FIELDS:\r\n            raise HTTPException(400, f\"Cannot sort by: {field_name}\")\r\n    \r\n    query = db.query(Product)\r\n    \r\n    # Apply cursor\r\n    if cursor:\r\n        cursor_data = decode_cursor(cursor)\r\n        query = query.filter(\r\n            (Product.created_at < cursor_data['created_at']) |\r\n            ((Product.created_at == cursor_data['created_at']) & \r\n             (Product.id < cursor_data['id']))\r\n        )\r\n    \r\n    # Apply filters\r\n    if status:\r\n        query = query.filter(Product.status == status)\r\n    \r\n    # Apply sort\r\n    query = query.order_by(Product.created_at.desc(), Product.id.desc())\r\n    \r\n    # Fetch limit + 1\r\n    items = query.limit(limit + 1).all()\r\n    has_more = len(items) > limit\r\n    data = items[:limit] if has_more else items\r\n    \r\n    return PaginatedResponse(\r\n        data=data,\r\n        pagination=PaginationMeta(\r\n            limit=limit,\r\n            has_more=has_more,\r\n            next_cursor=encode_cursor({\r\n                'created_at': data[-1].created_at.isoformat(),\r\n                'id': data[-1].id\r\n            }) if has_more and data else None,\r\n            prev_cursor=None,\r\n        ),\r\n        links={...}\r\n    )\r\n```\r\n\r\n## References\r\n\r\n- [Slack API Pagination](https://api.slack.com/docs/pagination)\r\n- [Stripe API Pagination](https://stripe.com/docs/api/pagination)\r\n- [Zalando RESTful API Guidelines - Pagination](https://opensource.zalando.com/restful-api-guidelines/#pagination)\r\n- [GraphQL Cursor Connections Spec](https://relay.dev/graphql/connections.htm)\r\n- [Use The Index, Luke - Pagination Done Right](https://use-the-index-luke.com/no-offset)\r\n"
  },
  {
    "id": "api-pagination",
    "title": "API Pagination Patterns",
    "tags": [
      "api",
      "pagination",
      "performance",
      "cursor",
      "offset"
    ],
    "level": "intermediate",
    "stacks": [
      "nodejs",
      "python",
      "go"
    ],
    "scope": "api",
    "maturity": "stable",
    "works_with": [
      "nodejs",
      "python",
      "go"
    ],
    "path": "D:/Code/backend-engineering-kit/.shared/production-backend-kit/patterns/api.pagination.md",
    "type": "pattern",
    "sections": [],
    "checklist": [],
    "sources": [],
    "rawContent": "\r\n# API Pagination Patterns\r\n\r\n## Problem\r\n\r\nWithout pagination:\r\n```\r\nGET /api/orders → Returns 1,000,000 orders\r\n                 → 500MB response\r\n                 → 30 second query\r\n                 → OOM on client\r\n                 → OOM on server\r\n```\r\n\r\nLarge result sets cause:\r\n- Memory exhaustion\r\n- Slow response times\r\n- Network saturation\r\n- Database lock contention\r\n- Poor user experience\r\n\r\n## When to use\r\n\r\n- **Any** endpoint that returns collections\r\n- Lists that can grow unbounded\r\n- Search results\r\n- Activity feeds\r\n- Admin dashboards\r\n\r\n## Solution\r\n\r\n### 1. Offset Pagination (Simple)\r\n\r\n```typescript\r\n// URL: GET /api/products?page=2&limit=20\r\n\r\ninterface OffsetPaginationParams {\r\n  page: number;    // 1-indexed\r\n  limit: number;\r\n}\r\n\r\ninterface OffsetPaginatedResponse<T> {\r\n  data: T[];\r\n  pagination: {\r\n    page: number;\r\n    limit: number;\r\n    totalItems: number;\r\n    totalPages: number;\r\n    hasNextPage: boolean;\r\n    hasPreviousPage: boolean;\r\n  };\r\n}\r\n\r\nasync function getProductsWithOffset(\r\n  params: OffsetPaginationParams\r\n): Promise<OffsetPaginatedResponse<Product>> {\r\n  const { page = 1, limit = 20 } = params;\r\n  const offset = (page - 1) * limit;\r\n  \r\n  // Get total count\r\n  const [{ count }] = await db.query(\r\n    'SELECT COUNT(*) as count FROM products WHERE active = true'\r\n  );\r\n  \r\n  // Get page of results\r\n  const products = await db.query(\r\n    `SELECT * FROM products \r\n     WHERE active = true \r\n     ORDER BY created_at DESC \r\n     LIMIT $1 OFFSET $2`,\r\n    [limit, offset]\r\n  );\r\n  \r\n  const totalItems = parseInt(count, 10);\r\n  const totalPages = Math.ceil(totalItems / limit);\r\n  \r\n  return {\r\n    data: products,\r\n    pagination: {\r\n      page,\r\n      limit,\r\n      totalItems,\r\n      totalPages,\r\n      hasNextPage: page < totalPages,\r\n      hasPreviousPage: page > 1,\r\n    },\r\n  };\r\n}\r\n\r\n// Express handler\r\napp.get('/api/products', async (req, res) => {\r\n  const page = Math.max(1, parseInt(req.query.page as string) || 1);\r\n  const limit = Math.min(100, Math.max(1, parseInt(req.query.limit as string) || 20));\r\n  \r\n  const result = await getProductsWithOffset({ page, limit });\r\n  \r\n  // Set pagination headers\r\n  res.setHeader('X-Total-Count', result.pagination.totalItems);\r\n  res.setHeader('X-Total-Pages', result.pagination.totalPages);\r\n  \r\n  // Link header for navigation\r\n  const links = [];\r\n  if (result.pagination.hasNextPage) {\r\n    links.push(`<${req.path}?page=${page + 1}&limit=${limit}>; rel=\"next\"`);\r\n  }\r\n  if (result.pagination.hasPreviousPage) {\r\n    links.push(`<${req.path}?page=${page - 1}&limit=${limit}>; rel=\"prev\"`);\r\n  }\r\n  links.push(`<${req.path}?page=1&limit=${limit}>; rel=\"first\"`);\r\n  links.push(`<${req.path}?page=${result.pagination.totalPages}&limit=${limit}>; rel=\"last\"`);\r\n  res.setHeader('Link', links.join(', '));\r\n  \r\n  res.json(result);\r\n});\r\n```\r\n\r\n**Pros:**\r\n- Simple to understand\r\n- Random page access\r\n- Total count available\r\n\r\n**Cons:**\r\n- Slow for deep pages (`OFFSET 100000` is slow)\r\n- Inconsistent with concurrent inserts/deletes\r\n- Count query can be expensive\r\n\r\n### 2. Cursor Pagination (Recommended)\r\n\r\n```typescript\r\n// URL: GET /api/orders?cursor=abc123&limit=20\r\n\r\ninterface CursorPaginationParams {\r\n  cursor?: string;  // Opaque cursor from previous response\r\n  limit: number;\r\n}\r\n\r\ninterface CursorPaginatedResponse<T> {\r\n  data: T[];\r\n  pageInfo: {\r\n    hasNextPage: boolean;\r\n    hasPreviousPage: boolean;\r\n    startCursor: string | null;\r\n    endCursor: string | null;\r\n  };\r\n}\r\n\r\n// Cursor encoding (hide implementation details)\r\nfunction encodeCursor(data: { id: string; createdAt: Date }): string {\r\n  const payload = JSON.stringify({\r\n    id: data.id,\r\n    ts: data.createdAt.toISOString(),\r\n  });\r\n  return Buffer.from(payload).toString('base64url');\r\n}\r\n\r\nfunction decodeCursor(cursor: string): { id: string; createdAt: Date } | null {\r\n  try {\r\n    const payload = JSON.parse(Buffer.from(cursor, 'base64url').toString());\r\n    return {\r\n      id: payload.id,\r\n      createdAt: new Date(payload.ts),\r\n    };\r\n  } catch {\r\n    return null;\r\n  }\r\n}\r\n\r\nasync function getOrdersWithCursor(\r\n  params: CursorPaginationParams\r\n): Promise<CursorPaginatedResponse<Order>> {\r\n  const { cursor, limit = 20 } = params;\r\n  \r\n  let whereClause = 'WHERE 1=1';\r\n  const queryParams: any[] = [];\r\n  \r\n  if (cursor) {\r\n    const decoded = decodeCursor(cursor);\r\n    if (!decoded) {\r\n      throw new BadRequestError('Invalid cursor');\r\n    }\r\n    \r\n    // Seek method: (created_at, id) < (cursor_ts, cursor_id)\r\n    // Assuming created_at DESC order\r\n    whereClause += ` AND (created_at, id) < ($1, $2)`;\r\n    queryParams.push(decoded.createdAt, decoded.id);\r\n  }\r\n  \r\n  // Fetch limit + 1 to check if there are more results\r\n  const orders = await db.query(\r\n    `SELECT * FROM orders \r\n     ${whereClause}\r\n     ORDER BY created_at DESC, id DESC\r\n     LIMIT $${queryParams.length + 1}`,\r\n    [...queryParams, limit + 1]\r\n  );\r\n  \r\n  const hasNextPage = orders.length > limit;\r\n  const results = orders.slice(0, limit);\r\n  \r\n  return {\r\n    data: results,\r\n    pageInfo: {\r\n      hasNextPage,\r\n      hasPreviousPage: !!cursor,\r\n      startCursor: results.length > 0 \r\n        ? encodeCursor({ id: results[0].id, createdAt: results[0].createdAt })\r\n        : null,\r\n      endCursor: results.length > 0\r\n        ? encodeCursor({ id: results[results.length - 1].id, createdAt: results[results.length - 1].createdAt })\r\n        : null,\r\n    },\r\n  };\r\n}\r\n\r\n// Express handler\r\napp.get('/api/orders', async (req, res) => {\r\n  const cursor = req.query.cursor as string | undefined;\r\n  const limit = Math.min(100, Math.max(1, parseInt(req.query.limit as string) || 20));\r\n  \r\n  const result = await getOrdersWithCursor({ cursor, limit });\r\n  \r\n  res.json(result);\r\n});\r\n```\r\n\r\n**Pros:**\r\n- Consistent pagination (no skipped/duplicate items)\r\n- Fast for any page (uses index seek)\r\n- Works well with real-time data\r\n\r\n**Cons:**\r\n- No random page access\r\n- No total count\r\n- Slightly more complex\r\n\r\n### 3. Keyset Pagination (Database-Level)\r\n\r\n```typescript\r\n// Most efficient for large datasets\r\n// Uses indexed columns for seeking\r\n\r\ninterface KeysetParams {\r\n  after?: { createdAt: Date; id: string };\r\n  before?: { createdAt: Date; id: string };\r\n  first?: number;\r\n  last?: number;\r\n}\r\n\r\nasync function getOrdersKeyset(params: KeysetParams): Promise<Order[]> {\r\n  const { after, before, first, last } = params;\r\n  \r\n  // Build query based on direction\r\n  if (first && after) {\r\n    // Forward pagination\r\n    return db.query(\r\n      `SELECT * FROM orders\r\n       WHERE (created_at, id) > ($1, $2)\r\n       ORDER BY created_at ASC, id ASC\r\n       LIMIT $3`,\r\n      [after.createdAt, after.id, first]\r\n    );\r\n  }\r\n  \r\n  if (last && before) {\r\n    // Backward pagination\r\n    const results = await db.query(\r\n      `SELECT * FROM orders\r\n       WHERE (created_at, id) < ($1, $2)\r\n       ORDER BY created_at DESC, id DESC\r\n       LIMIT $3`,\r\n      [before.createdAt, before.id, last]\r\n    );\r\n    return results.reverse();  // Restore original order\r\n  }\r\n  \r\n  // Default: first page\r\n  return db.query(\r\n    `SELECT * FROM orders\r\n     ORDER BY created_at DESC, id DESC\r\n     LIMIT $1`,\r\n    [first || 20]\r\n  );\r\n}\r\n\r\n// Create index for efficient keyset pagination\r\n// CREATE INDEX idx_orders_pagination ON orders (created_at DESC, id DESC);\r\n```\r\n\r\n### 4. GraphQL Connections (Relay Spec)\r\n\r\n```typescript\r\n// Relay-style connections for GraphQL\r\n\r\nimport { \r\n  GraphQLObjectType, \r\n  GraphQLList, \r\n  GraphQLString, \r\n  GraphQLBoolean,\r\n  GraphQLInt \r\n} from 'graphql';\r\n\r\n// Edge type\r\nconst OrderEdge = new GraphQLObjectType({\r\n  name: 'OrderEdge',\r\n  fields: {\r\n    cursor: { type: GraphQLString },\r\n    node: { type: OrderType },\r\n  },\r\n});\r\n\r\n// PageInfo type\r\nconst PageInfo = new GraphQLObjectType({\r\n  name: 'PageInfo',\r\n  fields: {\r\n    hasNextPage: { type: GraphQLBoolean },\r\n    hasPreviousPage: { type: GraphQLBoolean },\r\n    startCursor: { type: GraphQLString },\r\n    endCursor: { type: GraphQLString },\r\n  },\r\n});\r\n\r\n// Connection type\r\nconst OrderConnection = new GraphQLObjectType({\r\n  name: 'OrderConnection',\r\n  fields: {\r\n    edges: { type: new GraphQLList(OrderEdge) },\r\n    pageInfo: { type: PageInfo },\r\n    totalCount: { type: GraphQLInt },  // Optional\r\n  },\r\n});\r\n\r\n// Resolver\r\nconst resolvers = {\r\n  Query: {\r\n    orders: async (_, { first, after, last, before }) => {\r\n      const orders = await getOrdersWithCursor({\r\n        first,\r\n        after: after ? decodeCursor(after) : undefined,\r\n        last,\r\n        before: before ? decodeCursor(before) : undefined,\r\n      });\r\n      \r\n      return {\r\n        edges: orders.data.map(order => ({\r\n          cursor: encodeCursor({ id: order.id, createdAt: order.createdAt }),\r\n          node: order,\r\n        })),\r\n        pageInfo: orders.pageInfo,\r\n      };\r\n    },\r\n  },\r\n};\r\n\r\n// Usage\r\n// query {\r\n//   orders(first: 10, after: \"abc123\") {\r\n//     edges {\r\n//       cursor\r\n//       node {\r\n//         id\r\n//         total\r\n//       }\r\n//     }\r\n//     pageInfo {\r\n//       hasNextPage\r\n//       endCursor\r\n//     }\r\n//   }\r\n// }\r\n```\r\n\r\n### 5. Search Results Pagination\r\n\r\n```typescript\r\n// For search with scoring/relevance\r\n\r\ninterface SearchParams {\r\n  query: string;\r\n  page: number;\r\n  limit: number;\r\n  filters?: Record<string, any>;\r\n  sort?: 'relevance' | 'date' | 'price';\r\n}\r\n\r\ninterface SearchResult<T> {\r\n  data: Array<T & { score?: number }>;\r\n  pagination: {\r\n    page: number;\r\n    limit: number;\r\n    totalResults: number;\r\n    totalPages: number;\r\n  };\r\n  meta: {\r\n    took: number;  // Query time in ms\r\n    query: string;\r\n    filters: Record<string, any>;\r\n  };\r\n}\r\n\r\nasync function searchProducts(params: SearchParams): Promise<SearchResult<Product>> {\r\n  const { query, page = 1, limit = 20, filters = {}, sort = 'relevance' } = params;\r\n  const startTime = Date.now();\r\n  \r\n  // Elasticsearch example\r\n  const searchBody: any = {\r\n    query: {\r\n      bool: {\r\n        must: [\r\n          {\r\n            multi_match: {\r\n              query,\r\n              fields: ['name^3', 'description', 'tags^2'],\r\n              fuzziness: 'AUTO',\r\n            },\r\n          },\r\n        ],\r\n        filter: Object.entries(filters).map(([field, value]) => ({\r\n          term: { [field]: value },\r\n        })),\r\n      },\r\n    },\r\n    from: (page - 1) * limit,\r\n    size: limit,\r\n    track_total_hits: true,  // Get accurate total\r\n  };\r\n  \r\n  // Add sorting\r\n  if (sort === 'relevance') {\r\n    // Use default _score\r\n  } else if (sort === 'date') {\r\n    searchBody.sort = [{ created_at: 'desc' }];\r\n  } else if (sort === 'price') {\r\n    searchBody.sort = [{ price: 'asc' }];\r\n  }\r\n  \r\n  const response = await esClient.search({\r\n    index: 'products',\r\n    body: searchBody,\r\n  });\r\n  \r\n  const totalResults = response.hits.total.value;\r\n  \r\n  return {\r\n    data: response.hits.hits.map(hit => ({\r\n      ...hit._source,\r\n      score: hit._score,\r\n    })),\r\n    pagination: {\r\n      page,\r\n      limit,\r\n      totalResults,\r\n      totalPages: Math.ceil(totalResults / limit),\r\n    },\r\n    meta: {\r\n      took: Date.now() - startTime,\r\n      query,\r\n      filters,\r\n    },\r\n  };\r\n}\r\n\r\n// Note: For deep pagination in Elasticsearch, use search_after\r\nasync function searchDeepPage(params: SearchParams & { searchAfter?: any[] }) {\r\n  const searchBody = {\r\n    // ... query\r\n    size: params.limit,\r\n    sort: [\r\n      { created_at: 'desc' },\r\n      { _id: 'asc' },  // Tiebreaker\r\n    ],\r\n  };\r\n  \r\n  if (params.searchAfter) {\r\n    searchBody.search_after = params.searchAfter;\r\n  }\r\n  \r\n  const response = await esClient.search({\r\n    index: 'products',\r\n    body: searchBody,\r\n  });\r\n  \r\n  const lastHit = response.hits.hits[response.hits.hits.length - 1];\r\n  \r\n  return {\r\n    data: response.hits.hits.map(hit => hit._source),\r\n    nextSearchAfter: lastHit?.sort,  // Use for next page\r\n  };\r\n}\r\n```\r\n\r\n### 6. Infinite Scroll / Load More\r\n\r\n```typescript\r\n// Client-side implementation for infinite scroll\r\n\r\n// API response\r\ninterface InfiniteScrollResponse<T> {\r\n  items: T[];\r\n  nextCursor: string | null;\r\n  hasMore: boolean;\r\n}\r\n\r\n// Server handler\r\napp.get('/api/feed', async (req, res) => {\r\n  const cursor = req.query.cursor as string | undefined;\r\n  const limit = 10;\r\n  \r\n  const items = await getFeedItems({\r\n    cursor: cursor ? decodeCursor(cursor) : undefined,\r\n    limit: limit + 1,\r\n  });\r\n  \r\n  const hasMore = items.length > limit;\r\n  const resultItems = items.slice(0, limit);\r\n  \r\n  const response: InfiniteScrollResponse<FeedItem> = {\r\n    items: resultItems,\r\n    nextCursor: hasMore && resultItems.length > 0\r\n      ? encodeCursor({\r\n          id: resultItems[resultItems.length - 1].id,\r\n          createdAt: resultItems[resultItems.length - 1].createdAt,\r\n        })\r\n      : null,\r\n    hasMore,\r\n  };\r\n  \r\n  res.json(response);\r\n});\r\n\r\n// React hook for client\r\nfunction useInfiniteScroll<T>(\r\n  fetchFn: (cursor?: string) => Promise<InfiniteScrollResponse<T>>\r\n) {\r\n  const [items, setItems] = useState<T[]>([]);\r\n  const [nextCursor, setNextCursor] = useState<string | null>(null);\r\n  const [hasMore, setHasMore] = useState(true);\r\n  const [loading, setLoading] = useState(false);\r\n  \r\n  const loadMore = useCallback(async () => {\r\n    if (loading || !hasMore) return;\r\n    \r\n    setLoading(true);\r\n    try {\r\n      const response = await fetchFn(nextCursor || undefined);\r\n      setItems(prev => [...prev, ...response.items]);\r\n      setNextCursor(response.nextCursor);\r\n      setHasMore(response.hasMore);\r\n    } finally {\r\n      setLoading(false);\r\n    }\r\n  }, [nextCursor, hasMore, loading, fetchFn]);\r\n  \r\n  // Initial load\r\n  useEffect(() => {\r\n    loadMore();\r\n  }, []);\r\n  \r\n  return { items, loadMore, hasMore, loading };\r\n}\r\n```\r\n\r\n### 7. Batch Export / Streaming\r\n\r\n```typescript\r\n// For exporting large datasets\r\n\r\nimport { Readable } from 'stream';\r\n\r\n// Stream large result set\r\napp.get('/api/orders/export', async (req, res) => {\r\n  res.setHeader('Content-Type', 'application/json');\r\n  res.setHeader('Content-Disposition', 'attachment; filename=\"orders.json\"');\r\n  \r\n  // Stream array start\r\n  res.write('[\\n');\r\n  \r\n  let cursor: string | undefined;\r\n  let isFirst = true;\r\n  const batchSize = 1000;\r\n  \r\n  while (true) {\r\n    const batch = await getOrdersBatch({ cursor, limit: batchSize });\r\n    \r\n    for (const order of batch.data) {\r\n      if (!isFirst) {\r\n        res.write(',\\n');\r\n      }\r\n      res.write(JSON.stringify(order));\r\n      isFirst = false;\r\n    }\r\n    \r\n    if (!batch.pageInfo.hasNextPage) {\r\n      break;\r\n    }\r\n    \r\n    cursor = batch.pageInfo.endCursor!;\r\n  }\r\n  \r\n  // Stream array end\r\n  res.write('\\n]');\r\n  res.end();\r\n});\r\n\r\n// CSV streaming\r\napp.get('/api/orders/export.csv', async (req, res) => {\r\n  res.setHeader('Content-Type', 'text/csv');\r\n  res.setHeader('Content-Disposition', 'attachment; filename=\"orders.csv\"');\r\n  \r\n  // Write headers\r\n  res.write('id,customer_email,total,created_at\\n');\r\n  \r\n  let cursor: string | undefined;\r\n  \r\n  while (true) {\r\n    const batch = await getOrdersBatch({ cursor, limit: 1000 });\r\n    \r\n    for (const order of batch.data) {\r\n      res.write(`${order.id},${order.customerEmail},${order.total},${order.createdAt}\\n`);\r\n    }\r\n    \r\n    if (!batch.pageInfo.hasNextPage) break;\r\n    cursor = batch.pageInfo.endCursor!;\r\n  }\r\n  \r\n  res.end();\r\n});\r\n```\r\n\r\n### 8. Total Count Optimization\r\n\r\n```typescript\r\n// Avoid expensive COUNT(*) on every request\r\n\r\n// Option 1: Cache count\r\nasync function getCachedTotalCount(entity: string): Promise<number> {\r\n  const cached = await redis.get(`count:${entity}`);\r\n  if (cached) return parseInt(cached, 10);\r\n  \r\n  const [{ count }] = await db.query(`SELECT COUNT(*) as count FROM ${entity}`);\r\n  await redis.setex(`count:${entity}`, 60, count);  // Cache for 1 minute\r\n  \r\n  return parseInt(count, 10);\r\n}\r\n\r\n// Option 2: Approximate count\r\nasync function getApproximateCount(table: string): Promise<number> {\r\n  // PostgreSQL-specific: Use statistics\r\n  const [{ estimate }] = await db.query(\r\n    `SELECT reltuples::bigint AS estimate \r\n     FROM pg_class \r\n     WHERE relname = $1`,\r\n    [table]\r\n  );\r\n  return parseInt(estimate, 10);\r\n}\r\n\r\n// Option 3: Return total only on first page\r\ninterface PaginatedResponse<T> {\r\n  data: T[];\r\n  pageInfo: {\r\n    hasNextPage: boolean;\r\n    endCursor: string | null;\r\n    totalCount?: number;  // Only on first page\r\n  };\r\n}\r\n\r\nasync function getItems(params: {\r\n  cursor?: string;\r\n  limit: number;\r\n  includeTotalCount?: boolean;\r\n}): Promise<PaginatedResponse<Item>> {\r\n  const items = await fetchItems(params.cursor, params.limit);\r\n  \r\n  let totalCount: number | undefined;\r\n  if (params.includeTotalCount && !params.cursor) {\r\n    // Only count on first page, and cache result\r\n    totalCount = await getCachedTotalCount('items');\r\n  }\r\n  \r\n  return {\r\n    data: items,\r\n    pageInfo: {\r\n      hasNextPage: items.length === params.limit,\r\n      endCursor: items.length > 0 ? encodeCursor(items[items.length - 1]) : null,\r\n      totalCount,\r\n    },\r\n  };\r\n}\r\n```\r\n\r\n## Pitfalls\r\n\r\n| Pitfall | Impact | How to Avoid |\r\n|---------|--------|--------------|\r\n| No pagination | OOM, slow responses | Always paginate lists |\r\n| No max limit | Client can request all | Set server-side max |\r\n| Offset for large data | Slow queries | Use cursor/keyset |\r\n| COUNT(*) on every request | Database load | Cache or approximate |\r\n| Inconsistent sorting | Duplicates/missing items | Include unique tiebreaker |\r\n| Exposing internal IDs | Information leak | Use opaque cursors |\r\n\r\n## Checklist\r\n\r\n- [ ] All list endpoints are paginated\r\n- [ ] Maximum limit enforced server-side\r\n- [ ] Default limit is reasonable (10-50)\r\n- [ ] Cursor pagination for large/real-time data\r\n- [ ] Pagination metadata in response\r\n- [ ] Link headers for REST navigation\r\n- [ ] Total count cached or approximate\r\n- [ ] Unique sort tiebreaker (e.g., ID)\r\n- [ ] Streaming for large exports\r\n- [ ] Client library handles pagination\r\n\r\n## References\r\n\r\n- [Slack: Evolving API Pagination](https://slack.engineering/evolving-api-pagination-at-slack/)\r\n- [Relay Cursor Connections](https://relay.dev/graphql/connections.htm)\r\n- [Facebook Graph API Paging](https://developers.facebook.com/docs/graph-api/using-graph-api#paging)\r\n- [Use the Index, Luke: Pagination](https://use-the-index-luke.com/sql/partial-results/fetch-next-page)\r\n"
  },
  {
    "id": "api-request-validation",
    "title": "Request Validation & Sanitization",
    "tags": [
      "api",
      "validation",
      "security",
      "input",
      "owasp"
    ],
    "level": "beginner",
    "stacks": [
      "all"
    ],
    "scope": "api",
    "maturity": "stable",
    "works_with": [
      "all"
    ],
    "path": "D:/Code/backend-engineering-kit/.shared/production-backend-kit/patterns/api.request-validation.md",
    "type": "pattern",
    "sections": [],
    "checklist": [],
    "sources": [],
    "rawContent": "\r\n# Request Validation & Sanitization\r\n\r\n## Problem\r\n\r\nAccepting unvalidated input is the **#1 cause of security vulnerabilities**:\r\n- SQL Injection (OWASP #3)\r\n- Cross-Site Scripting (OWASP #7)\r\n- Data corruption and crashes\r\n- Business logic bypass\r\n- DoS via malformed input\r\n\r\n**Trust nothing from the client. Validate everything.**\r\n\r\n## When to use\r\n\r\n- **Every single API endpoint** - no exceptions\r\n- Request body (JSON, form data, XML)\r\n- Path parameters (`/users/:id`)\r\n- Query parameters (`?search=...`)\r\n- Headers (Authorization, Content-Type)\r\n- File uploads (name, type, size, content)\r\n- Cookies and session data\r\n\r\n## Solution\r\n\r\n### 1. Validation Strategy: Defense in Depth\r\n\r\n```\r\n┌────────────────────────────────────────────────────────────┐\r\n│ Layer 1: Schema Validation                                 │\r\n│   • Type checking (string, number, boolean, array)         │\r\n│   • Required vs optional fields                            │\r\n│   • Format validation (email, UUID, URL, date)             │\r\n│   • Range/length constraints                               │\r\n└────────────────────────────────────────────────────────────┘\r\n                          ▼\r\n┌────────────────────────────────────────────────────────────┐\r\n│ Layer 2: Business Rule Validation                          │\r\n│   • Cross-field validation                                 │\r\n│   • Reference integrity (does user_id exist?)              │\r\n│   • State validation (can this order be cancelled?)        │\r\n│   • Permission checks                                      │\r\n└────────────────────────────────────────────────────────────┘\r\n                          ▼\r\n┌────────────────────────────────────────────────────────────┐\r\n│ Layer 3: Sanitization & Encoding                           │\r\n│   • HTML encoding for display                              │\r\n│   • SQL parameterization (never string concat!)            │\r\n│   • Path traversal prevention                              │\r\n│   • Unicode normalization                                  │\r\n└────────────────────────────────────────────────────────────┘\r\n```\r\n\r\n### 2. Validation Rules by Type\r\n\r\n| Input Type | Validation Rules |\r\n|------------|------------------|\r\n| **Email** | RFC 5322 format, max 254 chars, lowercase normalize |\r\n| **Password** | Min 8 chars, max 128, no leading/trailing whitespace |\r\n| **Username** | Alphanumeric + underscore, 3-30 chars, no reserved words |\r\n| **UUID** | RFC 4122 format (v4 preferred) |\r\n| **URL** | Valid protocol (https only for external), max 2048 chars |\r\n| **Phone** | E.164 format (+1234567890), 7-15 digits |\r\n| **Date** | ISO 8601 (YYYY-MM-DD), valid calendar date |\r\n| **Integer** | Within safe range, no leading zeros |\r\n| **Currency** | Positive, max 2 decimal places, within limits |\r\n| **File** | Allowlist extensions, MIME type check, max size, scan content |\r\n\r\n### 3. The Allowlist Principle (CRITICAL)\r\n\r\n**NEVER use blocklists. ALWAYS use allowlists.**\r\n\r\n```typescript\r\n// ❌ BAD: Blocklist approach\r\nfunction validateUsername(input: string): boolean {\r\n  const blocked = ['admin', 'root', 'system'];\r\n  return !blocked.includes(input.toLowerCase()); // Attacker uses \"ádmin\"\r\n}\r\n\r\n// ✅ GOOD: Allowlist approach\r\nfunction validateUsername(input: string): boolean {\r\n  const pattern = /^[a-zA-Z0-9_]{3,30}$/;\r\n  return pattern.test(input); // Only allows exactly what's expected\r\n}\r\n```\r\n\r\n### 4. Common Validation Patterns (Regex)\r\n\r\n```typescript\r\nconst PATTERNS = {\r\n  // Email (simplified but effective)\r\n  email: /^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$/,\r\n  \r\n  // UUID v4\r\n  uuidV4: /^[0-9a-f]{8}-[0-9a-f]{4}-4[0-9a-f]{3}-[89ab][0-9a-f]{3}-[0-9a-f]{12}$/i,\r\n  \r\n  // ISO 8601 Date\r\n  isoDate: /^\\d{4}-(?:0[1-9]|1[0-2])-(?:0[1-9]|[12]\\d|3[01])$/,\r\n  \r\n  // Alphanumeric with underscore\r\n  alphanumericUnderscore: /^[a-zA-Z0-9_]+$/,\r\n  \r\n  // Slug (URL-safe)\r\n  slug: /^[a-z0-9]+(?:-[a-z0-9]+)*$/,\r\n  \r\n  // Phone (E.164)\r\n  phoneE164: /^\\+[1-9]\\d{6,14}$/,\r\n  \r\n  // Strong password (min 8, upper, lower, digit, special)\r\n  strongPassword: /^(?=.*[a-z])(?=.*[A-Z])(?=.*\\d)(?=.*[@$!%*?&])[A-Za-z\\d@$!%*?&]{8,}$/,\r\n  \r\n  // Credit card (Luhn validation needed separately)\r\n  creditCard: /^\\d{13,19}$/,\r\n  \r\n  // IPv4\r\n  ipv4: /^(?:(?:25[0-5]|2[0-4]\\d|[01]?\\d\\d?)\\.){3}(?:25[0-5]|2[0-4]\\d|[01]?\\d\\d?)$/,\r\n};\r\n```\r\n\r\n## Pitfalls\r\n\r\n| Pitfall | Impact | How to Avoid |\r\n|---------|--------|--------------|\r\n| Client-side only validation | Bypassed trivially | ALWAYS validate server-side |\r\n| Using blocklists | Attacker finds bypass | Use allowlists exclusively |\r\n| Trusting Content-Type | File upload attacks | Validate actual content |\r\n| SQL string concatenation | SQL Injection | Use parameterized queries |\r\n| Reflecting input in HTML | XSS attacks | HTML encode output |\r\n| No length limits | DoS via huge payloads | Set max lengths on all fields |\r\n| Unicode bypass | Security filter bypass | Normalize before validation |\r\n| Generic error messages | User frustration | Return field-specific errors |\r\n\r\n## Checklist\r\n\r\n- [ ] Schema validation on ALL endpoints (no exceptions)\r\n- [ ] Validation library used (Zod, Joi, Pydantic, class-validator)\r\n- [ ] All input sources validated (body, params, query, headers)\r\n- [ ] Allowlist approach for all pattern matching\r\n- [ ] Type coercion handled safely (string \"true\" → boolean)\r\n- [ ] Length limits on all string fields\r\n- [ ] Range limits on all numeric fields\r\n- [ ] Enum values validated against allowlist\r\n- [ ] File uploads validated (type, size, content)\r\n- [ ] SQL uses parameterized queries (never concat)\r\n- [ ] HTML output is encoded\r\n- [ ] Unicode is normalized before validation\r\n- [ ] Field-level errors returned to client\r\n- [ ] Validation errors logged (for security monitoring)\r\n- [ ] Validation happens BEFORE business logic\r\n\r\n## Code Examples\r\n\r\n### TypeScript/Node.js (Zod)\r\n\r\n```typescript\r\nimport { z } from 'zod';\r\n\r\n// Define schemas\r\nconst CreateUserSchema = z.object({\r\n  email: z.string()\r\n    .email('Invalid email format')\r\n    .max(254, 'Email too long')\r\n    .toLowerCase()\r\n    .trim(),\r\n  \r\n  password: z.string()\r\n    .min(8, 'Password must be at least 8 characters')\r\n    .max(128, 'Password too long')\r\n    .regex(\r\n      /^(?=.*[a-z])(?=.*[A-Z])(?=.*\\d)/,\r\n      'Password must contain uppercase, lowercase, and number'\r\n    ),\r\n  \r\n  username: z.string()\r\n    .min(3, 'Username must be at least 3 characters')\r\n    .max(30, 'Username too long')\r\n    .regex(/^[a-zA-Z0-9_]+$/, 'Username can only contain letters, numbers, and underscores')\r\n    .refine(\r\n      (val) => !['admin', 'root', 'system', 'null'].includes(val.toLowerCase()),\r\n      'This username is reserved'\r\n    ),\r\n  \r\n  age: z.number()\r\n    .int('Age must be a whole number')\r\n    .min(13, 'Must be at least 13 years old')\r\n    .max(120, 'Invalid age')\r\n    .optional(),\r\n  \r\n  role: z.enum(['user', 'moderator']).default('user'),\r\n});\r\n\r\ntype CreateUserInput = z.infer<typeof CreateUserSchema>;\r\n\r\n// Validation middleware\r\nfunction validateBody<T>(schema: z.ZodSchema<T>) {\r\n  return (req: Request, res: Response, next: NextFunction) => {\r\n    const result = schema.safeParse(req.body);\r\n    \r\n    if (!result.success) {\r\n      const errors = result.error.errors.map(err => ({\r\n        field: err.path.join('.'),\r\n        code: err.code,\r\n        message: err.message,\r\n      }));\r\n      \r\n      return res.status(400).json({\r\n        type: 'https://api.example.com/errors/validation',\r\n        title: 'Validation Error',\r\n        status: 400,\r\n        detail: 'Request body failed validation',\r\n        errors,\r\n      });\r\n    }\r\n    \r\n    req.validatedBody = result.data;\r\n    next();\r\n  };\r\n}\r\n\r\n// Path params validation\r\nconst UserIdParamSchema = z.object({\r\n  id: z.string().uuid('Invalid user ID format'),\r\n});\r\n\r\n// Query params validation\r\nconst ListUsersQuerySchema = z.object({\r\n  page: z.coerce.number().int().min(1).default(1),\r\n  limit: z.coerce.number().int().min(1).max(100).default(20),\r\n  status: z.enum(['active', 'inactive', 'pending']).optional(),\r\n  sort: z.enum(['created_at', 'name', 'email']).default('created_at'),\r\n  order: z.enum(['asc', 'desc']).default('desc'),\r\n});\r\n\r\n// Usage\r\napp.post('/users', validateBody(CreateUserSchema), async (req, res) => {\r\n  const data: CreateUserInput = req.validatedBody;\r\n  // data is fully typed and validated\r\n  const user = await userService.create(data);\r\n  res.status(201).json(user);\r\n});\r\n\r\napp.get('/users/:id', validateParams(UserIdParamSchema), async (req, res) => {\r\n  const { id } = req.validatedParams;\r\n  // id is guaranteed to be a valid UUID\r\n});\r\n```\r\n\r\n### Python/FastAPI (Pydantic)\r\n\r\n```python\r\nfrom pydantic import BaseModel, EmailStr, Field, field_validator\r\nfrom typing import Optional, Literal\r\nfrom datetime import date\r\nimport re\r\n\r\nclass CreateUserRequest(BaseModel):\r\n    email: EmailStr = Field(..., max_length=254)\r\n    \r\n    password: str = Field(\r\n        ...,\r\n        min_length=8,\r\n        max_length=128,\r\n        description=\"Must contain uppercase, lowercase, and number\"\r\n    )\r\n    \r\n    username: str = Field(\r\n        ...,\r\n        min_length=3,\r\n        max_length=30,\r\n        pattern=r'^[a-zA-Z0-9_]+$'\r\n    )\r\n    \r\n    age: Optional[int] = Field(None, ge=13, le=120)\r\n    role: Literal['user', 'moderator'] = 'user'\r\n    \r\n    @field_validator('password')\r\n    @classmethod\r\n    def validate_password_strength(cls, v: str) -> str:\r\n        if not re.search(r'[a-z]', v):\r\n            raise ValueError('Password must contain lowercase letter')\r\n        if not re.search(r'[A-Z]', v):\r\n            raise ValueError('Password must contain uppercase letter')\r\n        if not re.search(r'\\d', v):\r\n            raise ValueError('Password must contain a digit')\r\n        return v\r\n    \r\n    @field_validator('username')\r\n    @classmethod\r\n    def validate_username_not_reserved(cls, v: str) -> str:\r\n        reserved = {'admin', 'root', 'system', 'null', 'undefined'}\r\n        if v.lower() in reserved:\r\n            raise ValueError('This username is reserved')\r\n        return v\r\n    \r\n    @field_validator('email')\r\n    @classmethod\r\n    def normalize_email(cls, v: str) -> str:\r\n        return v.lower().strip()\r\n\r\nclass ListUsersQuery(BaseModel):\r\n    page: int = Field(default=1, ge=1)\r\n    limit: int = Field(default=20, ge=1, le=100)\r\n    status: Optional[Literal['active', 'inactive', 'pending']] = None\r\n    sort: Literal['created_at', 'name', 'email'] = 'created_at'\r\n    order: Literal['asc', 'desc'] = 'desc'\r\n\r\n@app.post(\"/users\", status_code=201)\r\nasync def create_user(user: CreateUserRequest):\r\n    # user is already validated by Pydantic\r\n    return await user_service.create(user.model_dump())\r\n\r\n@app.get(\"/users/{user_id}\")\r\nasync def get_user(user_id: UUID):\r\n    # FastAPI validates UUID format automatically\r\n    return await user_service.get(user_id)\r\n\r\n@app.get(\"/users\")\r\nasync def list_users(query: Annotated[ListUsersQuery, Query()]):\r\n    # All query params validated\r\n    return await user_service.list(\r\n        page=query.page,\r\n        limit=query.limit,\r\n        filters={'status': query.status},\r\n        sort=query.sort,\r\n        order=query.order\r\n    )\r\n```\r\n\r\n### Go\r\n\r\n```go\r\npackage validation\r\n\r\nimport (\r\n    \"github.com/go-playground/validator/v10\"\r\n    \"regexp\"\r\n)\r\n\r\nvar validate = validator.New()\r\n\r\ntype CreateUserRequest struct {\r\n    Email    string `json:\"email\" validate:\"required,email,max=254\"`\r\n    Password string `json:\"password\" validate:\"required,min=8,max=128,strongpassword\"`\r\n    Username string `json:\"username\" validate:\"required,min=3,max=30,alphanumund,notreserved\"`\r\n    Age      *int   `json:\"age,omitempty\" validate:\"omitempty,min=13,max=120\"`\r\n    Role     string `json:\"role\" validate:\"omitempty,oneof=user moderator\"`\r\n}\r\n\r\nfunc init() {\r\n    // Register custom validators\r\n    validate.RegisterValidation(\"strongpassword\", validateStrongPassword)\r\n    validate.RegisterValidation(\"alphanumund\", validateAlphanumUnderscore)\r\n    validate.RegisterValidation(\"notreserved\", validateNotReserved)\r\n}\r\n\r\nfunc validateStrongPassword(fl validator.FieldLevel) bool {\r\n    password := fl.Field().String()\r\n    hasLower := regexp.MustCompile(`[a-z]`).MatchString(password)\r\n    hasUpper := regexp.MustCompile(`[A-Z]`).MatchString(password)\r\n    hasDigit := regexp.MustCompile(`\\d`).MatchString(password)\r\n    return hasLower && hasUpper && hasDigit\r\n}\r\n\r\nfunc validateAlphanumUnderscore(fl validator.FieldLevel) bool {\r\n    return regexp.MustCompile(`^[a-zA-Z0-9_]+$`).MatchString(fl.Field().String())\r\n}\r\n\r\nfunc validateNotReserved(fl validator.FieldLevel) bool {\r\n    reserved := map[string]bool{\r\n        \"admin\": true, \"root\": true, \"system\": true, \"null\": true,\r\n    }\r\n    return !reserved[strings.ToLower(fl.Field().String())]\r\n}\r\n\r\nfunc ValidateStruct(s interface{}) []FieldError {\r\n    err := validate.Struct(s)\r\n    if err == nil {\r\n        return nil\r\n    }\r\n    \r\n    var errors []FieldError\r\n    for _, e := range err.(validator.ValidationErrors) {\r\n        errors = append(errors, FieldError{\r\n            Field:   e.Field(),\r\n            Code:    e.Tag(),\r\n            Message: getErrorMessage(e),\r\n        })\r\n    }\r\n    return errors\r\n}\r\n```\r\n\r\n## References\r\n\r\n- [OWASP Input Validation Cheat Sheet](https://cheatsheetseries.owasp.org/cheatsheets/Input_Validation_Cheat_Sheet.html)\r\n- [OWASP Validation Regex Repository](https://owasp.org/www-community/OWASP_Validation_Regex_Repository)\r\n- [Zod Documentation](https://zod.dev/)\r\n- [Pydantic Documentation](https://docs.pydantic.dev/)\r\n- [Go Validator](https://github.com/go-playground/validator)\r\n"
  },
  {
    "id": "api-versioning",
    "title": "API Versioning",
    "tags": [
      "api",
      "versioning",
      "rest",
      "backward-compatibility",
      "evolution"
    ],
    "level": "intermediate",
    "stacks": [
      "all"
    ],
    "scope": "api",
    "maturity": "stable",
    "works_with": [
      "all"
    ],
    "path": "D:/Code/backend-engineering-kit/.shared/production-backend-kit/patterns/api.versioning.md",
    "type": "pattern",
    "sections": [],
    "checklist": [],
    "sources": [],
    "rawContent": "\r\n# API Versioning\r\n\r\n## Problem\r\n\r\nAPIs evolve over time. Without versioning, breaking changes disrupt existing clients. Poor versioning strategy leads to maintenance burden and client frustration. The challenge is balancing evolution with stability.\r\n\r\n## When to use\r\n\r\n- Public APIs consumed by third parties\r\n- Internal APIs with multiple consumer teams\r\n- When breaking changes are anticipated\r\n- Long-lived APIs requiring evolution\r\n- Microservices with independent deployment\r\n- APIs with SLA/contract requirements\r\n\r\n## Solution\r\n\r\n### 1. Choose Versioning Strategy\r\n\r\n| Strategy | Example | Pros | Cons |\r\n|----------|---------|------|------|\r\n| **URL Path** | `/v1/users` | Explicit, cacheable, easy routing | URL changes, not RESTful purist |\r\n| **Header** | `Accept: application/vnd.api+json;version=1` | Clean URLs, content negotiation | Hidden, harder to test |\r\n| **Query Param** | `/users?version=1` | Easy to add | Caching issues, less clean |\r\n| **Media Type** | `Accept: application/vnd.company.v1+json` | Full content negotiation | Complex, verbose |\r\n\r\n**Recommendation**: URL path versioning (`/v1/`) for most cases - explicit and cache-friendly.\r\n\r\n### 2. Define What's Breaking vs Non-Breaking\r\n\r\n**Non-Breaking (Safe)**:\r\n- Adding new endpoints\r\n- Adding optional request fields\r\n- Adding response fields\r\n- Adding new enum values (if client handles unknown)\r\n- Relaxing validation (accepting more)\r\n\r\n**Breaking (Requires New Version)**:\r\n- Removing/renaming endpoints\r\n- Removing/renaming fields\r\n- Changing field types\r\n- Changing required/optional\r\n- Tightening validation (rejecting previously valid)\r\n- Changing error formats\r\n- Changing authentication\r\n\r\n### 3. Version Lifecycle Management\r\n\r\n```\r\nAlpha (v1-alpha) → Beta (v1-beta) → GA (v1) → Deprecated → Sunset\r\n    ↓                   ↓              ↓           ↓           ↓\r\n  Unstable          Breaking OK    Stable    6-12 months    Removed\r\n```\r\n\r\n### 4. Deprecation Headers (RFC 8594)\r\n\r\n```http\r\nDeprecation: true\r\nDeprecation: @1735689600\r\nSunset: Sat, 01 Jan 2028 00:00:00 GMT\r\nLink: </v2/users>; rel=\"successor-version\"\r\n```\r\n\r\n### 5. Minimize Breaking Changes\r\n\r\n- **Add, don't remove**: New fields, new endpoints\r\n- **Make new fields optional**: Backward compatible\r\n- **Use feature flags**: Gradual rollout\r\n- **Nullable over removal**: Mark deprecated, return null\r\n- **Expand-contract pattern**: Add new → migrate → remove old\r\n\r\n## Pitfalls\r\n\r\n| Pitfall | How to Avoid |\r\n|---------|--------------|\r\n| Too many versions | Limit to 2-3 active versions max |\r\n| Silent breaking changes | Use API contracts, run compatibility tests |\r\n| No deprecation notice | Add headers, notify consumers proactively |\r\n| Header versioning caching issues | URL versioning is more cache-friendly |\r\n| Versioning internal-only APIs | Consider if you really need versioning |\r\n\r\n## Checklist\r\n\r\n- [ ] Versioning strategy documented\r\n- [ ] Version included in all API routes\r\n- [ ] Deprecation timeline defined (6-12 months)\r\n- [ ] Deprecation header returned for old versions\r\n- [ ] Changelog maintained per version\r\n- [ ] Breaking changes trigger major version bump\r\n- [ ] SDKs versioned alongside API\r\n- [ ] Monitoring tracks version adoption\r\n- [ ] Migration guides available\r\n- [ ] Sunset date communicated clearly\r\n\r\n## Snippets (Generic)\r\n\r\n```\r\nURL Path Versioning:\r\nGET /v1/users          # Version 1\r\nGET /v2/users          # Version 2\r\n\r\nDeprecation Headers:\r\nDeprecation: true\r\nSunset: Sat, 01 Jun 2027 00:00:00 GMT\r\nLink: </v2/users>; rel=\"successor-version\"\r\n\r\nVersion Lifecycle:\r\n1. v1 (stable) ──┐\r\n2. v2 (beta)     │ overlap period (6-12 months)\r\n3. v1 deprecated │\r\n4. v1 sunset ────┘\r\n5. v2 (stable)\r\n\r\nSteps:\r\n1. Define versioning strategy in API guidelines\r\n2. Implement version routing (middleware/gateway)\r\n3. Add deprecation header middleware\r\n4. Set up version adoption metrics\r\n5. Document migration for each major version\r\n```\r\n\r\n## Sources\r\n\r\n- Stripe API Versioning: https://stripe.com/blog/api-versioning\r\n- Microsoft REST API Guidelines - Versioning: https://github.com/microsoft/api-guidelines\r\n- Google API Design Guide - Versioning: https://cloud.google.com/apis/design/versioning\r\n- Zalando RESTful API Guidelines - Compatibility: https://opensource.zalando.com/restful-api-guidelines/\r\n"
  },
  {
    "id": "api-webhooks-signatures",
    "title": "Webhook Signatures",
    "tags": [
      "api",
      "webhooks",
      "security",
      "signatures"
    ],
    "level": "intermediate",
    "stacks": [
      "all"
    ],
    "scope": "api",
    "maturity": "stable",
    "works_with": [
      "all"
    ],
    "path": "D:/Code/backend-engineering-kit/.shared/production-backend-kit/patterns/api.webhooks-signatures.md",
    "type": "pattern",
    "sections": [],
    "checklist": [],
    "sources": [],
    "rawContent": "\r\n# Webhook Signatures\r\n\r\n## Problem\r\n\r\nWebhooks can be spoofed by attackers sending fake payloads to your endpoints. Without signature verification, your system may process malicious or forged events.\r\n\r\n## When to use\r\n\r\n- Receiving webhooks from any external service\r\n- Building webhook delivery systems\r\n- Payment processor callbacks\r\n- GitHub/GitLab event hooks\r\n- Any event-driven integration\r\n\r\n## Solution\r\n\r\n1. **Signing (sender side)**\r\n   - Generate HMAC-SHA256 of payload\r\n   - Use shared secret per consumer\r\n   - Include timestamp to prevent replay\r\n   - Send signature in header\r\n\r\n2. **Verification (receiver side)**\r\n   - Extract signature from header\r\n   - Recompute HMAC with your secret\r\n   - Compare signatures (constant-time)\r\n   - Validate timestamp freshness (5-minute window)\r\n\r\n3. **Replay protection**\r\n   - Include timestamp in signed payload\r\n   - Reject events older than tolerance window\r\n   - Optionally store event IDs for dedup\r\n\r\n4. **Secret rotation**\r\n   - Support multiple active secrets\r\n   - Grace period during rotation\r\n   - Notify consumers before rotation\r\n\r\n## Pitfalls\r\n\r\n| Pitfall | How to Avoid |\r\n|---------|--------------|\r\n| Using simple string compare | Use constant-time comparison |\r\n| No timestamp validation | Check event is within 5-minute window |\r\n| Sharing secret across consumers | Unique secret per webhook endpoint |\r\n| Not logging verification failures | Track for security monitoring |\r\n| Ignoring replay attacks | Store event IDs or use timestamps |\r\n\r\n## Checklist\r\n\r\n- [ ] HMAC-SHA256 used for signatures\r\n- [ ] Unique secret per consumer/endpoint\r\n- [ ] Signature sent in secure header\r\n- [ ] Constant-time comparison used\r\n- [ ] Timestamp included and validated\r\n- [ ] Replay window enforced (5 min typical)\r\n- [ ] Failed verifications logged with context\r\n- [ ] Secret rotation mechanism in place\r\n- [ ] Multiple secrets supported during rotation\r\n- [ ] Signature scheme documented\r\n\r\n## Snippets (Generic)\r\n\r\n```\r\nSignature Generation (Sender):\r\n1. Create payload: { \"event\": \"payment.completed\", \"data\": {...}, \"timestamp\": 1642000000 }\r\n2. Stringify payload (canonical JSON)\r\n3. Compute: signature = HMAC-SHA256(secret, timestamp + \".\" + payload)\r\n4. Send header: X-Signature: t=1642000000,v1=abc123...\r\n\r\nSignature Verification (Receiver):\r\n1. Extract header: t=1642000000,v1=abc123...\r\n2. Parse timestamp and signature\r\n3. Check: abs(now - timestamp) < 300 seconds\r\n4. Compute expected: HMAC-SHA256(secret, timestamp + \".\" + raw_body)\r\n5. Compare: constant_time_equal(expected, received_signature)\r\n6. If mismatch → 401 Unauthorized\r\n\r\nHeader Format (Stripe-style):\r\nX-Signature: t=1642000000,v1=5257a869e7ecebeda32affa62cdca3fa51cad7e77a0e56ff536d0ce8e108d8bd\r\n```\r\n\r\n## Sources\r\n\r\n- Stripe Webhook Signatures: https://stripe.com/docs/webhooks/signatures\r\n- GitHub Webhook Secrets: https://docs.github.com/en/webhooks/securing\r\n- Twilio Request Validation: https://www.twilio.com/docs/usage/security\r\n- OWASP Webhook Security: https://cheatsheetseries.owasp.org/cheatsheets/Webhook_Security_Cheat_Sheet.html\r\n"
  },
  {
    "id": "db-connection-pooling",
    "title": "Database Connection Pooling",
    "tags": [
      "database",
      "connection-pool",
      "performance",
      "postgresql",
      "mysql"
    ],
    "level": "intermediate",
    "stacks": [
      "postgresql",
      "mysql",
      "sqlserver"
    ],
    "scope": "database",
    "maturity": "stable",
    "works_with": [
      "postgresql",
      "mysql",
      "sqlserver"
    ],
    "path": "D:/Code/backend-engineering-kit/.shared/production-backend-kit/patterns/db.connection-pooling.md",
    "type": "pattern",
    "sections": [],
    "checklist": [],
    "sources": [],
    "rawContent": "\r\n# Database Connection Pooling\r\n\r\n## Problem\r\n\r\nDatabase connections are expensive:\r\n- **TCP handshake** + TLS negotiation: 100-300ms\r\n- **Authentication**: Database validates credentials\r\n- **Memory overhead**: PostgreSQL ~10MB per connection\r\n- **Process/thread creation**: OS resources\r\n\r\nWithout pooling:\r\n- Connection storms during traffic spikes\r\n- Database overwhelmed with connection management\r\n- Slow response times waiting for connections\r\n- \"Too many connections\" errors\r\n\r\n## When to use\r\n\r\n- **Always** for production applications\r\n- Serverless/Lambda functions (even more critical)\r\n- High-concurrency workloads\r\n- Microservices with many instances\r\n- Connection-limited database tiers\r\n\r\n## Solution\r\n\r\n### 1. Connection Pool Sizing\r\n\r\n```\r\n┌─────────────────────────────────────────────────────────────────────────────┐\r\n│                    CONNECTION POOL FORMULA                                  │\r\n├─────────────────────────────────────────────────────────────────────────────┤\r\n│                                                                             │\r\n│  Optimal Pool Size = (core_count * 2) + effective_spindle_count             │\r\n│                                                                             │\r\n│  For SSD/NVMe (spindle_count = 1):                                         │\r\n│    8 cores → (8 * 2) + 1 = 17 connections                                  │\r\n│                                                                             │\r\n│  For most applications:                                                     │\r\n│    Start with: pool_size = 10-20                                           │\r\n│    Max practical limit: ~100 per application instance                       │\r\n│                                                                             │\r\n│  ⚠️  More connections ≠ better performance!                                │\r\n│      Too many connections = context switching overhead                      │\r\n│                                                                             │\r\n│  Total DB Connections = pool_size × num_app_instances                       │\r\n│  Example: 20 pool × 5 instances = 100 connections to database              │\r\n│                                                                             │\r\n└─────────────────────────────────────────────────────────────────────────────┘\r\n```\r\n\r\n### 2. Application-Level Pooling (Node.js)\r\n\r\n```typescript\r\nimport { Pool, PoolConfig } from 'pg';\r\n\r\nconst poolConfig: PoolConfig = {\r\n  // Connection settings\r\n  host: process.env.DB_HOST,\r\n  port: parseInt(process.env.DB_PORT || '5432'),\r\n  database: process.env.DB_NAME,\r\n  user: process.env.DB_USER,\r\n  password: process.env.DB_PASSWORD,\r\n  \r\n  // Pool size\r\n  min: 2,                    // Minimum connections to keep open\r\n  max: 20,                   // Maximum connections in pool\r\n  \r\n  // Timeouts\r\n  connectionTimeoutMillis: 10000,  // Time to wait for connection\r\n  idleTimeoutMillis: 30000,        // Close idle connections after 30s\r\n  \r\n  // Keep-alive (important for cloud DBs with idle timeouts)\r\n  keepAlive: true,\r\n  keepAliveInitialDelayMillis: 10000,\r\n  \r\n  // Statement timeout (prevent runaway queries)\r\n  statement_timeout: 30000,  // 30 seconds\r\n  \r\n  // SSL for production\r\n  ssl: process.env.NODE_ENV === 'production' ? {\r\n    rejectUnauthorized: true,\r\n    ca: process.env.DB_CA_CERT,\r\n  } : false,\r\n  \r\n  // Application name (helps identify connections in DB)\r\n  application_name: `${process.env.SERVICE_NAME}-${process.env.HOSTNAME}`,\r\n};\r\n\r\nconst pool = new Pool(poolConfig);\r\n\r\n// Event handlers for monitoring\r\npool.on('connect', (client) => {\r\n  metrics.increment('db.connection.created');\r\n  logger.debug('New database connection established');\r\n});\r\n\r\npool.on('acquire', (client) => {\r\n  metrics.increment('db.connection.acquired');\r\n});\r\n\r\npool.on('release', (client) => {\r\n  metrics.increment('db.connection.released');\r\n});\r\n\r\npool.on('remove', (client) => {\r\n  metrics.increment('db.connection.removed');\r\n});\r\n\r\npool.on('error', (err, client) => {\r\n  metrics.increment('db.connection.error');\r\n  logger.error({ err }, 'Unexpected database pool error');\r\n});\r\n\r\n// Expose pool metrics\r\nfunction getPoolStats() {\r\n  return {\r\n    total: pool.totalCount,      // Total connections created\r\n    idle: pool.idleCount,        // Connections waiting to be used\r\n    waiting: pool.waitingCount,  // Queries waiting for connection\r\n  };\r\n}\r\n\r\n// Periodic metrics collection\r\nsetInterval(() => {\r\n  const stats = getPoolStats();\r\n  metrics.gauge('db.pool.total', stats.total);\r\n  metrics.gauge('db.pool.idle', stats.idle);\r\n  metrics.gauge('db.pool.waiting', stats.waiting);\r\n  metrics.gauge('db.pool.utilization', \r\n    (stats.total - stats.idle) / poolConfig.max!\r\n  );\r\n}, 10000);\r\n\r\n// Graceful shutdown\r\nasync function closePool() {\r\n  logger.info('Closing database pool...');\r\n  await pool.end();\r\n  logger.info('Database pool closed');\r\n}\r\n\r\nprocess.on('SIGTERM', closePool);\r\n```\r\n\r\n### 3. PgBouncer (External Pooler)\r\n\r\n```ini\r\n; /etc/pgbouncer/pgbouncer.ini\r\n\r\n[databases]\r\n; database = host port dbname user password\r\nmyapp = host=db.internal port=5432 dbname=myapp\r\n\r\n[pgbouncer]\r\n; Listen on all interfaces\r\nlisten_addr = 0.0.0.0\r\nlisten_port = 6432\r\n\r\n; Authentication\r\nauth_type = scram-sha-256\r\nauth_file = /etc/pgbouncer/userlist.txt\r\n\r\n; Pool mode\r\n; - session: Connection assigned for entire session (default)\r\n; - transaction: Connection assigned per transaction (recommended)\r\n; - statement: Connection assigned per statement (most aggressive)\r\npool_mode = transaction\r\n\r\n; Pool size\r\ndefault_pool_size = 20\r\nmin_pool_size = 5\r\nmax_client_conn = 1000      ; Max client connections to PgBouncer\r\nmax_db_connections = 100     ; Max connections TO the database\r\n\r\n; Reserve connections for admin\r\nreserve_pool_size = 5\r\nreserve_pool_timeout = 3\r\n\r\n; Timeouts\r\nserver_connect_timeout = 10\r\nserver_idle_timeout = 600\r\nserver_lifetime = 3600\r\nclient_idle_timeout = 0      ; Don't timeout idle clients\r\n\r\n; Query timeout\r\nquery_timeout = 30\r\nquery_wait_timeout = 60\r\n\r\n; Logging\r\nlog_connections = 1\r\nlog_disconnections = 1\r\nlog_pooler_errors = 1\r\nstats_period = 60\r\n\r\n; Admin access\r\nadmin_users = postgres\r\nstats_users = monitoring\r\n```\r\n\r\n```yaml\r\n# Kubernetes deployment with PgBouncer sidecar\r\napiVersion: apps/v1\r\nkind: Deployment\r\nspec:\r\n  template:\r\n    spec:\r\n      containers:\r\n        # Application container\r\n        - name: app\r\n          image: myapp:latest\r\n          env:\r\n            - name: DATABASE_URL\r\n              value: \"postgresql://user:pass@localhost:6432/myapp\"\r\n        \r\n        # PgBouncer sidecar\r\n        - name: pgbouncer\r\n          image: edoburu/pgbouncer:latest\r\n          ports:\r\n            - containerPort: 6432\r\n          env:\r\n            - name: DATABASE_URL\r\n              valueFrom:\r\n                secretKeyRef:\r\n                  name: db-credentials\r\n                  key: url\r\n            - name: POOL_MODE\r\n              value: \"transaction\"\r\n            - name: DEFAULT_POOL_SIZE\r\n              value: \"20\"\r\n            - name: MAX_CLIENT_CONN\r\n              value: \"100\"\r\n          resources:\r\n            requests:\r\n              memory: \"64Mi\"\r\n              cpu: \"100m\"\r\n            limits:\r\n              memory: \"128Mi\"\r\n              cpu: \"500m\"\r\n```\r\n\r\n### 4. Python Connection Pooling\r\n\r\n```python\r\nfrom sqlalchemy import create_engine, event, text\r\nfrom sqlalchemy.pool import QueuePool\r\nimport os\r\n\r\n# SQLAlchemy with connection pool\r\nengine = create_engine(\r\n    os.environ[\"DATABASE_URL\"],\r\n    \r\n    # Pool class\r\n    poolclass=QueuePool,\r\n    \r\n    # Pool size\r\n    pool_size=10,           # Number of connections to keep\r\n    max_overflow=20,        # Extra connections when pool exhausted\r\n    pool_pre_ping=True,     # Verify connection before using\r\n    \r\n    # Timeouts\r\n    pool_timeout=30,        # Wait for connection\r\n    pool_recycle=1800,      # Recycle connections after 30 min\r\n    \r\n    # Connection args\r\n    connect_args={\r\n        \"connect_timeout\": 10,\r\n        \"application_name\": f\"{os.environ['SERVICE_NAME']}\",\r\n        \"options\": \"-c statement_timeout=30000\",\r\n    },\r\n)\r\n\r\n# Connection lifecycle events\r\n@event.listens_for(engine, \"connect\")\r\ndef on_connect(dbapi_connection, connection_record):\r\n    \"\"\"Called when a new connection is created\"\"\"\r\n    metrics.increment(\"db.connection.created\")\r\n\r\n@event.listens_for(engine, \"checkout\")\r\ndef on_checkout(dbapi_connection, connection_record, connection_proxy):\r\n    \"\"\"Called when a connection is retrieved from pool\"\"\"\r\n    metrics.increment(\"db.connection.checkout\")\r\n\r\n@event.listens_for(engine, \"checkin\")\r\ndef on_checkin(dbapi_connection, connection_record):\r\n    \"\"\"Called when a connection is returned to pool\"\"\"\r\n    metrics.increment(\"db.connection.checkin\")\r\n\r\n# Monitor pool stats\r\ndef get_pool_status():\r\n    return {\r\n        \"pool_size\": engine.pool.size(),\r\n        \"checked_in\": engine.pool.checkedin(),\r\n        \"checked_out\": engine.pool.checkedout(),\r\n        \"overflow\": engine.pool.overflow(),\r\n        \"invalid\": engine.pool.invalidatedcount(),\r\n    }\r\n\r\n# FastAPI integration\r\nfrom fastapi import FastAPI, Depends\r\nfrom sqlalchemy.orm import Session, sessionmaker\r\n\r\nSessionLocal = sessionmaker(bind=engine)\r\n\r\ndef get_db():\r\n    db = SessionLocal()\r\n    try:\r\n        yield db\r\n    finally:\r\n        db.close()\r\n\r\napp = FastAPI()\r\n\r\n@app.get(\"/users/{user_id}\")\r\ndef get_user(user_id: int, db: Session = Depends(get_db)):\r\n    # Connection automatically returned to pool when request ends\r\n    return db.query(User).filter(User.id == user_id).first()\r\n```\r\n\r\n### 5. Serverless/Lambda Considerations\r\n\r\n```typescript\r\n// Serverless needs external connection pooling!\r\n// Each Lambda invocation could create a new connection\r\n\r\n// Option 1: RDS Proxy (AWS managed)\r\nconst pool = new Pool({\r\n  host: 'my-proxy.proxy-xxx.us-east-1.rds.amazonaws.com',\r\n  // RDS Proxy handles pooling - use smaller pool per Lambda\r\n  max: 2,  // Each Lambda instance only needs 1-2 connections\r\n});\r\n\r\n// Option 2: Neon/PlanetScale with built-in pooling\r\nconst pool = new Pool({\r\n  connectionString: process.env.DATABASE_URL,\r\n  // Neon pooler URL: postgres://...@ep-xxx.us-east-1.aws.neon.tech/mydb?sslmode=require\r\n  max: 1,  // Pooler handles multiplexing\r\n});\r\n\r\n// Option 3: Singleton pattern for connection reuse\r\nlet pool: Pool | null = null;\r\n\r\nfunction getPool(): Pool {\r\n  if (!pool) {\r\n    pool = new Pool({\r\n      max: 2,\r\n      connectionTimeoutMillis: 5000,\r\n      // Important for Lambda: close idle connections quickly\r\n      idleTimeoutMillis: 1000,\r\n    });\r\n  }\r\n  return pool;\r\n}\r\n\r\n// Lambda handler\r\nexport async function handler(event: APIGatewayEvent) {\r\n  const pool = getPool();  // Reuse pool across invocations\r\n  \r\n  const client = await pool.connect();\r\n  try {\r\n    const result = await client.query('SELECT * FROM users WHERE id = $1', [event.pathParameters.id]);\r\n    return { statusCode: 200, body: JSON.stringify(result.rows[0]) };\r\n  } finally {\r\n    client.release();  // Return to pool, don't close\r\n  }\r\n}\r\n```\r\n\r\n### 6. Monitoring Queries\r\n\r\n```sql\r\n-- PostgreSQL: Active connections\r\nSELECT \r\n  usename,\r\n  application_name,\r\n  client_addr,\r\n  state,\r\n  query_start,\r\n  state_change,\r\n  wait_event_type,\r\n  wait_event\r\nFROM pg_stat_activity\r\nWHERE datname = 'mydb'\r\nORDER BY query_start;\r\n\r\n-- Connections by application\r\nSELECT \r\n  application_name,\r\n  count(*) as connections,\r\n  count(*) FILTER (WHERE state = 'idle') as idle,\r\n  count(*) FILTER (WHERE state = 'active') as active,\r\n  count(*) FILTER (WHERE state = 'idle in transaction') as idle_in_transaction\r\nFROM pg_stat_activity\r\nWHERE datname = 'mydb'\r\nGROUP BY application_name\r\nORDER BY connections DESC;\r\n\r\n-- Connection limits\r\nSELECT \r\n  max_conn,\r\n  used,\r\n  res_for_super,\r\n  max_conn - used - res_for_super AS available\r\nFROM \r\n  (SELECT count(*) used FROM pg_stat_activity) t1,\r\n  (SELECT setting::int res_for_super FROM pg_settings WHERE name = 'superuser_reserved_connections') t2,\r\n  (SELECT setting::int max_conn FROM pg_settings WHERE name = 'max_connections') t3;\r\n\r\n-- Long-running queries\r\nSELECT \r\n  pid,\r\n  now() - query_start AS duration,\r\n  query,\r\n  state\r\nFROM pg_stat_activity\r\nWHERE state != 'idle'\r\n  AND query NOT LIKE '%pg_stat_activity%'\r\n  AND now() - query_start > interval '5 minutes'\r\nORDER BY duration DESC;\r\n\r\n-- Kill long-running queries\r\nSELECT pg_terminate_backend(pid)\r\nFROM pg_stat_activity\r\nWHERE state = 'active'\r\n  AND now() - query_start > interval '30 minutes'\r\n  AND usename != 'postgres';\r\n```\r\n\r\n### 7. Health Check with Pool Validation\r\n\r\n```typescript\r\n// Health check endpoint\r\napp.get('/health/ready', async (req, res) => {\r\n  const checks = {\r\n    database: { status: 'unknown', latency: 0 },\r\n    pool: getPoolStats(),\r\n  };\r\n  \r\n  try {\r\n    const start = Date.now();\r\n    const client = await pool.connect();\r\n    try {\r\n      await client.query('SELECT 1');\r\n      checks.database = {\r\n        status: 'healthy',\r\n        latency: Date.now() - start,\r\n      };\r\n    } finally {\r\n      client.release();\r\n    }\r\n  } catch (error) {\r\n    checks.database = {\r\n      status: 'unhealthy',\r\n      error: error.message,\r\n    };\r\n  }\r\n  \r\n  // Check pool health\r\n  const poolHealth = checks.pool.waiting === 0 && \r\n                     checks.pool.idle > 0;\r\n  \r\n  const isHealthy = checks.database.status === 'healthy' && poolHealth;\r\n  \r\n  res.status(isHealthy ? 200 : 503).json({\r\n    status: isHealthy ? 'healthy' : 'unhealthy',\r\n    checks,\r\n  });\r\n});\r\n\r\n// Circuit breaker for database\r\nclass DatabaseCircuitBreaker {\r\n  private failures = 0;\r\n  private lastFailure = 0;\r\n  private state: 'closed' | 'open' | 'half-open' = 'closed';\r\n  \r\n  constructor(\r\n    private failureThreshold = 5,\r\n    private resetTimeout = 30000,\r\n  ) {}\r\n\r\n  async execute<T>(fn: () => Promise<T>): Promise<T> {\r\n    if (this.state === 'open') {\r\n      if (Date.now() - this.lastFailure > this.resetTimeout) {\r\n        this.state = 'half-open';\r\n      } else {\r\n        throw new Error('Database circuit breaker is open');\r\n      }\r\n    }\r\n\r\n    try {\r\n      const result = await fn();\r\n      this.onSuccess();\r\n      return result;\r\n    } catch (error) {\r\n      this.onFailure();\r\n      throw error;\r\n    }\r\n  }\r\n\r\n  private onSuccess() {\r\n    this.failures = 0;\r\n    this.state = 'closed';\r\n  }\r\n\r\n  private onFailure() {\r\n    this.failures++;\r\n    this.lastFailure = Date.now();\r\n    \r\n    if (this.failures >= this.failureThreshold) {\r\n      this.state = 'open';\r\n      logger.error('Database circuit breaker opened');\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n### 8. Connection Pool Tuning\r\n\r\n```typescript\r\n// Dynamic pool sizing based on load\r\nclass AdaptivePool {\r\n  private pool: Pool;\r\n  private config: PoolConfig;\r\n  \r\n  constructor(config: PoolConfig) {\r\n    this.config = config;\r\n    this.pool = new Pool(config);\r\n    this.startMonitoring();\r\n  }\r\n\r\n  private startMonitoring() {\r\n    setInterval(() => {\r\n      const stats = {\r\n        total: this.pool.totalCount,\r\n        idle: this.pool.idleCount,\r\n        waiting: this.pool.waitingCount,\r\n      };\r\n      \r\n      // If queries are waiting, consider scaling up\r\n      if (stats.waiting > 0 && stats.total < (this.config.max || 20)) {\r\n        logger.warn({\r\n          stats,\r\n          message: 'Queries waiting for connections',\r\n          suggestion: 'Consider increasing pool size or adding read replicas',\r\n        });\r\n      }\r\n      \r\n      // If pool is mostly idle, log for potential downsizing\r\n      if (stats.idle > stats.total * 0.8) {\r\n        logger.info({\r\n          stats,\r\n          message: 'Pool is mostly idle',\r\n          suggestion: 'Consider decreasing pool size',\r\n        });\r\n      }\r\n      \r\n      metrics.gauge('db.pool.total', stats.total);\r\n      metrics.gauge('db.pool.idle', stats.idle);\r\n      metrics.gauge('db.pool.waiting', stats.waiting);\r\n      metrics.gauge('db.pool.utilization', \r\n        stats.total > 0 ? (stats.total - stats.idle) / stats.total : 0\r\n      );\r\n    }, 10000);\r\n  }\r\n}\r\n\r\n// Recommended settings by workload\r\nconst POOL_PRESETS = {\r\n  // Low traffic, cost-sensitive\r\n  small: {\r\n    min: 1,\r\n    max: 5,\r\n    idleTimeoutMillis: 60000,\r\n  },\r\n  \r\n  // Medium traffic, balanced\r\n  medium: {\r\n    min: 5,\r\n    max: 20,\r\n    idleTimeoutMillis: 30000,\r\n  },\r\n  \r\n  // High traffic, performance-critical\r\n  large: {\r\n    min: 10,\r\n    max: 50,\r\n    idleTimeoutMillis: 10000,\r\n  },\r\n  \r\n  // Serverless/Lambda\r\n  serverless: {\r\n    min: 0,\r\n    max: 2,\r\n    idleTimeoutMillis: 1000,  // Close quickly\r\n  },\r\n};\r\n```\r\n\r\n## Pitfalls\r\n\r\n| Pitfall | Impact | How to Avoid |\r\n|---------|--------|--------------|\r\n| Pool too large | Database overwhelmed | Start small, monitor, increase gradually |\r\n| Pool too small | Requests waiting, timeouts | Monitor waiting count |\r\n| No connection timeout | Hanging requests | Set connectionTimeoutMillis |\r\n| No statement timeout | Runaway queries | Set statement_timeout |\r\n| Leaking connections | Pool exhaustion | Always release/close in finally block |\r\n| No health checks | Using dead connections | Enable pool_pre_ping |\r\n| Lambda without pooler | Connection explosion | Use RDS Proxy/PgBouncer |\r\n| Idle in transaction | Locks held, pool blocked | Set idle_in_transaction_session_timeout |\r\n\r\n## Checklist\r\n\r\n- [ ] Connection pool configured in application\r\n- [ ] Pool size based on workload, not arbitrary number\r\n- [ ] Connection timeout set\r\n- [ ] Statement timeout set\r\n- [ ] Idle timeout configured\r\n- [ ] Connection keep-alive enabled\r\n- [ ] Pool metrics exposed (total, idle, waiting)\r\n- [ ] Health check validates pool\r\n- [ ] Graceful shutdown drains pool\r\n- [ ] SSL/TLS enabled for connections\r\n- [ ] Application name set for debugging\r\n- [ ] External pooler (PgBouncer) for high scale\r\n- [ ] Serverless uses managed pooler\r\n\r\n## References\r\n\r\n- [PostgreSQL Connection Settings](https://www.postgresql.org/docs/current/runtime-config-connection.html)\r\n- [PgBouncer Documentation](https://www.pgbouncer.org/)\r\n- [AWS RDS Proxy](https://aws.amazon.com/rds/proxy/)\r\n- [HikariCP (Java) Sizing](https://github.com/brettwooldridge/HikariCP/wiki/About-Pool-Sizing)\r\n- [node-postgres Pool](https://node-postgres.com/features/pooling)\r\n"
  },
  {
    "id": "db-indexing-basics",
    "title": "Database Indexing Basics",
    "tags": [
      "database",
      "indexing",
      "performance",
      "query-optimization",
      "explain"
    ],
    "level": "intermediate",
    "stacks": [
      "postgresql",
      "mysql",
      "sqlserver",
      "mongodb"
    ],
    "scope": "database",
    "maturity": "stable",
    "works_with": [
      "postgresql",
      "mysql",
      "sqlserver",
      "mongodb"
    ],
    "path": "D:/Code/backend-engineering-kit/.shared/production-backend-kit/patterns/db.indexing-basics.md",
    "type": "pattern",
    "sections": [],
    "checklist": [],
    "sources": [],
    "rawContent": "\r\n# Database Indexing Basics\r\n\r\n## Problem\r\n\r\nQueries on large tables without proper indexes result in full table scans, causing slow response times, high CPU usage, and poor user experience. Over-indexing wastes disk space and slows writes. The key is finding the right balance.\r\n\r\n## When to use\r\n\r\n- Columns used in WHERE clauses (especially with high selectivity)\r\n- Columns used in JOIN conditions\r\n- Columns used in ORDER BY / GROUP BY\r\n- Foreign key columns (prevents full scans on DELETE)\r\n- Columns with unique constraints\r\n- Frequently searched text (full-text indexes)\r\n\r\n## Solution\r\n\r\n### 1. Understand Index Types\r\n\r\n| Type | Use Case | PostgreSQL | MySQL |\r\n|------|----------|------------|-------|\r\n| **B-Tree** | Default, equality, range, sorting | ✓ Default | ✓ Default |\r\n| **Hash** | Equality only (rarely better) | ✓ | ✓ (InnoDB internal) |\r\n| **GIN** | Arrays, JSONB, full-text | ✓ | - |\r\n| **GiST** | Geometric, range types | ✓ | - |\r\n| **BRIN** | Very large, naturally ordered tables | ✓ | - |\r\n| **Full-Text** | Text search | tsvector + GIN | FULLTEXT |\r\n\r\n### 2. Composite Index Column Order\r\n\r\n**Rule**: Equality columns first, then range columns, then sort columns.\r\n\r\n```sql\r\n-- Query:\r\nSELECT * FROM orders \r\nWHERE tenant_id = $1      -- Equality\r\n  AND status = $2         -- Equality  \r\n  AND created_at > $3     -- Range\r\nORDER BY created_at DESC;\r\n\r\n-- Optimal index:\r\nCREATE INDEX idx_orders_tenant_status_created \r\n  ON orders (tenant_id, status, created_at DESC);\r\n```\r\n\r\n### 3. Covering Indexes (Index-Only Scans)\r\n\r\nInclude all columns needed by query to avoid table lookup:\r\n\r\n```sql\r\n-- Query only needs these columns\r\nSELECT id, status, total FROM orders WHERE user_id = $1;\r\n\r\n-- Covering index\r\nCREATE INDEX idx_orders_user_covering \r\n  ON orders (user_id) INCLUDE (id, status, total);\r\n```\r\n\r\n### 4. Partial Indexes (PostgreSQL)\r\n\r\nIndex only rows matching a condition:\r\n\r\n```sql\r\n-- Only 5% of orders are 'pending', but queried frequently\r\nCREATE INDEX idx_orders_pending \r\n  ON orders (created_at) WHERE status = 'pending';\r\n```\r\n\r\n### 5. Index Maintenance\r\n\r\n```sql\r\n-- PostgreSQL: Check unused indexes\r\nSELECT indexrelname, idx_scan, idx_tup_read \r\nFROM pg_stat_user_indexes \r\nWHERE idx_scan = 0;\r\n\r\n-- PostgreSQL: Check index bloat\r\nSELECT * FROM pgstattuple('idx_orders_user_id');\r\n\r\n-- Reindex (careful in production)\r\nREINDEX INDEX CONCURRENTLY idx_orders_user_id;\r\n```\r\n\r\n## Pitfalls\r\n\r\n| Pitfall | How to Avoid |\r\n|---------|--------------|\r\n| Indexing every column | Only index queried columns |\r\n| Wrong column order in composite | Put highest selectivity first |\r\n| Ignoring index on FKs | Always index foreign keys |\r\n| Not using EXPLAIN | Always verify index usage |\r\n| Indexes on low-cardinality columns | Boolean/status rarely benefit |\r\n\r\n## Checklist\r\n\r\n- [ ] Foreign key columns indexed\r\n- [ ] Frequently queried columns indexed\r\n- [ ] Composite index column order optimized\r\n- [ ] EXPLAIN used to verify index usage\r\n- [ ] Unused indexes identified and removed\r\n- [ ] Partial indexes used where applicable\r\n- [ ] Index on columns in WHERE/JOIN/ORDER BY\r\n- [ ] Write performance impact considered\r\n- [ ] Index statistics up to date\r\n- [ ] Index naming convention followed\r\n\r\n## Snippets (Generic)\r\n\r\n```\r\nEXPLAIN ANALYZE:\r\nEXPLAIN ANALYZE SELECT * FROM users WHERE email = 'test@example.com';\r\n-- Look for: \"Index Scan\" vs \"Seq Scan\"\r\n\r\nCreating Indexes:\r\n-- Single column\r\nCREATE INDEX idx_users_email ON users(email);\r\n\r\n-- Composite (order matters!)\r\nCREATE INDEX idx_orders_user_status ON orders(user_id, status);\r\n\r\n-- Partial index\r\nCREATE INDEX idx_orders_pending ON orders(created_at) \r\n  WHERE status = 'pending';\r\n\r\n-- Covering index (includes all needed columns)\r\nCREATE INDEX idx_orders_covering ON orders(user_id, status) \r\n  INCLUDE (total, created_at);\r\n\r\nIndex Selection Rules:\r\n1. Column in WHERE → Consider index\r\n2. Column in JOIN ON → Index required\r\n3. Column in ORDER BY → May benefit from index\r\n4. Low cardinality (boolean) → Usually skip\r\n5. High write table → Be conservative\r\n\r\nSteps:\r\n1. Enable slow query logging\r\n2. Identify top slow queries\r\n3. Run EXPLAIN ANALYZE on them\r\n4. Add targeted indexes\r\n5. Verify with EXPLAIN again\r\n6. Monitor index usage over time\r\n```\r\n\r\n## Sources\r\n\r\n- Use The Index, Luke: https://use-the-index-luke.com/\r\n- PostgreSQL Indexes: https://www.postgresql.org/docs/current/indexes.html\r\n- MySQL Index Optimization: https://dev.mysql.com/doc/refman/8.0/en/optimization-indexes.html\r\n- MongoDB Indexing Strategies: https://www.mongodb.com/docs/manual/indexes/\r\n"
  },
  {
    "id": "db-migrations-strategy",
    "title": "Database Migrations Strategy",
    "tags": [
      "database",
      "migrations",
      "schema",
      "devops",
      "zero-downtime"
    ],
    "level": "intermediate",
    "stacks": [
      "all"
    ],
    "scope": "database",
    "maturity": "stable",
    "works_with": [
      "all"
    ],
    "path": "D:/Code/backend-engineering-kit/.shared/production-backend-kit/patterns/db.migrations-strategy.md",
    "type": "pattern",
    "sections": [],
    "checklist": [],
    "sources": [],
    "rawContent": "\r\n# Database Migrations Strategy\r\n\r\n## Problem\r\n\r\nUnmanaged schema changes cause deployment failures, data loss, and environment drift. Without migration strategy, rolling back is painful and synchronizing schemas across environments is error-prone. Zero-downtime deployments require special care.\r\n\r\n## When to use\r\n\r\n- Any application with database\r\n- Multi-environment deployments (dev, staging, prod)\r\n- Team collaboration on schema\r\n- CI/CD pipelines\r\n- Blue-green or rolling deployments\r\n- Zero-downtime requirements\r\n\r\n## Solution\r\n\r\n### 1. Migration File Structure\r\n\r\n```\r\nmigrations/\r\n├── V001__create_users_table.sql\r\n├── V002__add_email_to_users.sql\r\n├── V003__create_orders_table.sql\r\n├── V004__add_index_orders_user_id.sql\r\n└── V005__add_status_to_orders.sql\r\n\r\n# Naming: V{version}__{description}.sql\r\n# Version: Sequential number (001, 002, ...)\r\n# Description: Snake_case, describes change\r\n```\r\n\r\n### 2. Safe Migration Patterns\r\n\r\n| Change | Unsafe | Safe Approach |\r\n|--------|--------|---------------|\r\n| Add column | - | Add with NULL or DEFAULT |\r\n| Add NOT NULL | Add directly | Add NULL → backfill → alter NOT NULL |\r\n| Rename column | Rename directly | Add new → copy → drop old |\r\n| Change type | Alter directly | Add new → migrate → drop old |\r\n| Add index | CREATE INDEX | CREATE INDEX CONCURRENTLY |\r\n| Add FK | ADD CONSTRAINT | ADD CONSTRAINT NOT VALID → VALIDATE |\r\n| Drop column | Drop directly | Remove code references → drop |\r\n| Drop table | Drop directly | Rename to _deleted → wait → drop |\r\n\r\n### 3. Expand-Contract Pattern (Zero Downtime)\r\n\r\n```\r\nPhase 1: EXPAND (Backward Compatible)\r\n──────────────────────────────────\r\n- Add new column (nullable)\r\n- Deploy: New code writes to BOTH old and new columns\r\n- Backfill: Copy data from old to new column\r\n- Old code still works (reads old column)\r\n\r\nPhase 2: MIGRATE (Switch Over)\r\n──────────────────────────────────\r\n- Deploy: Code reads from NEW column only\r\n- Old column is now unused\r\n- Verify all data migrated correctly\r\n\r\nPhase 3: CONTRACT (Cleanup)\r\n──────────────────────────────────\r\n- Drop old column\r\n- Remove dual-write code\r\n- Schema is clean\r\n```\r\n\r\n**Example: Rename Column `name` to `full_name`**\r\n\r\n```sql\r\n-- Migration 1: EXPAND\r\nALTER TABLE users ADD COLUMN full_name VARCHAR(255);\r\nUPDATE users SET full_name = name WHERE full_name IS NULL;\r\n\r\n-- Deploy code that writes to BOTH columns\r\n-- Deploy code that reads from full_name with fallback to name\r\n\r\n-- Migration 2: Add NOT NULL (after backfill verified)\r\nALTER TABLE users ALTER COLUMN full_name SET NOT NULL;\r\n\r\n-- Deploy code that reads ONLY from full_name\r\n\r\n-- Migration 3: CONTRACT (after all code deployed)\r\nALTER TABLE users DROP COLUMN name;\r\n```\r\n\r\n### 4. Safe Index Creation (PostgreSQL)\r\n\r\n```sql\r\n-- UNSAFE: Locks table for entire duration\r\nCREATE INDEX idx_orders_user_id ON orders(user_id);\r\n\r\n-- SAFE: Non-blocking (PostgreSQL)\r\nSET lock_timeout = '5s';\r\nCREATE INDEX CONCURRENTLY idx_orders_user_id ON orders(user_id);\r\n\r\n-- Note: CONCURRENTLY cannot run in transaction\r\n-- If it fails partway, you may have an INVALID index\r\n-- Check with: SELECT * FROM pg_indexes WHERE indexname = 'idx_orders_user_id';\r\n-- Drop invalid: DROP INDEX CONCURRENTLY idx_orders_user_id;\r\n```\r\n\r\n### 5. Safe Foreign Key Addition (PostgreSQL)\r\n\r\n```sql\r\n-- UNSAFE: Scans entire table while holding lock\r\nALTER TABLE orders \r\n  ADD CONSTRAINT fk_orders_user_id \r\n  FOREIGN KEY (user_id) REFERENCES users(id);\r\n\r\n-- SAFE: Two-step process\r\n-- Step 1: Add constraint without validation (instant)\r\nALTER TABLE orders \r\n  ADD CONSTRAINT fk_orders_user_id \r\n  FOREIGN KEY (user_id) REFERENCES users(id) \r\n  NOT VALID;\r\n\r\n-- Step 2: Validate separately (non-blocking in PG 12+)\r\nALTER TABLE orders \r\n  VALIDATE CONSTRAINT fk_orders_user_id;\r\n```\r\n\r\n### 6. Batched Data Migration\r\n\r\n```sql\r\n-- UNSAFE: Updates all rows, holds locks\r\nUPDATE orders SET status = 'pending' WHERE status IS NULL;\r\n\r\n-- SAFE: Batch updates\r\nDO $$\r\nDECLARE\r\n  batch_size INT := 10000;\r\n  rows_updated INT;\r\nBEGIN\r\n  LOOP\r\n    UPDATE orders\r\n    SET status = 'pending'\r\n    WHERE id IN (\r\n      SELECT id FROM orders \r\n      WHERE status IS NULL \r\n      LIMIT batch_size\r\n      FOR UPDATE SKIP LOCKED\r\n    );\r\n    \r\n    GET DIAGNOSTICS rows_updated = ROW_COUNT;\r\n    \r\n    IF rows_updated = 0 THEN\r\n      EXIT;\r\n    END IF;\r\n    \r\n    COMMIT;\r\n    PERFORM pg_sleep(0.1); -- Brief pause to reduce load\r\n  END LOOP;\r\nEND $$;\r\n```\r\n\r\n## Pitfalls\r\n\r\n| Pitfall | How to Avoid |\r\n|---------|--------------|\r\n| Missing rollback script | Always write down + up migration |\r\n| Locking tables in prod | Use CREATE INDEX CONCURRENTLY |\r\n| Dropping columns immediately | Expand-contract: deprecate first |\r\n| Testing on empty DB | Test with production-like data |\r\n| Manual schema changes | Only apply through migrations |\r\n\r\n## Checklist\r\n\r\n- [ ] Migration tool configured\r\n- [ ] Migrations version-controlled\r\n- [ ] Up and down scripts provided\r\n- [ ] Migrations run in CI\r\n- [ ] Non-destructive changes preferred\r\n- [ ] Large data updates batched\r\n- [ ] Indexes created concurrently (if supported)\r\n- [ ] Production backup taken before major changes\r\n- [ ] Schema drift detection in place\r\n- [ ] Migration rollback tested\r\n\r\n## Snippets (Generic)\r\n\r\n```\r\nMigration File Structure:\r\nmigrations/\r\n├── 001_create_users_table.sql\r\n├── 002_add_email_to_users.sql\r\n├── 003_create_orders_table.sql\r\n└── 004_add_user_id_index_orders.sql\r\n\r\nExpand-Contract Pattern:\r\nPhase 1 (Expand):\r\n  - Add new column (nullable)\r\n  - Backfill data\r\n  - Update app to write to both\r\n  \r\nPhase 2 (Migrate):\r\n  - Update app to read from new column\r\n  - Verify all data migrated\r\n\r\nPhase 3 (Contract):\r\n  - Remove old column references\r\n  - Drop old column\r\n\r\nSafe Index Creation (PostgreSQL):\r\nCREATE INDEX CONCURRENTLY idx_orders_user_id ON orders(user_id);\r\n\r\nSteps:\r\n1. Generate migration file (timestamped)\r\n2. Write UP migration (apply change)\r\n3. Write DOWN migration (rollback)\r\n4. Test in development\r\n5. Apply in staging\r\n6. Backup production\r\n7. Apply in production (during low traffic)\r\n8. Verify and monitor\r\n```\r\n\r\n## Sources\r\n\r\n- Flyway Documentation: https://documentation.red-gate.com/fd/flyway-documentation-138346877.html\r\n- GitHub - Expand and Contract: https://github.blog/2024-02-12-how-github-evolved-its-schema-to-support-notifications-at-scale/\r\n- Strong Migrations (Ruby): https://github.com/ankane/strong_migrations\r\n- Liquibase Best Practices: https://docs.liquibase.com/concepts/bestpractices.html\r\n"
  },
  {
    "id": "db-n-plus-1-avoid",
    "title": "Avoiding N+1 Query Problem",
    "tags": [
      "database",
      "performance",
      "orm",
      "optimization",
      "dataloader"
    ],
    "level": "intermediate",
    "stacks": [
      "all"
    ],
    "scope": "database",
    "maturity": "stable",
    "works_with": [
      "all"
    ],
    "path": "D:/Code/backend-engineering-kit/.shared/production-backend-kit/patterns/db.n-plus-1-avoid.md",
    "type": "pattern",
    "sections": [],
    "checklist": [],
    "sources": [],
    "rawContent": "\r\n# Avoiding N+1 Query Problem\r\n\r\n## Problem\r\n\r\nN+1 queries occur when code fetches a list, then executes one query per item to get related data. This causes 1 + N database roundtrips instead of 2, destroying performance. With 1000 items, you make 1001 queries instead of 2.\r\n\r\n## When to use\r\n\r\n- Using any ORM (Prisma, Drizzle, TypeORM, SQLAlchemy, Hibernate)\r\n- Fetching collections with relationships\r\n- API endpoints returning nested data\r\n- GraphQL resolvers\r\n- Anytime you loop and query\r\n\r\n## Solution\r\n\r\n### 1. Identify N+1 Patterns\r\n\r\n**Symptoms**:\r\n- Endpoint latency scales linearly with result size\r\n- Query log shows repeated similar queries\r\n- APM shows high query count per request\r\n\r\n**Detection Tools**:\r\n- PostgreSQL: `pg_stat_statements`\r\n- Node.js: `DEBUG=knex:query`\r\n- Python: SQLAlchemy echo mode\r\n- Ruby: Bullet gem\r\n- Java: Hibernate statistics\r\n\r\n### 2. Eager Loading (ORM)\r\n\r\n```typescript\r\n// Prisma - Bad (N+1)\r\nconst users = await prisma.user.findMany();\r\nfor (const user of users) {\r\n  const orders = await prisma.order.findMany({ where: { userId: user.id } });\r\n}\r\n\r\n// Prisma - Good (2 queries)\r\nconst users = await prisma.user.findMany({\r\n  include: { orders: true }\r\n});\r\n```\r\n\r\n```python\r\n# SQLAlchemy - Bad (N+1)\r\nusers = session.query(User).all()\r\nfor user in users:\r\n    print(user.orders)  # Triggers query each time!\r\n\r\n# SQLAlchemy - Good (eager load)\r\nusers = session.query(User).options(joinedload(User.orders)).all()\r\n```\r\n\r\n### 3. DataLoader Pattern (GraphQL)\r\n\r\nBatch and dedupe requests within a single tick:\r\n\r\n```typescript\r\n// DataLoader batches all user.id lookups\r\nconst userLoader = new DataLoader(async (userIds: string[]) => {\r\n  const users = await db.user.findMany({\r\n    where: { id: { in: userIds } }\r\n  });\r\n  // Return in same order as input\r\n  const userMap = new Map(users.map(u => [u.id, u]));\r\n  return userIds.map(id => userMap.get(id));\r\n});\r\n\r\n// In resolver\r\nconst resolvers = {\r\n  Order: {\r\n    user: (order, _, { loaders }) => loaders.user.load(order.userId)\r\n  }\r\n};\r\n```\r\n\r\n### 4. Raw SQL Solutions\r\n\r\n```sql\r\n-- Instead of N+1:\r\nSELECT * FROM users;  -- 1 query\r\nSELECT * FROM orders WHERE user_id = 1;  -- N queries\r\nSELECT * FROM orders WHERE user_id = 2;\r\n...\r\n\r\n-- Use JOIN (1 query, may duplicate user data):\r\nSELECT u.*, o.* FROM users u\r\nLEFT JOIN orders o ON o.user_id = u.id;\r\n\r\n-- Or IN clause (2 queries, cleaner):\r\nSELECT * FROM users;\r\nSELECT * FROM orders WHERE user_id IN (1, 2, 3, ...);\r\n```\r\n\r\n## Pitfalls\r\n\r\n| Pitfall | How to Avoid |\r\n|---------|--------------|\r\n| Over-eager loading | Only load what you need |\r\n| Lazy loading by default | Configure ORM for explicit loading |\r\n| Not monitoring queries | Enable query logging always |\r\n| Ignoring in tests | Tests often hide N+1 (small data) |\r\n| Nested N+1 | Check all relationship levels |\r\n\r\n## Checklist\r\n\r\n- [ ] Query logging enabled in development\r\n- [ ] N+1 detection tool configured\r\n- [ ] Eager loading used for known relationships\r\n- [ ] DataLoader used for GraphQL (or similar)\r\n- [ ] Code review checks for loops with queries\r\n- [ ] Lazy loading avoided by default\r\n- [ ] APM tracks query counts per request\r\n- [ ] Benchmarks use realistic data volume\r\n- [ ] SELECT fields limited to needed columns\r\n- [ ] Complex queries reviewed by DBA\r\n\r\n## Snippets (Generic)\r\n\r\n```\r\nN+1 Problem Example:\r\n-- First query: get all orders\r\nSELECT * FROM orders;\r\n\r\n-- Then for EACH order (N times):\r\nSELECT * FROM users WHERE id = ?;  -- Called N times!\r\n\r\nFixed with Eager Loading:\r\n-- Option 1: JOIN\r\nSELECT orders.*, users.* \r\nFROM orders \r\nJOIN users ON orders.user_id = users.id;\r\n\r\n-- Option 2: Separate IN query (2 queries total)\r\nSELECT * FROM orders;\r\nSELECT * FROM users WHERE id IN (1, 2, 3, 4, 5);\r\n\r\nORM Eager Loading (pseudo):\r\n-- Bad\r\norders = Order.all()\r\nfor order in orders:\r\n  print(order.user.name)  # Triggers query each time!\r\n\r\n-- Good\r\norders = Order.all().include('user')  # or .prefetch_related()\r\nfor order in orders:\r\n  print(order.user.name)  # No additional queries\r\n\r\nDataLoader Pattern:\r\n1. Collect all user_ids needed in request\r\n2. Batch into single query: WHERE id IN (...)\r\n3. Return results mapped by id\r\n4. Each resolver gets pre-fetched data\r\n\r\nDetection Steps:\r\n1. Enable query logging\r\n2. Run typical request\r\n3. Count queries (should be ~2-3 for list endpoint)\r\n4. If queries ~ item count, you have N+1\r\n5. Add eager loading or DataLoader\r\n```\r\n\r\n## Sources\r\n\r\n- Rails Bullet Gem: https://github.com/flyerhzm/bullet\r\n- SQLAlchemy Eager Loading: https://docs.sqlalchemy.org/en/14/orm/loading_relationships.html\r\n- GraphQL DataLoader: https://github.com/graphql/dataloader\r\n- Django select_related/prefetch_related: https://docs.djangoproject.com/en/5.0/ref/models/querysets/#select-related\r\n"
  },
  {
    "id": "db-schema-constraints",
    "title": "Database Schema Constraints",
    "tags": [
      "database",
      "schema",
      "constraints",
      "integrity"
    ],
    "level": "beginner",
    "stacks": [
      "postgresql",
      "mysql",
      "sqlserver"
    ],
    "scope": "database",
    "maturity": "stable",
    "works_with": [
      "postgresql",
      "mysql",
      "sqlserver"
    ],
    "path": "D:/Code/backend-engineering-kit/.shared/production-backend-kit/patterns/db.schema-constraints.md",
    "type": "pattern",
    "sections": [],
    "checklist": [],
    "sources": [],
    "rawContent": "\r\n# Database Schema Constraints\r\n\r\n## Problem\r\n\r\nWithout proper constraints, invalid data enters the database, causing application crashes, data corruption, and expensive cleanup operations. Business rules enforced only in application code can be bypassed.\r\n\r\n## When to use\r\n\r\n- Every database table design\r\n- Data integrity is critical\r\n- Multiple applications access same database\r\n- Preventing orphan records\r\n- Enforcing business rules at data layer\r\n\r\n## Solution\r\n\r\n1. **Primary key constraints**\r\n   - Every table needs a primary key\r\n   - Prefer surrogate keys (UUID, auto-increment)\r\n   - Composite keys for junction tables\r\n\r\n2. **Foreign key constraints**\r\n   - Define relationships explicitly\r\n   - Choose appropriate ON DELETE action\r\n   - Index foreign key columns\r\n\r\n3. **Unique constraints**\r\n   - Prevent duplicates on business keys\r\n   - Consider partial unique indexes\r\n   - Handle NULL behavior\r\n\r\n4. **Check constraints**\r\n   - Validate ranges, formats, enums\r\n   - Enforce business rules\r\n   - Keep simple for performance\r\n\r\n5. **Not null constraints**\r\n   - Default to NOT NULL\r\n   - Only allow NULL when meaningful\r\n   - Document NULL semantics\r\n\r\n## Pitfalls\r\n\r\n| Pitfall | How to Avoid |\r\n|---------|--------------|\r\n| No foreign key indexes | Always index FK columns |\r\n| CASCADE DELETE on critical data | Use RESTRICT or SET NULL |\r\n| Over-constraining early | Start strict, relax if needed |\r\n| Ignoring NULL in unique | Use partial index or COALESCE |\r\n| Complex check constraints | Move complex logic to triggers/app |\r\n\r\n## Checklist\r\n\r\n- [ ] Every table has primary key\r\n- [ ] Foreign keys defined for relationships\r\n- [ ] Foreign key columns indexed\r\n- [ ] ON DELETE behavior explicitly chosen\r\n- [ ] Unique constraints on business keys\r\n- [ ] NOT NULL on required fields\r\n- [ ] Check constraints for simple validations\r\n- [ ] Constraint names are descriptive\r\n- [ ] Migration tests validate constraints\r\n- [ ] Constraints documented in schema\r\n\r\n## Snippets (Generic)\r\n\r\n```\r\nTable Definition:\r\nCREATE TABLE orders (\r\n  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\r\n  user_id UUID NOT NULL REFERENCES users(id) ON DELETE RESTRICT,\r\n  status VARCHAR(20) NOT NULL CHECK (status IN ('pending', 'paid', 'shipped')),\r\n  total_cents INTEGER NOT NULL CHECK (total_cents >= 0),\r\n  email VARCHAR(255) NOT NULL,\r\n  created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),\r\n  \r\n  CONSTRAINT orders_user_id_idx INDEX (user_id),\r\n  CONSTRAINT orders_email_unique UNIQUE (email)\r\n);\r\n\r\nON DELETE Options:\r\n- RESTRICT: Prevent delete if children exist\r\n- CASCADE: Delete children automatically\r\n- SET NULL: Set FK to NULL on parent delete\r\n- SET DEFAULT: Set FK to default value\r\n\r\nSteps:\r\n1. Define primary key (UUID or serial)\r\n2. Add foreign keys with ON DELETE behavior\r\n3. Create indexes on foreign key columns\r\n4. Add NOT NULL to required fields\r\n5. Add unique constraints on business keys\r\n6. Add check constraints for enums/ranges\r\n```\r\n\r\n## Sources\r\n\r\n- PostgreSQL Constraints: https://www.postgresql.org/docs/current/ddl-constraints.html\r\n- MySQL Foreign Keys: https://dev.mysql.com/doc/refman/8.0/en/create-table-foreign-keys.html\r\n- Use The Index, Luke: https://use-the-index-luke.com/\r\n- Database Design Best Practices: https://vertabelo.com/blog/database-design-best-practices/\r\n"
  },
  {
    "id": "db-soft-delete-audit",
    "title": "Soft Delete & Audit Trail",
    "tags": [
      "database",
      "soft-delete",
      "auditing",
      "compliance"
    ],
    "level": "intermediate",
    "stacks": [
      "all"
    ],
    "scope": "database",
    "maturity": "stable",
    "works_with": [
      "all"
    ],
    "path": "D:/Code/backend-engineering-kit/.shared/production-backend-kit/patterns/db.soft-delete-audit.md",
    "type": "pattern",
    "sections": [],
    "checklist": [],
    "sources": [],
    "rawContent": "\r\n# Soft Delete & Audit Trail\r\n\r\n## Problem\r\n\r\nHard deletes permanently lose data, making debugging, compliance, and recovery impossible. Without audit trails, you can't answer \"who changed what and when.\"\r\n\r\n## When to use\r\n\r\n- Regulatory compliance (GDPR, SOX, HIPAA)\r\n- Data recovery requirements\r\n- Undo/restore functionality\r\n- Historical analysis and reporting\r\n- Debugging production issues\r\n- Multi-tenant applications\r\n\r\n## Solution\r\n\r\n1. **Implement soft delete**\r\n   - Add `deleted_at` timestamp column\r\n   - Filter out soft-deleted in default queries\r\n   - Keep foreign key integrity\r\n   - Consider global query scope/filter\r\n\r\n2. **Create audit trail**\r\n   - Track who, what, when for changes\r\n   - Store old and new values\r\n   - Use triggers or application middleware\r\n   - Consider event sourcing for critical domains\r\n\r\n3. **Handle related data**\r\n   - Cascade soft-delete to children\r\n   - Consider restore implications\r\n   - Archive old soft-deleted data\r\n\r\n4. **Set retention policy**\r\n   - Define how long to keep deleted data\r\n   - Automate hard-delete after retention period\r\n   - Consider compliance requirements\r\n\r\n## Pitfalls\r\n\r\n| Pitfall | How to Avoid |\r\n|---------|--------------|\r\n| Forgetting filter in queries | Use global scope/default clause |\r\n| Unique constraint conflicts | Include deleted_at in unique indexes |\r\n| Performance degradation | Partition or archive old records |\r\n| Missing FK in soft-delete | Soft-delete must cascade appropriately |\r\n| Audit table bloat | Archive or partition audit logs |\r\n\r\n## Checklist\r\n\r\n- [ ] deleted_at column added to relevant tables\r\n- [ ] Default query scope filters deleted records\r\n- [ ] Unique indexes include deleted_at\r\n- [ ] Audit table captures who/what/when\r\n- [ ] Old and new values stored in audit\r\n- [ ] Foreign key implications handled\r\n- [ ] Restore functionality tested\r\n- [ ] Retention policy defined\r\n- [ ] Archive strategy for old data\r\n- [ ] GDPR right-to-erasure considered\r\n\r\n## Snippets (Generic)\r\n\r\n```\r\nSoft Delete Schema:\r\nCREATE TABLE users (\r\n  id UUID PRIMARY KEY,\r\n  email VARCHAR(255) NOT NULL,\r\n  name VARCHAR(255),\r\n  created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),\r\n  updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),\r\n  deleted_at TIMESTAMPTZ NULL  -- NULL = not deleted\r\n);\r\n\r\n-- Unique that allows \"deleted\" duplicates\r\nCREATE UNIQUE INDEX idx_users_email_active \r\n  ON users(email) WHERE deleted_at IS NULL;\r\n\r\nAudit Table Schema:\r\nCREATE TABLE audit_log (\r\n  id UUID PRIMARY KEY,\r\n  table_name VARCHAR(100) NOT NULL,\r\n  record_id UUID NOT NULL,\r\n  action VARCHAR(10) NOT NULL,  -- INSERT, UPDATE, DELETE\r\n  old_values JSONB,\r\n  new_values JSONB,\r\n  changed_by UUID REFERENCES users(id),\r\n  changed_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),\r\n  ip_address INET,\r\n  user_agent TEXT\r\n);\r\n\r\nSoft Delete Query:\r\n-- Default: only active records\r\nSELECT * FROM users WHERE deleted_at IS NULL;\r\n\r\n-- Include deleted (admin view)\r\nSELECT * FROM users;\r\n\r\n-- Only deleted (recovery view)\r\nSELECT * FROM users WHERE deleted_at IS NOT NULL;\r\n\r\nSteps:\r\n1. Add deleted_at column (nullable timestamp)\r\n2. Update unique constraints to be partial\r\n3. Add default scope to filter deleted\r\n4. Create audit_log table\r\n5. Implement trigger or middleware for auditing\r\n6. Define retention and archive strategy\r\n```\r\n\r\n## Sources\r\n\r\n- Soft Delete Patterns: https://brandur.org/soft-deletion\r\n- PostgreSQL Audit Trigger: https://wiki.postgresql.org/wiki/Audit_trigger\r\n- Django Simple History: https://django-simple-history.readthedocs.io/\r\n- Database Audit Logging (OWASP): https://cheatsheetseries.owasp.org/cheatsheets/Logging_Cheat_Sheet.html\r\n"
  },
  {
    "id": "db-transactions-boundaries",
    "title": "Transaction Boundaries",
    "tags": [
      "database",
      "transactions",
      "acid",
      "consistency",
      "isolation"
    ],
    "level": "intermediate",
    "stacks": [
      "all"
    ],
    "scope": "database",
    "maturity": "stable",
    "works_with": [
      "all"
    ],
    "path": "D:/Code/backend-engineering-kit/.shared/production-backend-kit/patterns/db.transactions-boundaries.md",
    "type": "pattern",
    "sections": [],
    "checklist": [],
    "sources": [],
    "rawContent": "\r\n# Transaction Boundaries\r\n\r\n## Problem\r\n\r\nIncorrect transaction boundaries lead to partial updates, data inconsistency, and race conditions. Too-long transactions cause lock contention; too-short transactions cause inconsistent state. Understanding isolation levels is critical for correctness.\r\n\r\n## When to use\r\n\r\n- Multiple related writes that must succeed together\r\n- Read-modify-write operations\r\n- Business operations with consistency requirements\r\n- Cross-table updates\r\n- Financial or inventory operations\r\n\r\n## Solution\r\n\r\n### 1. Transaction Boundary Rules\r\n\r\n| Rule | Description |\r\n|------|-------------|\r\n| **One business operation** | Transaction = one logical operation |\r\n| **Keep short** | Target < 1 second, max 30 seconds |\r\n| **No external calls** | HTTP, queue, file I/O outside transaction |\r\n| **Atomic write batches** | Group related writes |\r\n| **Explicit boundaries** | Don't rely on auto-commit |\r\n\r\n### 2. Isolation Levels\r\n\r\n| Level | Dirty Read | Non-Repeatable Read | Phantom Read | Use Case |\r\n|-------|------------|---------------------|--------------|----------|\r\n| **Read Uncommitted** | ✔ | ✔ | ✔ | Never use |\r\n| **Read Committed** | ✖ | ✔ | ✔ | Default, most apps |\r\n| **Repeatable Read** | ✖ | ✖ | ✔* | Reports, analytics |\r\n| **Serializable** | ✖ | ✖ | ✖ | Financial, critical |\r\n\r\n*PostgreSQL's Repeatable Read also prevents phantoms.\r\n\r\n```sql\r\n-- PostgreSQL: Set for session\r\nSET SESSION CHARACTERISTICS AS TRANSACTION ISOLATION LEVEL REPEATABLE READ;\r\n\r\n-- Or per transaction\r\nBEGIN ISOLATION LEVEL SERIALIZABLE;\r\n  -- operations\r\nCOMMIT;\r\n```\r\n\r\n### 3. Implementation Patterns\r\n\r\n**Basic Transaction (TypeScript/Prisma):**\r\n```typescript\r\nasync function transferFunds(\r\n  fromAccountId: string,\r\n  toAccountId: string,\r\n  amount: number\r\n): Promise<void> {\r\n  await prisma.$transaction(async (tx) => {\r\n    // Debit source account\r\n    const from = await tx.account.update({\r\n      where: { id: fromAccountId },\r\n      data: { balance: { decrement: amount } },\r\n    });\r\n    \r\n    if (from.balance < 0) {\r\n      throw new InsufficientFundsError();\r\n    }\r\n    \r\n    // Credit destination account\r\n    await tx.account.update({\r\n      where: { id: toAccountId },\r\n      data: { balance: { increment: amount } },\r\n    });\r\n    \r\n    // Record transfer\r\n    await tx.transfer.create({\r\n      data: { fromAccountId, toAccountId, amount },\r\n    });\r\n  }, {\r\n    isolationLevel: 'Serializable',\r\n    timeout: 10000, // 10 seconds max\r\n  });\r\n}\r\n```\r\n\r\n**Optimistic Locking:**\r\n```typescript\r\nasync function updateOrderWithOptimisticLock(\r\n  orderId: string,\r\n  updates: Partial<Order>,\r\n  expectedVersion: number\r\n): Promise<Order> {\r\n  const result = await prisma.order.updateMany({\r\n    where: {\r\n      id: orderId,\r\n      version: expectedVersion, // Only update if version matches\r\n    },\r\n    data: {\r\n      ...updates,\r\n      version: { increment: 1 },\r\n    },\r\n  });\r\n  \r\n  if (result.count === 0) {\r\n    throw new OptimisticLockError(\r\n      'Order was modified by another process. Please retry.'\r\n    );\r\n  }\r\n  \r\n  return prisma.order.findUnique({ where: { id: orderId } });\r\n}\r\n```\r\n\r\n**SELECT FOR UPDATE (Pessimistic Locking):**\r\n```typescript\r\nasync function processInventory(productId: string, quantity: number): Promise<void> {\r\n  await prisma.$transaction(async (tx) => {\r\n    // Lock the row for update\r\n    const [inventory] = await tx.$queryRaw<Inventory[]>`\r\n      SELECT * FROM inventory \r\n      WHERE product_id = ${productId}\r\n      FOR UPDATE NOWAIT\r\n    `;\r\n    \r\n    if (!inventory || inventory.quantity < quantity) {\r\n      throw new InsufficientInventoryError();\r\n    }\r\n    \r\n    await tx.inventory.update({\r\n      where: { productId },\r\n      data: { quantity: { decrement: quantity } },\r\n    });\r\n  });\r\n}\r\n```\r\n\r\n### 4. Deadlock Handling\r\n\r\n```typescript\r\nasync function executeWithRetry<T>(\r\n  operation: () => Promise<T>,\r\n  maxRetries: number = 3\r\n): Promise<T> {\r\n  let lastError: Error;\r\n  \r\n  for (let attempt = 1; attempt <= maxRetries; attempt++) {\r\n    try {\r\n      return await operation();\r\n    } catch (error) {\r\n      lastError = error;\r\n      \r\n      // Check if deadlock or serialization failure\r\n      const isRetryable = \r\n        error.code === '40001' || // Serialization failure (Postgres)\r\n        error.code === '40P01' || // Deadlock detected (Postgres)\r\n        error.code === 'ER_LOCK_DEADLOCK'; // MySQL deadlock\r\n      \r\n      if (!isRetryable || attempt === maxRetries) {\r\n        throw error;\r\n      }\r\n      \r\n      // Exponential backoff with jitter\r\n      const delay = Math.min(100 * Math.pow(2, attempt), 5000);\r\n      const jitter = Math.random() * delay * 0.1;\r\n      await sleep(delay + jitter);\r\n      \r\n      logger.warn({ attempt, error: error.code }, 'Retrying after deadlock');\r\n    }\r\n  }\r\n  \r\n  throw lastError!;\r\n}\r\n\r\n// Usage\r\nawait executeWithRetry(() => transferFunds(from, to, amount));\r\n```\r\n\r\n### 5. Anti-Patterns to Avoid\r\n\r\n```typescript\r\n// ❌ BAD: External call inside transaction\r\nawait prisma.$transaction(async (tx) => {\r\n  await tx.order.update({ where: { id }, data: { status: 'paid' } });\r\n  await stripe.charges.create({ amount: 1000 }); // HTTP call!\r\n  await tx.payment.create({ data: { orderId: id } });\r\n});\r\n\r\n// ✅ GOOD: External call outside, use outbox pattern\r\nconst order = await prisma.$transaction(async (tx) => {\r\n  const order = await tx.order.update({ where: { id }, data: { status: 'processing' } });\r\n  // Write to outbox for later processing\r\n  await tx.outbox.create({\r\n    data: {\r\n      eventType: 'OrderPaymentRequested',\r\n      payload: { orderId: id, amount: 1000 },\r\n    },\r\n  });\r\n  return order;\r\n});\r\n// Background worker processes outbox and calls Stripe\r\n```\r\n\r\n## Pitfalls\r\n\r\n| Pitfall | How to Avoid |\r\n|---------|--------------|\r\n| HTTP calls inside transaction | Move external calls outside |\r\n| Long-running transactions | Keep transactions short (< 1 sec) |\r\n| Not handling deadlocks | Implement retry logic |\r\n| Mixing read-write isolation | Understand isolation trade-offs |\r\n| Auto-commit mode surprises | Explicitly manage transactions |\r\n\r\n## Checklist\r\n\r\n- [ ] Transaction boundaries match business operations\r\n- [ ] Transactions kept as short as possible\r\n- [ ] No external calls inside transactions\r\n- [ ] Isolation level explicitly chosen\r\n- [ ] Error handling includes rollback\r\n- [ ] Deadlock retry logic implemented\r\n- [ ] Optimistic locking used where appropriate\r\n- [ ] Connection pool sized correctly\r\n- [ ] Transaction timeouts configured\r\n- [ ] Nested transactions avoided or handled\r\n\r\n## Snippets (Generic)\r\n\r\n```\r\nBasic Transaction Pattern:\r\nBEGIN TRANSACTION;\r\n  UPDATE accounts SET balance = balance - 100 WHERE id = 1;\r\n  UPDATE accounts SET balance = balance + 100 WHERE id = 2;\r\n  INSERT INTO transfers (from_id, to_id, amount) VALUES (1, 2, 100);\r\nCOMMIT;\r\n-- On any error: ROLLBACK;\r\n\r\nApplication Code Pattern:\r\ntry:\r\n  tx = db.begin_transaction()\r\n  account_from = tx.update(account1, balance=balance - amount)\r\n  account_to = tx.update(account2, balance=balance + amount)\r\n  tx.insert(Transfer(from=1, to=2, amount=amount))\r\n  tx.commit()\r\nexcept Exception:\r\n  tx.rollback()\r\n  raise\r\n\r\nOptimistic Locking:\r\nUPDATE products \r\nSET quantity = quantity - 1, version = version + 1\r\nWHERE id = 123 AND version = 5;\r\n-- If affected_rows == 0, retry (someone else updated)\r\n\r\nIsolation Levels:\r\n- READ UNCOMMITTED: Dirty reads possible (avoid!)\r\n- READ COMMITTED: Only committed data visible\r\n- REPEATABLE READ: Snapshot at transaction start\r\n- SERIALIZABLE: Full isolation, may have failures\r\n\r\nSteps:\r\n1. Identify business operation boundaries\r\n2. Wrap related writes in single transaction\r\n3. Choose appropriate isolation level\r\n4. Add error handling with rollback\r\n5. Implement retry for transient failures\r\n6. Monitor for lock contention\r\n```\r\n\r\n## Sources\r\n\r\n- PostgreSQL Transaction Isolation: https://www.postgresql.org/docs/current/transaction-iso.html\r\n- Martin Kleppmann - Designing Data-Intensive Applications: https://dataintensive.net/\r\n- MySQL Transaction Management: https://dev.mysql.com/doc/refman/8.0/en/innodb-transaction-model.html\r\n- Jepsen Analysis (Consistency Testing): https://jepsen.io/analyses\r\n"
  },
  {
    "id": "obs-alerting-strategies",
    "title": "Alerting Strategies & Best Practices",
    "tags": [
      "observability",
      "alerting",
      "monitoring",
      "sre",
      "on-call"
    ],
    "level": "intermediate",
    "stacks": [
      "all"
    ],
    "scope": "observability",
    "maturity": "stable",
    "works_with": [
      "all"
    ],
    "path": "D:/Code/backend-engineering-kit/.shared/production-backend-kit/patterns/obs.alerting-strategies.md",
    "type": "pattern",
    "sections": [],
    "checklist": [],
    "sources": [],
    "rawContent": "\r\n# Alerting Strategies & Best Practices\r\n\r\n## Problem\r\n\r\nPoor alerting leads to:\r\n- **Alert fatigue**: Too many alerts → engineers ignore them all\r\n- **Missed incidents**: Not alerting on what matters\r\n- **Slow response**: Unclear alerts delay diagnosis\r\n- **Burnout**: Constant pages for non-issues\r\n- **Over-engineering**: Alerts on every metric\r\n\r\n**\"The only purpose of an alert is to bring a human into the loop.\"**\r\n— Google SRE Book\r\n\r\n## When to use\r\n\r\n- Any production system\r\n- SLO/SLI-based monitoring\r\n- On-call rotations\r\n- Business-critical services\r\n- Infrastructure monitoring\r\n\r\n## Solution\r\n\r\n### 1. Alert Philosophy\r\n\r\n```\r\n┌─────────────────────────────────────────────────────────────────────────────┐\r\n│                        ALERTING PRINCIPLES                                  │\r\n├─────────────────────────────────────────────────────────────────────────────┤\r\n│                                                                             │\r\n│  ✓ Alert on SYMPTOMS, not causes                                           │\r\n│    Bad:  \"CPU > 80%\"                                                        │\r\n│    Good: \"Error rate > 1%\" or \"Latency p99 > 500ms\"                        │\r\n│                                                                             │\r\n│  ✓ Alert on USER IMPACT                                                    │\r\n│    Bad:  \"Pod restarted\"                                                    │\r\n│    Good: \"Users seeing errors\" or \"Checkout failures increasing\"            │\r\n│                                                                             │\r\n│  ✓ Alert should be ACTIONABLE                                              │\r\n│    Ask: \"What will the on-call DO when paged?\"                              │\r\n│    If no action needed → don't alert                                        │\r\n│                                                                             │\r\n│  ✓ Every alert needs a RUNBOOK                                             │\r\n│    Clear steps to diagnose and mitigate                                     │\r\n│                                                                             │\r\n│  ✓ Alerts should be URGENT                                                 │\r\n│    Page: Requires immediate human action                                    │\r\n│    Ticket: Can wait until business hours                                    │\r\n│    Log: Informational, no action needed                                     │\r\n│                                                                             │\r\n└─────────────────────────────────────────────────────────────────────────────┘\r\n```\r\n\r\n### 2. SLO-Based Alerting (Recommended)\r\n\r\n```yaml\r\n# Alert when burning through error budget too fast\r\n# This is the modern, recommended approach\r\n\r\n# SLO: 99.9% availability over 30 days\r\n# Error budget: 0.1% = 43.2 minutes/month\r\n\r\n# Multi-window, multi-burn-rate alerts\r\ngroups:\r\n  - name: slo-alerts\r\n    rules:\r\n      # Fast burn: 14.4x burn rate over 1 hour\r\n      # Will exhaust budget in ~2 days\r\n      - alert: HighErrorBudgetBurn_Fast\r\n        expr: |\r\n          (\r\n            sum(rate(http_requests_total{status=~\"5..\"}[1h]))\r\n            / sum(rate(http_requests_total[1h]))\r\n          ) > (14.4 * 0.001)\r\n          and\r\n          (\r\n            sum(rate(http_requests_total{status=~\"5..\"}[5m]))\r\n            / sum(rate(http_requests_total[5m]))\r\n          ) > (14.4 * 0.001)\r\n        for: 2m\r\n        labels:\r\n          severity: critical\r\n        annotations:\r\n          summary: \"High error rate - burning error budget fast\"\r\n          description: \"Error rate is {{ $value | humanizePercentage }}, consuming error budget at 14x normal rate\"\r\n          runbook_url: \"https://runbooks.example.com/high-error-rate\"\r\n          dashboard: \"https://grafana.example.com/d/slo-dashboard\"\r\n\r\n      # Slow burn: 3x burn rate over 3 days\r\n      # Will exhaust budget in ~10 days\r\n      - alert: HighErrorBudgetBurn_Slow\r\n        expr: |\r\n          (\r\n            sum(rate(http_requests_total{status=~\"5..\"}[3d]))\r\n            / sum(rate(http_requests_total[3d]))\r\n          ) > (3 * 0.001)\r\n        for: 1h\r\n        labels:\r\n          severity: warning\r\n        annotations:\r\n          summary: \"Elevated error rate - slowly burning error budget\"\r\n          description: \"Error rate elevated for extended period\"\r\n```\r\n\r\n### 3. Alert Severity Levels\r\n\r\n```typescript\r\n// Define clear severity levels\r\nenum AlertSeverity {\r\n  CRITICAL = 'critical',  // Page immediately, 24/7\r\n  WARNING = 'warning',    // Page during business hours, or ticket\r\n  INFO = 'info',          // Log, no notification\r\n}\r\n\r\ninterface AlertDefinition {\r\n  name: string;\r\n  severity: AlertSeverity;\r\n  condition: string;\r\n  duration: string;       // How long condition must be true\r\n  runbookUrl: string;\r\n  notificationChannels: string[];\r\n  autoResolve: boolean;\r\n}\r\n\r\nconst alerts: AlertDefinition[] = [\r\n  // CRITICAL: Immediate page\r\n  {\r\n    name: 'ServiceDown',\r\n    severity: AlertSeverity.CRITICAL,\r\n    condition: 'up == 0',\r\n    duration: '1m',\r\n    runbookUrl: 'https://runbooks/service-down',\r\n    notificationChannels: ['pagerduty', 'slack-critical'],\r\n    autoResolve: true,\r\n  },\r\n  {\r\n    name: 'HighErrorRate',\r\n    severity: AlertSeverity.CRITICAL,\r\n    condition: 'error_rate > 0.05', // 5% errors\r\n    duration: '5m',\r\n    runbookUrl: 'https://runbooks/high-errors',\r\n    notificationChannels: ['pagerduty', 'slack-critical'],\r\n    autoResolve: true,\r\n  },\r\n  \r\n  // WARNING: Business hours or ticket\r\n  {\r\n    name: 'ElevatedLatency',\r\n    severity: AlertSeverity.WARNING,\r\n    condition: 'p99_latency > 1000', // 1 second\r\n    duration: '15m',\r\n    runbookUrl: 'https://runbooks/high-latency',\r\n    notificationChannels: ['slack-warnings', 'ticket'],\r\n    autoResolve: true,\r\n  },\r\n  {\r\n    name: 'DiskSpaceLow',\r\n    severity: AlertSeverity.WARNING,\r\n    condition: 'disk_used_percent > 80',\r\n    duration: '30m',\r\n    runbookUrl: 'https://runbooks/disk-space',\r\n    notificationChannels: ['slack-warnings'],\r\n    autoResolve: true,\r\n  },\r\n  \r\n  // INFO: Log only\r\n  {\r\n    name: 'DeploymentStarted',\r\n    severity: AlertSeverity.INFO,\r\n    condition: 'deployment_in_progress == 1',\r\n    duration: '0m',\r\n    runbookUrl: '',\r\n    notificationChannels: ['slack-deployments'],\r\n    autoResolve: true,\r\n  },\r\n];\r\n```\r\n\r\n### 4. Alert Content Template\r\n\r\n```yaml\r\n# Good alert has all context needed\r\n- alert: PaymentServiceErrors\r\n  expr: |\r\n    sum(rate(payment_requests_total{status=\"error\"}[5m])) \r\n    / sum(rate(payment_requests_total[5m])) > 0.01\r\n  for: 5m\r\n  labels:\r\n    severity: critical\r\n    team: payments\r\n    service: payment-service\r\n  annotations:\r\n    # Short, scannable summary\r\n    summary: \"Payment service error rate > 1%\"\r\n    \r\n    # Detailed description with current value\r\n    description: |\r\n      Payment service error rate is {{ $value | humanizePercentage }}.\r\n      This affects customer checkout flow.\r\n      \r\n      Triggered at: {{ .StartsAt }}\r\n      Current value: {{ $value | humanizePercentage }}\r\n    \r\n    # Impact statement\r\n    impact: \"Customers may be unable to complete purchases\"\r\n    \r\n    # Direct links to everything needed\r\n    runbook_url: \"https://runbooks.example.com/payment-errors\"\r\n    dashboard_url: \"https://grafana.example.com/d/payments?from=now-1h\"\r\n    logs_url: \"https://logs.example.com/search?query=service:payment+level:error\"\r\n    \r\n    # Who to escalate to\r\n    escalation: \"payments-oncall → payments-lead → vp-engineering\"\r\n```\r\n\r\n### 5. Prometheus Alert Examples\r\n\r\n```yaml\r\ngroups:\r\n  - name: application-alerts\r\n    rules:\r\n      # Availability alert\r\n      - alert: ServiceUnavailable\r\n        expr: up{job=\"api-server\"} == 0\r\n        for: 1m\r\n        labels:\r\n          severity: critical\r\n        annotations:\r\n          summary: \"Service {{ $labels.instance }} is down\"\r\n          runbook_url: \"https://runbooks/service-down\"\r\n\r\n      # Error rate alert (symptom-based)\r\n      - alert: HighErrorRate\r\n        expr: |\r\n          sum(rate(http_requests_total{status=~\"5..\"}[5m])) by (service)\r\n          / sum(rate(http_requests_total[5m])) by (service) > 0.01\r\n        for: 5m\r\n        labels:\r\n          severity: critical\r\n        annotations:\r\n          summary: \"High error rate on {{ $labels.service }}\"\r\n          description: \"Error rate is {{ $value | humanizePercentage }}\"\r\n\r\n      # Latency alert (symptom-based)\r\n      - alert: HighLatency\r\n        expr: |\r\n          histogram_quantile(0.99, \r\n            sum(rate(http_request_duration_seconds_bucket[5m])) by (le, service)\r\n          ) > 1\r\n        for: 10m\r\n        labels:\r\n          severity: warning\r\n        annotations:\r\n          summary: \"High latency on {{ $labels.service }}\"\r\n          description: \"p99 latency is {{ $value | humanizeDuration }}\"\r\n\r\n      # Saturation alert\r\n      - alert: HighMemoryUsage\r\n        expr: |\r\n          (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes)\r\n          / node_memory_MemTotal_bytes > 0.9\r\n        for: 15m\r\n        labels:\r\n          severity: warning\r\n        annotations:\r\n          summary: \"High memory usage on {{ $labels.instance }}\"\r\n          description: \"Memory usage is {{ $value | humanizePercentage }}\"\r\n\r\n      # Queue depth (leading indicator)\r\n      - alert: HighQueueDepth\r\n        expr: |\r\n          sum(rabbitmq_queue_messages) by (queue) > 10000\r\n        for: 10m\r\n        labels:\r\n          severity: warning\r\n        annotations:\r\n          summary: \"Queue {{ $labels.queue }} backing up\"\r\n          description: \"{{ $value }} messages in queue\"\r\n\r\n      # Certificate expiry (proactive)\r\n      - alert: CertificateExpiringSoon\r\n        expr: |\r\n          (probe_ssl_earliest_cert_expiry - time()) / 86400 < 14\r\n        for: 1h\r\n        labels:\r\n          severity: warning\r\n        annotations:\r\n          summary: \"SSL certificate expiring soon\"\r\n          description: \"Certificate for {{ $labels.instance }} expires in {{ $value | humanize }} days\"\r\n\r\n  - name: business-alerts\r\n    rules:\r\n      # Business metric alert\r\n      - alert: LowConversionRate\r\n        expr: |\r\n          sum(rate(checkout_completed_total[1h]))\r\n          / sum(rate(checkout_started_total[1h])) < 0.3\r\n        for: 30m\r\n        labels:\r\n          severity: warning\r\n          team: product\r\n        annotations:\r\n          summary: \"Checkout conversion rate dropped below 30%\"\r\n\r\n      # Revenue alert\r\n      - alert: RevenueAnomaly\r\n        expr: |\r\n          abs(\r\n            sum(rate(order_total_usd[1h]))\r\n            - sum(rate(order_total_usd[1h] offset 1d))\r\n          ) / sum(rate(order_total_usd[1h] offset 1d)) > 0.5\r\n        for: 1h\r\n        labels:\r\n          severity: warning\r\n        annotations:\r\n          summary: \"Revenue changed by more than 50% vs yesterday\"\r\n```\r\n\r\n### 6. Alert Routing & Escalation\r\n\r\n```yaml\r\n# Alertmanager configuration\r\nglobal:\r\n  resolve_timeout: 5m\r\n  slack_api_url: 'https://hooks.slack.com/services/xxx'\r\n  pagerduty_url: 'https://events.pagerduty.com/v2/enqueue'\r\n\r\nroute:\r\n  # Default route\r\n  receiver: 'slack-default'\r\n  group_by: ['alertname', 'service']\r\n  group_wait: 30s      # Wait before first notification\r\n  group_interval: 5m   # Time between grouped notifications\r\n  repeat_interval: 4h  # How often to re-send\r\n  \r\n  routes:\r\n    # Critical → PagerDuty immediately\r\n    - match:\r\n        severity: critical\r\n      receiver: 'pagerduty-critical'\r\n      group_wait: 10s\r\n      repeat_interval: 1h\r\n      \r\n    # Payments team critical → dedicated PagerDuty\r\n    - match:\r\n        severity: critical\r\n        team: payments\r\n      receiver: 'pagerduty-payments'\r\n      \r\n    # Warnings → Slack during business hours\r\n    - match:\r\n        severity: warning\r\n      receiver: 'slack-warnings'\r\n      active_time_intervals:\r\n        - business-hours\r\n      \r\n    # Security alerts → immediate + security channel\r\n    - match:\r\n        category: security\r\n      receiver: 'security-team'\r\n      group_wait: 0s\r\n\r\nreceivers:\r\n  - name: 'slack-default'\r\n    slack_configs:\r\n      - channel: '#alerts'\r\n        send_resolved: true\r\n        title: '{{ .Status | toUpper }}: {{ .CommonAnnotations.summary }}'\r\n        text: '{{ .CommonAnnotations.description }}'\r\n        \r\n  - name: 'pagerduty-critical'\r\n    pagerduty_configs:\r\n      - service_key: 'xxx'\r\n        severity: critical\r\n        description: '{{ .CommonAnnotations.summary }}'\r\n        details:\r\n          runbook: '{{ .CommonAnnotations.runbook_url }}'\r\n          dashboard: '{{ .CommonAnnotations.dashboard_url }}'\r\n\r\n  - name: 'slack-warnings'\r\n    slack_configs:\r\n      - channel: '#alerts-warnings'\r\n        send_resolved: true\r\n\r\ntime_intervals:\r\n  - name: business-hours\r\n    time_intervals:\r\n      - weekdays: ['monday:friday']\r\n        times:\r\n          - start_time: '09:00'\r\n            end_time: '18:00'\r\n\r\n# Inhibition rules - suppress related alerts\r\ninhibit_rules:\r\n  # If service is down, don't alert on latency\r\n  - source_match:\r\n      alertname: ServiceUnavailable\r\n    target_match:\r\n      alertname: HighLatency\r\n    equal: ['service']\r\n    \r\n  # If cluster is down, don't alert on individual nodes\r\n  - source_match:\r\n      alertname: ClusterDown\r\n    target_match_re:\r\n      alertname: 'Node.*'\r\n    equal: ['cluster']\r\n```\r\n\r\n### 7. Alert Testing & Validation\r\n\r\n```typescript\r\n// Test alerts before deploying\r\nimport { PrometheusRulesValidator } from 'prom-rules-validator';\r\n\r\ndescribe('Alert Rules', () => {\r\n  const validator = new PrometheusRulesValidator();\r\n  \r\n  it('should have valid syntax', async () => {\r\n    const result = await validator.validate('./alerts/*.yaml');\r\n    expect(result.errors).toHaveLength(0);\r\n  });\r\n  \r\n  it('should have required annotations', async () => {\r\n    const rules = await loadAlertRules('./alerts/*.yaml');\r\n    \r\n    for (const rule of rules) {\r\n      expect(rule.annotations).toHaveProperty('summary');\r\n      expect(rule.annotations).toHaveProperty('runbook_url');\r\n      expect(rule.annotations.runbook_url).toMatch(/^https:\\/\\//);\r\n    }\r\n  });\r\n  \r\n  it('should have valid severity labels', async () => {\r\n    const rules = await loadAlertRules('./alerts/*.yaml');\r\n    const validSeverities = ['critical', 'warning', 'info'];\r\n    \r\n    for (const rule of rules) {\r\n      expect(validSeverities).toContain(rule.labels.severity);\r\n    }\r\n  });\r\n  \r\n  it('critical alerts should have PagerDuty routing', async () => {\r\n    const config = await loadAlertmanagerConfig();\r\n    const criticalRoute = config.route.routes.find(\r\n      r => r.match?.severity === 'critical'\r\n    );\r\n    \r\n    expect(criticalRoute?.receiver).toContain('pagerduty');\r\n  });\r\n});\r\n\r\n// Chaos testing for alerts\r\ndescribe('Alert Triggers', () => {\r\n  it('should fire HighErrorRate when errors spike', async () => {\r\n    // Inject errors\r\n    await injectErrors({ rate: 0.1, duration: '10m' });\r\n    \r\n    // Wait for alert\r\n    const alert = await waitForAlert('HighErrorRate', { timeout: '15m' });\r\n    expect(alert).toBeTruthy();\r\n    expect(alert.labels.severity).toBe('critical');\r\n  });\r\n});\r\n```\r\n\r\n### 8. Alert Hygiene & Review\r\n\r\n```typescript\r\n// Weekly alert review metrics\r\ninterface AlertMetrics {\r\n  totalAlerts: number;\r\n  uniqueAlerts: number;\r\n  meanTimeToAcknowledge: number;\r\n  meanTimeToResolve: number;\r\n  falsePositiveRate: number;\r\n  alertsPerOnCall: number;\r\n  noisyAlerts: string[];  // Alerts that fired > 10 times\r\n  staleAlerts: string[];  // Alerts that never fired\r\n}\r\n\r\nasync function generateAlertReport(period: string): Promise<AlertMetrics> {\r\n  const alerts = await queryAlertHistory(period);\r\n  \r\n  // Find noisy alerts (alert fatigue candidates)\r\n  const alertCounts = countBy(alerts, 'alertname');\r\n  const noisyAlerts = Object.entries(alertCounts)\r\n    .filter(([_, count]) => count > 10)\r\n    .map(([name]) => name);\r\n  \r\n  // Find stale alerts (never fired in period)\r\n  const allDefinedAlerts = await getAllAlertRules();\r\n  const firedAlerts = new Set(alerts.map(a => a.alertname));\r\n  const staleAlerts = allDefinedAlerts.filter(a => !firedAlerts.has(a.name));\r\n  \r\n  return {\r\n    totalAlerts: alerts.length,\r\n    uniqueAlerts: new Set(alerts.map(a => a.alertname)).size,\r\n    meanTimeToAcknowledge: calculateMTTA(alerts),\r\n    meanTimeToResolve: calculateMTTR(alerts),\r\n    falsePositiveRate: calculateFalsePositiveRate(alerts),\r\n    alertsPerOnCall: alerts.length / getOnCallRotationCount(period),\r\n    noisyAlerts,\r\n    staleAlerts: staleAlerts.map(a => a.name),\r\n  };\r\n}\r\n\r\n// Monthly alert review checklist\r\nconst alertReviewChecklist = [\r\n  '[ ] Review noisy alerts - tune thresholds or delete',\r\n  '[ ] Review stale alerts - still needed?',\r\n  '[ ] Check false positive rate - should be < 5%',\r\n  '[ ] Verify all runbooks are up to date',\r\n  '[ ] Check escalation paths are correct',\r\n  '[ ] Review on-call feedback from past month',\r\n  '[ ] Update SLO thresholds if needed',\r\n];\r\n```\r\n\r\n## Pitfalls\r\n\r\n| Pitfall | Impact | How to Avoid |\r\n|---------|--------|--------------|\r\n| Alerting on causes not symptoms | Miss real issues, false positives | Alert on user-facing metrics |\r\n| No runbooks | Slow incident response | Every alert needs runbook |\r\n| Too many alerts | Alert fatigue, ignored pages | Strict alert review process |\r\n| Same severity for everything | Can't prioritize | Clear severity definitions |\r\n| Not testing alerts | Alerts don't fire when needed | Regular alert testing |\r\n| No auto-resolve | Manual work to close alerts | Configure auto-resolution |\r\n| Alerting on metrics, not SLOs | Chasing causes | Use SLO-based alerting |\r\n| No grouping | Alert storms | Configure proper grouping |\r\n\r\n## Checklist\r\n\r\n- [ ] Alerts on symptoms, not causes\r\n- [ ] SLO-based alerting implemented\r\n- [ ] Clear severity levels defined\r\n- [ ] Every alert has runbook URL\r\n- [ ] Alerts include dashboard links\r\n- [ ] Routing to correct teams configured\r\n- [ ] Escalation paths defined\r\n- [ ] Business-hours routing for warnings\r\n- [ ] Alert grouping configured\r\n- [ ] Inhibition rules for related alerts\r\n- [ ] False positive rate tracked\r\n- [ ] Monthly alert review scheduled\r\n- [ ] On-call rotation documented\r\n- [ ] Alert testing in CI/CD\r\n\r\n## References\r\n\r\n- [Google SRE Book - Monitoring](https://sre.google/sre-book/monitoring-distributed-systems/)\r\n- [Google SRE Workbook - Alerting on SLOs](https://sre.google/workbook/alerting-on-slos/)\r\n- [Prometheus Alerting Best Practices](https://prometheus.io/docs/practices/alerting/)\r\n- [Alertmanager Configuration](https://prometheus.io/docs/alerting/latest/configuration/)\r\n- [PagerDuty Incident Response](https://response.pagerduty.com/)\r\n"
  },
  {
    "id": "obs-correlation-id",
    "title": "Correlation ID & Distributed Tracing",
    "tags": [
      "observability",
      "tracing",
      "debugging",
      "distributed-systems",
      "opentelemetry"
    ],
    "level": "beginner",
    "stacks": [
      "all"
    ],
    "scope": "observability",
    "maturity": "stable",
    "works_with": [
      "all"
    ],
    "path": "D:/Code/backend-engineering-kit/.shared/production-backend-kit/patterns/obs.correlation-id.md",
    "type": "pattern",
    "sections": [],
    "checklist": [],
    "sources": [],
    "rawContent": "\r\n# Correlation ID & Distributed Tracing\r\n\r\n## Problem\r\n\r\nIn distributed systems, a single user request may span multiple services, databases, and queues. Without a common identifier linking all these operations, debugging becomes nearly impossible. \"Why did this request fail?\" becomes an hours-long investigation.\r\n\r\n## When to use\r\n\r\n- Any distributed system\r\n- Microservices architecture\r\n- Request tracing across services\r\n- Log correlation\r\n- Debugging production issues\r\n- Performance analysis\r\n\r\n## Solution\r\n\r\n### 1. W3C Trace Context Standard\r\n\r\nUse the W3C standard for interoperability:\r\n\r\n```http\r\n# Request headers\r\ntraceparent: 00-0af7651916cd43dd8448eb211c80319c-b7ad6b7169203331-01\r\ntracestate: vendor1=value1,vendor2=value2\r\n\r\n# Format: version-traceId-spanId-flags\r\n# 00 = version\r\n# 0af7651916cd43dd8448eb211c80319c = trace ID (32 hex chars)\r\n# b7ad6b7169203331 = span ID (16 hex chars)  \r\n# 01 = flags (01 = sampled)\r\n```\r\n\r\n### 2. Implementation with OpenTelemetry\r\n\r\n```typescript\r\nimport { trace, context, propagation } from '@opentelemetry/api';\r\nimport { NodeTracerProvider } from '@opentelemetry/sdk-trace-node';\r\nimport { W3CTraceContextPropagator } from '@opentelemetry/core';\r\n\r\n// Setup\r\nconst provider = new NodeTracerProvider();\r\nprovider.register({\r\n  propagator: new W3CTraceContextPropagator(),\r\n});\r\n\r\nconst tracer = trace.getTracer('order-service');\r\n\r\n// Create span for operation\r\nasync function createOrder(orderData: OrderInput): Promise<Order> {\r\n  return tracer.startActiveSpan('createOrder', async (span) => {\r\n    try {\r\n      span.setAttribute('order.items_count', orderData.items.length);\r\n      \r\n      // Child span for database\r\n      const order = await tracer.startActiveSpan('db.insert', async (dbSpan) => {\r\n        const result = await db.orders.create(orderData);\r\n        dbSpan.setAttribute('db.table', 'orders');\r\n        dbSpan.end();\r\n        return result;\r\n      });\r\n      \r\n      span.setAttribute('order.id', order.id);\r\n      return order;\r\n    } catch (error) {\r\n      span.recordException(error);\r\n      span.setStatus({ code: SpanStatusCode.ERROR });\r\n      throw error;\r\n    } finally {\r\n      span.end();\r\n    }\r\n  });\r\n}\r\n```\r\n\r\n### 3. Simple Correlation ID (Without Full Tracing)\r\n\r\n```typescript\r\nimport { v4 as uuidv4 } from 'uuid';\r\nimport { AsyncLocalStorage } from 'async_hooks';\r\n\r\ninterface RequestContext {\r\n  correlationId: string;\r\n  userId?: string;\r\n  tenantId?: string;\r\n}\r\n\r\nconst asyncLocalStorage = new AsyncLocalStorage<RequestContext>();\r\n\r\n// Middleware\r\nexport function correlationMiddleware(req: Request, res: Response, next: NextFunction) {\r\n  // Extract or generate correlation ID\r\n  const correlationId = \r\n    req.headers['x-correlation-id'] as string ||\r\n    req.headers['x-request-id'] as string ||\r\n    `req_${uuidv4()}`;\r\n  \r\n  const context: RequestContext = {\r\n    correlationId,\r\n    userId: req.user?.id,\r\n    tenantId: req.tenant?.id,\r\n  };\r\n  \r\n  // Set response header\r\n  res.setHeader('X-Correlation-ID', correlationId);\r\n  \r\n  // Run request in context\r\n  asyncLocalStorage.run(context, () => next());\r\n}\r\n\r\n// Get context anywhere\r\nexport function getCorrelationId(): string | undefined {\r\n  return asyncLocalStorage.getStore()?.correlationId;\r\n}\r\n\r\nexport function getRequestContext(): RequestContext | undefined {\r\n  return asyncLocalStorage.getStore();\r\n}\r\n```\r\n\r\n### 4. Propagation to Async Jobs\r\n\r\n```typescript\r\nimport { Queue, Worker } from 'bullmq';\r\n\r\ninterface JobData {\r\n  // Always include context\r\n  _context: {\r\n    correlationId: string;\r\n    userId?: string;\r\n    tenantId?: string;\r\n  };\r\n  // Job-specific data\r\n  payload: any;\r\n}\r\n\r\n// When creating job\r\nconst queue = new Queue('orders');\r\n\r\nasync function queueOrderProcessing(orderId: string) {\r\n  const context = getRequestContext();\r\n  \r\n  await queue.add('process-order', {\r\n    _context: {\r\n      correlationId: context?.correlationId || `job_${uuidv4()}`,\r\n      userId: context?.userId,\r\n      tenantId: context?.tenantId,\r\n    },\r\n    payload: { orderId },\r\n  });\r\n}\r\n\r\n// Worker restores context\r\nconst worker = new Worker('orders', async (job) => {\r\n  const { _context, payload } = job.data as JobData;\r\n  \r\n  // Restore context for this job\r\n  return asyncLocalStorage.run(_context, async () => {\r\n    logger.info({ orderId: payload.orderId }, 'Processing order');\r\n    // All logs will include correlationId\r\n    await processOrder(payload.orderId);\r\n  });\r\n});\r\n```\r\n\r\n### 5. HTTP Client Propagation\r\n\r\n```typescript\r\nimport axios from 'axios';\r\n\r\n// Create axios instance that propagates context\r\nconst httpClient = axios.create();\r\n\r\nhttpClient.interceptors.request.use((config) => {\r\n  const context = getRequestContext();\r\n  \r\n  if (context?.correlationId) {\r\n    config.headers['X-Correlation-ID'] = context.correlationId;\r\n  }\r\n  if (context?.tenantId) {\r\n    config.headers['X-Tenant-ID'] = context.tenantId;\r\n  }\r\n  \r\n  return config;\r\n});\r\n\r\n// Usage - context automatically propagated\r\nconst response = await httpClient.get('http://payment-service/api/charge');\r\n```\r\n\r\n## Pitfalls\r\n\r\n| Pitfall | How to Avoid |\r\n|---------|--------------|\r\n| Not propagating to async jobs | Explicitly pass correlation ID in job payload |\r\n| Generating new ID at each service | Only generate at edge, propagate everywhere |\r\n| Not logging correlation ID | Add to logging context/MDC |\r\n| Missing from error responses | Include in all responses for debugging |\r\n| Inconsistent header names | Standardize on one name across org |\r\n\r\n## Checklist\r\n\r\n- [ ] Correlation ID generated at entry point\r\n- [ ] ID propagated in HTTP headers\r\n- [ ] ID attached to all log entries\r\n- [ ] ID included in error responses\r\n- [ ] ID passed in async message payloads\r\n- [ ] Thread-local/context stores current ID\r\n- [ ] Downstream services extract and use ID\r\n- [ ] Header name standardized across services\r\n- [ ] ID format is URL-safe\r\n- [ ] Integration with distributed tracing\r\n\r\n## Snippets (Generic)\r\n\r\n```\r\nHTTP Header:\r\nX-Correlation-ID: corr_a1b2c3d4-e5f6-7890-abcd-ef1234567890\r\n\r\nRequest Flow:\r\nClient → API Gateway → Service A → Service B → Database\r\n         │                │            │\r\n         └── corr_abc ────┴── corr_abc─┴── corr_abc\r\n\r\nMiddleware Implementation:\r\ndef correlation_middleware(request, next):\r\n  # Extract or generate\r\n  correlation_id = request.headers.get('X-Correlation-ID')\r\n  if not correlation_id:\r\n    correlation_id = generate_uuid()\r\n  \r\n  # Store in context\r\n  context.set('correlation_id', correlation_id)\r\n  \r\n  # Add to logging context\r\n  logging.set_context({'correlation_id': correlation_id})\r\n  \r\n  # Process request\r\n  response = next(request)\r\n  \r\n  # Include in response\r\n  response.headers['X-Correlation-ID'] = correlation_id\r\n  return response\r\n\r\nLog Entry:\r\n{\r\n  \"timestamp\": \"2026-01-14T12:00:00Z\",\r\n  \"level\": \"INFO\",\r\n  \"message\": \"Order created\",\r\n  \"correlation_id\": \"corr_a1b2c3d4\",\r\n  \"service\": \"order-service\",\r\n  \"user_id\": \"user_123\"\r\n}\r\n\r\nAsync Job Payload:\r\n{\r\n  \"job_type\": \"send_email\",\r\n  \"correlation_id\": \"corr_a1b2c3d4\",  # Propagated!\r\n  \"payload\": { ... }\r\n}\r\n```\r\n\r\n## Sources\r\n\r\n- OpenTelemetry Trace Context: https://www.w3.org/TR/trace-context/\r\n- AWS X-Ray Tracing: https://docs.aws.amazon.com/xray/latest/devguide/xray-concepts.html\r\n- Microservices Logging Best Practices: https://www.datadoghq.com/blog/microservices-logging-best-practices/\r\n- Spring Cloud Sleuth: https://spring.io/projects/spring-cloud-sleuth\r\n"
  },
  {
    "id": "obs-distributed-tracing",
    "title": "Distributed Tracing with OpenTelemetry",
    "tags": [
      "observability",
      "tracing",
      "opentelemetry",
      "debugging",
      "microservices"
    ],
    "level": "intermediate",
    "stacks": [
      "all"
    ],
    "scope": "observability",
    "maturity": "stable",
    "works_with": [
      "all"
    ],
    "path": "D:/Code/backend-engineering-kit/.shared/production-backend-kit/patterns/obs.distributed-tracing.md",
    "type": "pattern",
    "sections": [],
    "checklist": [],
    "sources": [],
    "rawContent": "\r\n# Distributed Tracing with OpenTelemetry\r\n\r\n## Problem\r\n\r\nIn microservices architectures:\r\n- Request flows through multiple services - hard to debug\r\n- Latency issues are difficult to pinpoint\r\n- Errors propagate without clear origin\r\n- Performance bottlenecks hidden across service boundaries\r\n- \"It worked on my machine\" but fails in production\r\n\r\nLogs alone can't show the complete picture of a distributed request.\r\n\r\n## When to use\r\n\r\n- Microservices or service-oriented architectures\r\n- Any system with >2 networked services\r\n- Debugging latency issues\r\n- Understanding request flows\r\n- Performance optimization\r\n- Root cause analysis for errors\r\n- SLO compliance tracking\r\n\r\n## Solution\r\n\r\n### 1. Core Concepts\r\n\r\n```\r\n┌─────────────────────────────────────────────────────────────────────────────┐\r\n│                           DISTRIBUTED TRACE                                 │\r\n├─────────────────────────────────────────────────────────────────────────────┤\r\n│                                                                             │\r\n│  Trace ID: abc123 (unique identifier for entire request flow)               │\r\n│                                                                             │\r\n│  ┌─────────────────────────────────────────────────────────────────────┐   │\r\n│  │ Span: API Gateway (parent)                                          │   │\r\n│  │ span_id: span_001, duration: 250ms                                  │   │\r\n│  │ ┌─────────────────────────────────────────────────────────────────┐ │   │\r\n│  │ │ Span: User Service (child)                                      │ │   │\r\n│  │ │ span_id: span_002, parent: span_001, duration: 50ms             │ │   │\r\n│  │ └─────────────────────────────────────────────────────────────────┘ │   │\r\n│  │ ┌─────────────────────────────────────────────────────────────────┐ │   │\r\n│  │ │ Span: Order Service (child)                                     │ │   │\r\n│  │ │ span_id: span_003, parent: span_001, duration: 180ms            │ │   │\r\n│  │ │ ┌─────────────────────────────────────────────────────────────┐ │ │   │\r\n│  │ │ │ Span: Database Query                                        │ │ │   │\r\n│  │ │ │ span_id: span_004, parent: span_003, duration: 45ms         │ │ │   │\r\n│  │ │ └─────────────────────────────────────────────────────────────┘ │ │   │\r\n│  │ │ ┌─────────────────────────────────────────────────────────────┐ │ │   │\r\n│  │ │ │ Span: Payment Service                                       │ │ │   │\r\n│  │ │ │ span_id: span_005, parent: span_003, duration: 120ms        │ │ │   │\r\n│  │ │ └─────────────────────────────────────────────────────────────┘ │ │   │\r\n│  │ └─────────────────────────────────────────────────────────────────┘ │   │\r\n│  └─────────────────────────────────────────────────────────────────────┘   │\r\n│                                                                             │\r\n└─────────────────────────────────────────────────────────────────────────────┘\r\n\r\nKey Terms:\r\n- Trace: End-to-end journey of a request\r\n- Span: Single operation within a trace\r\n- Context: Propagated metadata (trace_id, span_id, flags)\r\n```\r\n\r\n### 2. OpenTelemetry Setup (Node.js)\r\n\r\n```typescript\r\n// tracing.ts - Initialize BEFORE other imports!\r\nimport { NodeSDK } from '@opentelemetry/sdk-node';\r\nimport { getNodeAutoInstrumentations } from '@opentelemetry/auto-instrumentations-node';\r\nimport { OTLPTraceExporter } from '@opentelemetry/exporter-trace-otlp-http';\r\nimport { Resource } from '@opentelemetry/resources';\r\nimport { SemanticResourceAttributes } from '@opentelemetry/semantic-conventions';\r\nimport { BatchSpanProcessor } from '@opentelemetry/sdk-trace-base';\r\n\r\nconst exporter = new OTLPTraceExporter({\r\n  url: process.env.OTEL_EXPORTER_OTLP_ENDPOINT || 'http://localhost:4318/v1/traces',\r\n});\r\n\r\nconst sdk = new NodeSDK({\r\n  resource: new Resource({\r\n    [SemanticResourceAttributes.SERVICE_NAME]: process.env.SERVICE_NAME || 'my-service',\r\n    [SemanticResourceAttributes.SERVICE_VERSION]: process.env.SERVICE_VERSION || '1.0.0',\r\n    [SemanticResourceAttributes.DEPLOYMENT_ENVIRONMENT]: process.env.NODE_ENV || 'development',\r\n  }),\r\n  spanProcessor: new BatchSpanProcessor(exporter, {\r\n    maxQueueSize: 2048,\r\n    maxExportBatchSize: 512,\r\n    scheduledDelayMillis: 5000,\r\n  }),\r\n  instrumentations: [\r\n    getNodeAutoInstrumentations({\r\n      // Customize which instrumentations to enable\r\n      '@opentelemetry/instrumentation-fs': { enabled: false }, // Disable noisy fs\r\n      '@opentelemetry/instrumentation-http': {\r\n        ignoreIncomingPaths: ['/health', '/ready', '/metrics'],\r\n      },\r\n    }),\r\n  ],\r\n});\r\n\r\nsdk.start();\r\n\r\nprocess.on('SIGTERM', () => {\r\n  sdk.shutdown()\r\n    .then(() => console.log('Tracing shut down'))\r\n    .catch((err) => console.error('Error shutting down tracing', err))\r\n    .finally(() => process.exit(0));\r\n});\r\n\r\nexport { sdk };\r\n```\r\n\r\n```typescript\r\n// index.ts - Import tracing FIRST\r\nimport './tracing';\r\nimport express from 'express';\r\nimport { trace, SpanStatusCode, context } from '@opentelemetry/api';\r\n\r\nconst tracer = trace.getTracer('my-service');\r\n\r\nconst app = express();\r\n\r\n// Manual span creation for custom operations\r\napp.post('/orders', async (req, res) => {\r\n  // Auto-instrumentation creates HTTP span automatically\r\n  \r\n  // Add custom span for business logic\r\n  const span = tracer.startSpan('process-order', {\r\n    attributes: {\r\n      'order.customer_id': req.body.customerId,\r\n      'order.item_count': req.body.items.length,\r\n    },\r\n  });\r\n  \r\n  try {\r\n    // Validate order\r\n    await tracer.startActiveSpan('validate-order', async (validateSpan) => {\r\n      await validateOrder(req.body);\r\n      validateSpan.end();\r\n    });\r\n    \r\n    // Process payment\r\n    await tracer.startActiveSpan('process-payment', async (paymentSpan) => {\r\n      paymentSpan.setAttribute('payment.amount', req.body.total);\r\n      const result = await paymentService.charge(req.body);\r\n      paymentSpan.setAttribute('payment.transaction_id', result.transactionId);\r\n      paymentSpan.end();\r\n    });\r\n    \r\n    // Create order record\r\n    const order = await createOrder(req.body);\r\n    \r\n    span.setAttribute('order.id', order.id);\r\n    span.setStatus({ code: SpanStatusCode.OK });\r\n    \r\n    res.json(order);\r\n  } catch (error) {\r\n    span.recordException(error);\r\n    span.setStatus({ code: SpanStatusCode.ERROR, message: error.message });\r\n    res.status(500).json({ error: error.message });\r\n  } finally {\r\n    span.end();\r\n  }\r\n});\r\n```\r\n\r\n### 3. Context Propagation\r\n\r\n```typescript\r\n// W3C Trace Context headers (standard)\r\n// traceparent: 00-{trace_id}-{span_id}-{flags}\r\n// tracestate: vendor-specific data\r\n\r\n// Propagating context to downstream services\r\nimport { propagation, context } from '@opentelemetry/api';\r\n\r\nasync function callDownstreamService(endpoint: string, data: any) {\r\n  const headers: Record<string, string> = {};\r\n  \r\n  // Inject trace context into headers\r\n  propagation.inject(context.active(), headers);\r\n  \r\n  // headers now contains:\r\n  // traceparent: 00-abc123-def456-01\r\n  // tracestate: ...\r\n  \r\n  const response = await fetch(endpoint, {\r\n    method: 'POST',\r\n    headers: {\r\n      'Content-Type': 'application/json',\r\n      ...headers, // Include trace context\r\n    },\r\n    body: JSON.stringify(data),\r\n  });\r\n  \r\n  return response.json();\r\n}\r\n\r\n// For message queues - include context in message\r\nasync function publishMessage(queue: string, message: any) {\r\n  const carrier: Record<string, string> = {};\r\n  propagation.inject(context.active(), carrier);\r\n  \r\n  await messageQueue.publish(queue, {\r\n    ...message,\r\n    _traceContext: carrier, // Embed trace context\r\n  });\r\n}\r\n\r\n// Consumer extracts context\r\nasync function consumeMessage(message: any) {\r\n  const extractedContext = propagation.extract(context.active(), message._traceContext);\r\n  \r\n  return context.with(extractedContext, async () => {\r\n    // All spans created here will be part of original trace\r\n    await processMessage(message);\r\n  });\r\n}\r\n```\r\n\r\n### 4. Python Setup (FastAPI)\r\n\r\n```python\r\n# tracing.py\r\nfrom opentelemetry import trace\r\nfrom opentelemetry.sdk.trace import TracerProvider\r\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\r\nfrom opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter\r\nfrom opentelemetry.sdk.resources import Resource\r\nfrom opentelemetry.instrumentation.fastapi import FastAPIInstrumentor\r\nfrom opentelemetry.instrumentation.sqlalchemy import SQLAlchemyInstrumentor\r\nfrom opentelemetry.instrumentation.httpx import HTTPXClientInstrumentor\r\nimport os\r\n\r\ndef setup_tracing(app):\r\n    resource = Resource.create({\r\n        \"service.name\": os.getenv(\"SERVICE_NAME\", \"my-service\"),\r\n        \"service.version\": os.getenv(\"SERVICE_VERSION\", \"1.0.0\"),\r\n        \"deployment.environment\": os.getenv(\"ENVIRONMENT\", \"development\"),\r\n    })\r\n    \r\n    provider = TracerProvider(resource=resource)\r\n    \r\n    exporter = OTLPSpanExporter(\r\n        endpoint=os.getenv(\"OTEL_EXPORTER_OTLP_ENDPOINT\", \"localhost:4317\"),\r\n    )\r\n    \r\n    provider.add_span_processor(BatchSpanProcessor(exporter))\r\n    trace.set_tracer_provider(provider)\r\n    \r\n    # Auto-instrument\r\n    FastAPIInstrumentor.instrument_app(app)\r\n    SQLAlchemyInstrumentor().instrument()\r\n    HTTPXClientInstrumentor().instrument()\r\n\r\n# main.py\r\nfrom fastapi import FastAPI\r\nfrom tracing import setup_tracing\r\nfrom opentelemetry import trace\r\n\r\napp = FastAPI()\r\nsetup_tracing(app)\r\n\r\ntracer = trace.get_tracer(__name__)\r\n\r\n@app.post(\"/orders\")\r\nasync def create_order(order: OrderCreate):\r\n    with tracer.start_as_current_span(\"process-order\") as span:\r\n        span.set_attribute(\"order.customer_id\", order.customer_id)\r\n        \r\n        try:\r\n            # Validate\r\n            with tracer.start_as_current_span(\"validate-order\"):\r\n                await validate_order(order)\r\n            \r\n            # Process payment\r\n            with tracer.start_as_current_span(\"process-payment\") as payment_span:\r\n                result = await payment_service.charge(order)\r\n                payment_span.set_attribute(\"payment.transaction_id\", result.transaction_id)\r\n            \r\n            # Create record\r\n            created = await order_service.create(order)\r\n            span.set_attribute(\"order.id\", str(created.id))\r\n            \r\n            return created\r\n            \r\n        except Exception as e:\r\n            span.record_exception(e)\r\n            span.set_status(trace.StatusCode.ERROR, str(e))\r\n            raise\r\n```\r\n\r\n### 5. Span Attributes & Events\r\n\r\n```typescript\r\n// Semantic conventions - use standard attribute names\r\nimport { SemanticAttributes } from '@opentelemetry/semantic-conventions';\r\n\r\nspan.setAttributes({\r\n  // HTTP\r\n  [SemanticAttributes.HTTP_METHOD]: 'POST',\r\n  [SemanticAttributes.HTTP_URL]: '/api/orders',\r\n  [SemanticAttributes.HTTP_STATUS_CODE]: 201,\r\n  [SemanticAttributes.HTTP_USER_AGENT]: req.headers['user-agent'],\r\n  \r\n  // Database\r\n  [SemanticAttributes.DB_SYSTEM]: 'postgresql',\r\n  [SemanticAttributes.DB_NAME]: 'orders',\r\n  [SemanticAttributes.DB_OPERATION]: 'INSERT',\r\n  [SemanticAttributes.DB_STATEMENT]: 'INSERT INTO orders...', // Be careful with PII!\r\n  \r\n  // Messaging\r\n  [SemanticAttributes.MESSAGING_SYSTEM]: 'rabbitmq',\r\n  [SemanticAttributes.MESSAGING_DESTINATION]: 'orders.created',\r\n  [SemanticAttributes.MESSAGING_MESSAGE_ID]: message.id,\r\n  \r\n  // Custom business attributes\r\n  'order.id': orderId,\r\n  'order.total': total,\r\n  'customer.tier': 'premium',\r\n});\r\n\r\n// Events - point-in-time occurrences within a span\r\nspan.addEvent('order.validated', {\r\n  'validation.checks_passed': 5,\r\n  'validation.duration_ms': 12,\r\n});\r\n\r\nspan.addEvent('payment.initiated', {\r\n  'payment.provider': 'stripe',\r\n  'payment.amount': 9999,\r\n});\r\n\r\nspan.addEvent('inventory.reserved', {\r\n  'items.count': 3,\r\n  'warehouse': 'US-WEST-1',\r\n});\r\n```\r\n\r\n### 6. Sampling Strategies\r\n\r\n```typescript\r\nimport { \r\n  ParentBasedSampler, \r\n  TraceIdRatioBasedSampler,\r\n  AlwaysOnSampler,\r\n  AlwaysOffSampler,\r\n} from '@opentelemetry/sdk-trace-base';\r\n\r\n// Sample 10% of traces in production\r\nconst sampler = new ParentBasedSampler({\r\n  root: new TraceIdRatioBasedSampler(0.1), // 10%\r\n});\r\n\r\n// Custom sampler - always sample errors and slow requests\r\nclass SmartSampler implements Sampler {\r\n  shouldSample(context, traceId, spanName, spanKind, attributes) {\r\n    // Always sample errors\r\n    if (attributes['error'] === true) {\r\n      return { decision: SamplingDecision.RECORD_AND_SAMPLED };\r\n    }\r\n    \r\n    // Always sample specific endpoints\r\n    const importantEndpoints = ['/api/payments', '/api/orders'];\r\n    if (importantEndpoints.some(e => attributes['http.url']?.includes(e))) {\r\n      return { decision: SamplingDecision.RECORD_AND_SAMPLED };\r\n    }\r\n    \r\n    // Sample 5% of everything else\r\n    return Math.random() < 0.05 \r\n      ? { decision: SamplingDecision.RECORD_AND_SAMPLED }\r\n      : { decision: SamplingDecision.NOT_RECORD };\r\n  }\r\n}\r\n\r\n// Tail-based sampling (in collector)\r\n// Sample after seeing full trace - useful for sampling slow traces\r\n// Configure in OpenTelemetry Collector:\r\n/*\r\nprocessors:\r\n  tail_sampling:\r\n    decision_wait: 10s\r\n    policies:\r\n      - name: latency-policy\r\n        type: latency\r\n        latency:\r\n          threshold_ms: 1000  # Sample traces > 1s\r\n      - name: error-policy\r\n        type: status_code\r\n        status_code:\r\n          status_codes: [ERROR]\r\n      - name: probabilistic-policy\r\n        type: probabilistic\r\n        probabilistic:\r\n          sampling_percentage: 10\r\n*/\r\n```\r\n\r\n### 7. Connecting Traces to Logs\r\n\r\n```typescript\r\nimport pino from 'pino';\r\nimport { trace, context } from '@opentelemetry/api';\r\n\r\nconst logger = pino({\r\n  mixin() {\r\n    const span = trace.getSpan(context.active());\r\n    if (span) {\r\n      const spanContext = span.spanContext();\r\n      return {\r\n        trace_id: spanContext.traceId,\r\n        span_id: spanContext.spanId,\r\n        trace_flags: spanContext.traceFlags,\r\n      };\r\n    }\r\n    return {};\r\n  },\r\n});\r\n\r\n// Now all logs include trace context!\r\nlogger.info({ orderId: '123' }, 'Order created');\r\n// Output: { \"trace_id\": \"abc123\", \"span_id\": \"def456\", \"orderId\": \"123\", \"msg\": \"Order created\" }\r\n\r\n// In your log aggregator, you can now:\r\n// 1. Click on trace_id to see full distributed trace\r\n// 2. Filter logs by trace_id to see all logs for one request\r\n```\r\n\r\n### 8. Jaeger/Grafana Tempo Setup\r\n\r\n```yaml\r\n# docker-compose.yml\r\nversion: '3'\r\nservices:\r\n  # OpenTelemetry Collector\r\n  otel-collector:\r\n    image: otel/opentelemetry-collector-contrib:latest\r\n    command: [\"--config=/etc/otel-collector-config.yaml\"]\r\n    volumes:\r\n      - ./otel-collector-config.yaml:/etc/otel-collector-config.yaml\r\n    ports:\r\n      - \"4317:4317\"   # OTLP gRPC\r\n      - \"4318:4318\"   # OTLP HTTP\r\n      - \"8889:8889\"   # Prometheus metrics\r\n\r\n  # Jaeger for trace visualization\r\n  jaeger:\r\n    image: jaegertracing/all-in-one:latest\r\n    ports:\r\n      - \"16686:16686\" # UI\r\n      - \"14250:14250\" # gRPC\r\n\r\n  # Or Grafana Tempo\r\n  tempo:\r\n    image: grafana/tempo:latest\r\n    command: [\"-config.file=/etc/tempo.yaml\"]\r\n    volumes:\r\n      - ./tempo.yaml:/etc/tempo.yaml\r\n\r\n---\r\n# otel-collector-config.yaml\r\nreceivers:\r\n  otlp:\r\n    protocols:\r\n      grpc:\r\n        endpoint: 0.0.0.0:4317\r\n      http:\r\n        endpoint: 0.0.0.0:4318\r\n\r\nprocessors:\r\n  batch:\r\n    timeout: 5s\r\n    send_batch_size: 1024\r\n\r\nexporters:\r\n  jaeger:\r\n    endpoint: jaeger:14250\r\n    tls:\r\n      insecure: true\r\n  \r\n  logging:\r\n    loglevel: debug\r\n\r\nservice:\r\n  pipelines:\r\n    traces:\r\n      receivers: [otlp]\r\n      processors: [batch]\r\n      exporters: [jaeger, logging]\r\n```\r\n\r\n## Pitfalls\r\n\r\n| Pitfall | Impact | How to Avoid |\r\n|---------|--------|--------------|\r\n| No sampling | Massive data volume, cost | Use head or tail sampling |\r\n| PII in spans | Security/compliance violation | Scrub sensitive data |\r\n| Missing context propagation | Broken traces | Test cross-service flows |\r\n| Too many spans | Noise, performance overhead | Instrument meaningful operations |\r\n| No correlation with logs | Hard to debug | Include trace_id in all logs |\r\n| Not instrumenting queues | Gaps in traces | Propagate context in messages |\r\n| Ignoring async operations | Missing spans | Use context.with() |\r\n\r\n## Checklist\r\n\r\n- [ ] OpenTelemetry SDK initialized early (before imports)\r\n- [ ] Auto-instrumentation enabled for frameworks\r\n- [ ] Context propagation across HTTP calls\r\n- [ ] Context propagation for message queues\r\n- [ ] Custom spans for business operations\r\n- [ ] Semantic attributes used\r\n- [ ] Sampling strategy defined\r\n- [ ] Trace ID in all logs\r\n- [ ] Sensitive data scrubbed\r\n- [ ] Collector deployed and configured\r\n- [ ] Visualization tool (Jaeger/Tempo) accessible\r\n- [ ] Alerts on trace anomalies\r\n\r\n## References\r\n\r\n- [OpenTelemetry Documentation](https://opentelemetry.io/docs/)\r\n- [W3C Trace Context](https://www.w3.org/TR/trace-context/)\r\n- [Google Dapper Paper](https://research.google/pubs/pub36356/)\r\n- [Jaeger Documentation](https://www.jaegertracing.io/docs/)\r\n- [Grafana Tempo](https://grafana.com/docs/tempo/latest/)\r\n"
  },
  {
    "id": "obs-metrics-red-use",
    "title": "RED & USE Metrics",
    "tags": [
      "observability",
      "metrics",
      "monitoring",
      "sre",
      "prometheus"
    ],
    "level": "intermediate",
    "stacks": [
      "all"
    ],
    "scope": "observability",
    "maturity": "stable",
    "works_with": [
      "all"
    ],
    "path": "D:/Code/backend-engineering-kit/.shared/production-backend-kit/patterns/obs.metrics-red-use.md",
    "type": "pattern",
    "sections": [],
    "checklist": [],
    "sources": [],
    "rawContent": "\r\n# RED & USE Metrics\r\n\r\n## Problem\r\n\r\nWithout standardized metrics, teams measure random things and miss critical signals. You need a consistent framework to monitor service health and diagnose issues quickly. The challenge is knowing what to measure.\r\n\r\n## When to use\r\n\r\n- All production services (RED)\r\n- Infrastructure monitoring (USE)\r\n- SLO/SLA tracking\r\n- Capacity planning\r\n- On-call dashboards\r\n- Performance debugging\r\n\r\n## Solution\r\n\r\n### 1. RED Method (Services/Endpoints)\r\n\r\nFor every service/endpoint, track:\r\n\r\n| Metric | What | Why |\r\n|--------|------|-----|\r\n| **R**ate | Requests per second | Traffic volume, scaling needs |\r\n| **E**rrors | Failed requests per second (or %) | Reliability, user impact |\r\n| **D**uration | Latency (p50, p95, p99) | User experience, SLO |\r\n\r\n**Prometheus Implementation:**\r\n```prometheus\r\n# Counter - total requests\r\nhttp_requests_total{method=\"GET\", endpoint=\"/api/users\", status=\"200\"}\r\nhttp_requests_total{method=\"GET\", endpoint=\"/api/users\", status=\"500\"}\r\n\r\n# Histogram - latency distribution\r\nhttp_request_duration_seconds_bucket{method=\"GET\", endpoint=\"/api/users\", le=\"0.1\"}\r\nhttp_request_duration_seconds_bucket{method=\"GET\", endpoint=\"/api/users\", le=\"0.5\"}\r\nhttp_request_duration_seconds_bucket{method=\"GET\", endpoint=\"/api/users\", le=\"1.0\"}\r\nhttp_request_duration_seconds_bucket{method=\"GET\", endpoint=\"/api/users\", le=\"+Inf\"}\r\n\r\n# PromQL Queries\r\n# Request rate\r\nrate(http_requests_total[5m])\r\n\r\n# Error rate\r\nsum(rate(http_requests_total{status=~\"5..\"}[5m])) / sum(rate(http_requests_total[5m]))\r\n\r\n# P99 latency\r\nhistogram_quantile(0.99, rate(http_request_duration_seconds_bucket[5m]))\r\n```\r\n\r\n### 2. USE Method (Resources)\r\n\r\nFor every resource (CPU, memory, disk, network, queues, pools):\r\n\r\n| Metric | What | Example |\r\n|--------|------|--------|\r\n| **U**tilization | % time resource is busy | CPU 75%, Disk 40% |\r\n| **S**aturation | Work waiting (queue length) | 50 pending requests |\r\n| **E**rrors | Error count | Disk errors, OOM events |\r\n\r\n**Key Resources to Monitor:**\r\n\r\n| Resource | Utilization | Saturation | Errors |\r\n|----------|-------------|------------|--------|\r\n| CPU | `cpu_usage_percent` | Run queue length | - |\r\n| Memory | `memory_used_percent` | Swap usage, OOM | OOM kills |\r\n| Disk | `disk_used_percent`, IOPS | I/O wait | Read/write errors |\r\n| Network | Bandwidth % | Socket queue | Packet drops |\r\n| DB Pool | `active_connections / max` | Pending acquisitions | Timeouts |\r\n| Thread Pool | `active_threads / max` | Queue depth | Rejections |\r\n\r\n### 3. Connecting to SLOs\r\n\r\n```yaml\r\n# Example SLOs\r\navailability:\r\n  target: 99.9%\r\n  metric: 1 - (error_rate)\r\n  window: 30 days\r\n\r\nlatency:\r\n  target: 95% of requests < 200ms\r\n  metric: histogram_quantile(0.95, latency) < 0.2\r\n  window: 30 days\r\n```\r\n\r\n## Pitfalls\r\n\r\n| Pitfall | How to Avoid |\r\n|---------|--------------|\r\n| Too many metrics | Focus on RED/USE first |\r\n| Not tracking percentiles | Always track p50, p95, p99 |\r\n| Alerting on averages | Use percentiles for latency alerts |\r\n| Ignoring saturation | Track queue depths, thread pool usage |\r\n| No baseline | Establish normal ranges before alerting |\r\n\r\n## Checklist\r\n\r\n- [ ] Request rate tracked per endpoint\r\n- [ ] Error rate tracked per endpoint\r\n- [ ] Latency percentiles tracked (p50, p95, p99)\r\n- [ ] CPU utilization monitored\r\n- [ ] Memory utilization monitored\r\n- [ ] Disk I/O and space monitored\r\n- [ ] Queue depths tracked (saturation)\r\n- [ ] Connection pool utilization tracked\r\n- [ ] Dashboards created for RED and USE\r\n- [ ] Alerts configured with appropriate thresholds\r\n\r\n## Snippets (Generic)\r\n\r\n```\r\nRED Metrics (per endpoint):\r\nhttp_requests_total{method=\"GET\", path=\"/api/users\", status=\"200\"} 1000\r\nhttp_requests_total{method=\"GET\", path=\"/api/users\", status=\"500\"} 5\r\nhttp_request_duration_seconds{method=\"GET\", path=\"/api/users\", quantile=\"0.50\"} 0.05\r\nhttp_request_duration_seconds{method=\"GET\", path=\"/api/users\", quantile=\"0.95\"} 0.2\r\nhttp_request_duration_seconds{method=\"GET\", path=\"/api/users\", quantile=\"0.99\"} 0.5\r\n\r\nUSE Metrics (per resource):\r\n# CPU\r\ncpu_utilization_percent 75\r\ncpu_saturation (runnable processes waiting) 2\r\n\r\n# Memory\r\nmemory_utilization_percent 60\r\nmemory_saturation (swap usage) 0\r\n\r\n# Disk\r\ndisk_utilization_percent 40\r\ndisk_io_wait_seconds 0.01\r\n\r\n# Connection Pool\r\npool_utilization_percent 80\r\npool_pending_requests 5\r\npool_errors_total 0\r\n\r\nDashboard Layout:\r\n┌─────────────────────────────────────────┐\r\n│ SERVICE RED METRICS                     │\r\n├─────────────┬─────────────┬─────────────┤\r\n│ Request Rate│ Error Rate  │ Latency p99 │\r\n│ 1000 req/s  │ 0.5%        │ 200ms       │\r\n└─────────────┴─────────────┴─────────────┘\r\n┌─────────────────────────────────────────┐\r\n│ RESOURCE USE METRICS                    │\r\n├──────┬──────┬──────┬──────┬─────────────┤\r\n│ CPU  │ Mem  │ Disk │ Pool │ Queue Depth │\r\n│ 75%  │ 60%  │ 40%  │ 80%  │ 50 msgs     │\r\n└──────┴──────┴──────┴──────┴─────────────┘\r\n\r\nAlerting Thresholds (example):\r\n- Error rate > 1% for 5 min → Page\r\n- p99 latency > 500ms for 10 min → Warn\r\n- CPU utilization > 80% for 15 min → Warn\r\n- Queue depth > 1000 for 5 min → Page\r\n```\r\n\r\n## Sources\r\n\r\n- RED Method (Tom Wilkie): https://www.weave.works/blog/the-red-method-key-metrics-for-microservices-architecture/\r\n- USE Method (Brendan Gregg): https://www.brendangregg.com/usemethod.html\r\n- Google SRE Book - Monitoring: https://sre.google/sre-book/monitoring-distributed-systems/\r\n- Prometheus Best Practices: https://prometheus.io/docs/practices/naming/\r\n"
  },
  {
    "id": "obs-structured-logging",
    "title": "Structured Logging",
    "tags": [
      "observability",
      "logging",
      "json",
      "debugging",
      "elk"
    ],
    "level": "beginner",
    "stacks": [
      "all"
    ],
    "scope": "observability",
    "maturity": "stable",
    "works_with": [
      "all"
    ],
    "path": "D:/Code/backend-engineering-kit/.shared/production-backend-kit/patterns/obs.structured-logging.md",
    "type": "pattern",
    "sections": [],
    "checklist": [],
    "sources": [],
    "rawContent": "\r\n# Structured Logging\r\n\r\n## Problem\r\n\r\nUnstructured log messages are hard to parse, search, and analyze. Free-form text logs make automated monitoring and alerting nearly impossible at scale. Finding issues across distributed services becomes a nightmare.\r\n\r\n## When to use\r\n\r\n- All production applications\r\n- Microservices environments\r\n- Any system requiring log analysis\r\n- When using log aggregation (ELK, Datadog, CloudWatch)\r\n- Debugging distributed systems\r\n\r\n## Solution\r\n\r\n### 1. JSON Log Format\r\n\r\n```json\r\n{\r\n  \"timestamp\": \"2026-01-15T10:30:00.123Z\",\r\n  \"level\": \"INFO\",\r\n  \"message\": \"Order created successfully\",\r\n  \"service\": \"order-service\",\r\n  \"version\": \"1.2.3\",\r\n  \"environment\": \"production\",\r\n  \"correlationId\": \"req_abc123\",\r\n  \"traceId\": \"trace_xyz789\",\r\n  \"spanId\": \"span_def456\",\r\n  \"userId\": \"user_123\",\r\n  \"tenantId\": \"tenant_456\",\r\n  \"orderId\": \"order_789\",\r\n  \"duration_ms\": 145,\r\n  \"request\": {\r\n    \"method\": \"POST\",\r\n    \"path\": \"/api/orders\",\r\n    \"userAgent\": \"Mozilla/5.0...\"\r\n  }\r\n}\r\n```\r\n\r\n### 2. Standard Fields (ECS-Inspired)\r\n\r\n| Field | Type | Description |\r\n|-------|------|-------------|\r\n| `timestamp` | string | ISO 8601 with milliseconds |\r\n| `level` | string | DEBUG, INFO, WARN, ERROR |\r\n| `message` | string | Human-readable description |\r\n| `service` | string | Service/application name |\r\n| `version` | string | Application version |\r\n| `environment` | string | prod, staging, dev |\r\n| `correlationId` | string | Request tracing ID |\r\n| `traceId` | string | Distributed trace ID |\r\n| `userId` | string | User identifier (if authenticated) |\r\n| `tenantId` | string | Tenant identifier (if multi-tenant) |\r\n| `duration_ms` | number | Operation duration |\r\n| `error` | object | Error details (type, message, stack) |\r\n\r\n### 3. Implementation (Node.js with Pino)\r\n\r\n```typescript\r\nimport pino from 'pino';\r\n\r\nconst logger = pino({\r\n  level: process.env.LOG_LEVEL || 'info',\r\n  formatters: {\r\n    level: (label) => ({ level: label.toUpperCase() }),\r\n  },\r\n  base: {\r\n    service: 'order-service',\r\n    version: process.env.APP_VERSION,\r\n    environment: process.env.NODE_ENV,\r\n  },\r\n  timestamp: () => `\"timestamp\":\"${new Date().toISOString()}\"`,\r\n  redact: {\r\n    paths: ['password', 'token', 'authorization', 'cookie', '*.password', '*.token'],\r\n    censor: '[REDACTED]',\r\n  },\r\n});\r\n\r\n// Create child logger with request context\r\nfunction createRequestLogger(req: Request) {\r\n  return logger.child({\r\n    correlationId: req.headers['x-correlation-id'],\r\n    traceId: req.headers['x-trace-id'],\r\n    userId: req.user?.id,\r\n    tenantId: req.tenant?.id,\r\n    request: {\r\n      method: req.method,\r\n      path: req.path,\r\n      userAgent: req.headers['user-agent'],\r\n    },\r\n  });\r\n}\r\n\r\n// Usage\r\napp.use((req, res, next) => {\r\n  req.log = createRequestLogger(req);\r\n  next();\r\n});\r\n\r\n// In handlers\r\nreq.log.info({ orderId: order.id }, 'Order created successfully');\r\nreq.log.error({ err, orderId }, 'Failed to create order');\r\n```\r\n\r\n### 4. Log Levels Usage\r\n\r\n| Level | When to Use | Examples |\r\n|-------|-------------|----------|\r\n| **DEBUG** | Detailed dev info, disable in prod | Variable values, loop iterations |\r\n| **INFO** | Normal operations, milestones | Request received, job completed |\r\n| **WARN** | Unexpected but handled | Retry succeeded, fallback used |\r\n| **ERROR** | Failures requiring attention | Unhandled exception, external failure |\r\n\r\n```typescript\r\n// DEBUG - development only\r\nlogger.debug({ payload, headers }, 'Processing webhook');\r\n\r\n// INFO - normal operations\r\nlogger.info({ userId, action: 'LOGIN' }, 'User logged in');\r\n\r\n// WARN - degraded but functional\r\nlogger.warn({ retryCount, service }, 'Retrying failed request');\r\n\r\n// ERROR - needs attention\r\nlogger.error({ err, userId, orderId }, 'Payment processing failed');\r\n```\r\n\r\n### 5. Sensitive Data Redaction\r\n\r\n```typescript\r\nimport pino from 'pino';\r\n\r\nconst sensitiveFields = [\r\n  'password',\r\n  'token',\r\n  'accessToken',\r\n  'refreshToken',\r\n  'authorization',\r\n  'cookie',\r\n  'creditCard',\r\n  'ssn',\r\n  'apiKey',\r\n  '*.password',\r\n  '*.token',\r\n  'headers.authorization',\r\n  'body.password',\r\n];\r\n\r\nconst logger = pino({\r\n  redact: {\r\n    paths: sensitiveFields,\r\n    censor: '[REDACTED]',\r\n  },\r\n});\r\n\r\n// Custom redaction for complex cases\r\nfunction sanitizeForLogging(obj: any): any {\r\n  const clone = JSON.parse(JSON.stringify(obj));\r\n  \r\n  // Mask email\r\n  if (clone.email) {\r\n    clone.email = clone.email.replace(/(.{2}).*@/, '$1***@');\r\n  }\r\n  \r\n  // Mask credit card\r\n  if (clone.creditCard) {\r\n    clone.creditCard = '**** **** **** ' + clone.creditCard.slice(-4);\r\n  }\r\n  \r\n  return clone;\r\n}\r\n```\r\n\r\n## Pitfalls\r\n\r\n| Pitfall | How to Avoid |\r\n|---------|--------------|\r\n| Logging sensitive data | Create allowlist, auto-mask patterns |\r\n| Inconsistent field names | Define org-wide logging schema |\r\n| Giant log entries | Set size limits, truncate long values |\r\n| Not including context | Use logging context/MDC |\r\n| Mixing formats | Enforce JSON-only in production |\r\n\r\n## Checklist\r\n\r\n- [ ] JSON format used for all logs\r\n- [ ] Standard fields defined (timestamp, level, message)\r\n- [ ] Correlation ID in every log entry\r\n- [ ] Service name included\r\n- [ ] Sensitive data excluded/masked\r\n- [ ] Field names standardized across services\r\n- [ ] Log levels used correctly\r\n- [ ] Stack traces included for errors\r\n- [ ] Log aggregation configured\r\n- [ ] Alerting based on log patterns\r\n\r\n## Snippets (Generic)\r\n\r\n```\r\nStructured Log Entry:\r\n{\r\n  \"timestamp\": \"2026-01-14T12:00:00.123Z\",\r\n  \"level\": \"INFO\",\r\n  \"message\": \"User login successful\",\r\n  \"service\": \"auth-service\",\r\n  \"version\": \"1.2.3\",\r\n  \"correlation_id\": \"corr_abc123\",\r\n  \"user_id\": \"user_456\",\r\n  \"duration_ms\": 45,\r\n  \"request\": {\r\n    \"method\": \"POST\",\r\n    \"path\": \"/api/login\",\r\n    \"ip\": \"192.168.1.1\"\r\n  }\r\n}\r\n\r\nError Log Entry:\r\n{\r\n  \"timestamp\": \"2026-01-14T12:00:00.123Z\",\r\n  \"level\": \"ERROR\",\r\n  \"message\": \"Failed to process order\",\r\n  \"service\": \"order-service\",\r\n  \"correlation_id\": \"corr_def456\",\r\n  \"error\": {\r\n    \"type\": \"ValidationError\",\r\n    \"message\": \"Invalid product ID\",\r\n    \"stack\": \"ValidationError: Invalid product ID\\n  at ...\"\r\n  },\r\n  \"order_id\": \"order_789\"\r\n}\r\n\r\nLog Levels:\r\n- DEBUG: Detailed dev info (disable in prod)\r\n- INFO: Normal operations, milestones\r\n- WARN: Unexpected but handled situations\r\n- ERROR: Failures requiring attention\r\n\r\nStandard Fields:\r\n| Field | Type | Description |\r\n|-------|------|-------------|\r\n| timestamp | string | ISO 8601 with milliseconds |\r\n| level | string | DEBUG, INFO, WARN, ERROR |\r\n| message | string | Human-readable description |\r\n| service | string | Service/app name |\r\n| correlation_id | string | Request tracing ID |\r\n| duration_ms | number | Operation duration |\r\n\r\nLogging Setup:\r\n1. Configure JSON formatter\r\n2. Define standard fields\r\n3. Set up logging context/MDC\r\n4. Add sensitive data filters\r\n5. Configure log shipping to aggregator\r\n```\r\n\r\n## Sources\r\n\r\n- Google SRE Book - Practical Alerting: https://sre.google/sre-book/practical-alerting/\r\n- The 12 Factor App - Logs: https://12factor.net/logs\r\n- ELK Stack Documentation: https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html\r\n- Datadog Logging Best Practices: https://docs.datadoghq.com/logs/\r\n"
  },
  {
    "id": "rel-bulkhead-pattern",
    "title": "Bulkhead Pattern",
    "tags": [
      "reliability",
      "bulkhead",
      "isolation",
      "fault-tolerance",
      "resilience"
    ],
    "level": "intermediate",
    "stacks": [
      "all"
    ],
    "scope": "reliability",
    "maturity": "stable",
    "works_with": [
      "all"
    ],
    "path": "D:/Code/backend-engineering-kit/.shared/production-backend-kit/patterns/rel.bulkhead-pattern.md",
    "type": "pattern",
    "sections": [],
    "checklist": [],
    "sources": [],
    "rawContent": "\r\n# Bulkhead Pattern\r\n\r\n## Problem\r\n\r\nWithout isolation, a failure in one part of the system cascades everywhere:\r\n- Slow downstream service exhausts all threads\r\n- Memory leak in one feature crashes entire app\r\n- Runaway query blocks all database connections\r\n- One noisy tenant degrades service for everyone\r\n- Single misbehaving endpoint takes down API\r\n\r\n**Named after ship bulkheads that contain flooding to one compartment.**\r\n\r\n## When to use\r\n\r\n- Multiple downstream dependencies with varying reliability\r\n- Multi-tenant systems requiring isolation\r\n- Critical features that must remain available\r\n- Background jobs competing with real-time requests\r\n- Rate-limited external APIs\r\n- Workloads with different latency requirements\r\n\r\n## Solution\r\n\r\n### 1. Bulkhead Types\r\n\r\n```\r\n┌─────────────────────────────────────────────────────────────────────────────┐\r\n│                        BULKHEAD STRATEGIES                                  │\r\n├─────────────────────────────────────────────────────────────────────────────┤\r\n│                                                                             │\r\n│  ┌─────────────────┐  ┌─────────────────┐  ┌─────────────────┐             │\r\n│  │  THREAD POOL    │  │   SEMAPHORE     │  │  CONNECTION     │             │\r\n│  │   ISOLATION     │  │   ISOLATION     │  │    POOL         │             │\r\n│  ├─────────────────┤  ├─────────────────┤  ├─────────────────┤             │\r\n│  │ Dedicated       │  │ Limit concurrent│  │ Separate pools  │             │\r\n│  │ threads per     │  │ calls with      │  │ per service/    │             │\r\n│  │ operation type  │  │ counting        │  │ tenant          │             │\r\n│  │                 │  │ semaphore       │  │                 │             │\r\n│  │ Pros:           │  │ Pros:           │  │ Pros:           │             │\r\n│  │ - Full isolation│  │ - Lightweight   │  │ - DB isolation  │             │\r\n│  │ - Timeout ctrl  │  │ - Less overhead │  │ - Prevents      │             │\r\n│  │                 │  │ - No thread     │  │   monopolization│             │\r\n│  │ Cons:           │  │   management    │  │                 │             │\r\n│  │ - More resources│  │ Cons:           │  │ Cons:           │             │\r\n│  │ - Complex       │  │ - Shared thread │  │ - More          │             │\r\n│  │                 │  │   pool risks    │  │   connections   │             │\r\n│  └─────────────────┘  └─────────────────┘  └─────────────────┘             │\r\n│                                                                             │\r\n│  ┌─────────────────┐  ┌─────────────────┐  ┌─────────────────┐             │\r\n│  │  PROCESS        │  │   QUEUE         │  │  RESOURCE       │             │\r\n│  │  ISOLATION      │  │   ISOLATION     │  │    QUOTAS       │             │\r\n│  ├─────────────────┤  ├─────────────────┤  ├─────────────────┤             │\r\n│  │ Separate        │  │ Separate queues │  │ Per-tenant/     │             │\r\n│  │ processes per   │  │ per priority/   │  │ per-endpoint    │             │\r\n│  │ workload        │  │ type            │  │ resource limits │             │\r\n│  └─────────────────┘  └─────────────────┘  └─────────────────┘             │\r\n│                                                                             │\r\n└─────────────────────────────────────────────────────────────────────────────┘\r\n```\r\n\r\n### 2. Semaphore Bulkhead (TypeScript)\r\n\r\n```typescript\r\n// Simple semaphore-based bulkhead\r\nclass Bulkhead {\r\n  private currentCalls = 0;\r\n  private waitQueue: Array<{ resolve: () => void; reject: (err: Error) => void }> = [];\r\n  \r\n  constructor(\r\n    private readonly name: string,\r\n    private readonly maxConcurrent: number,\r\n    private readonly maxWait: number = 100, // Max queue size\r\n    private readonly timeout: number = 30000, // Wait timeout\r\n  ) {}\r\n\r\n  async execute<T>(fn: () => Promise<T>): Promise<T> {\r\n    // Try to acquire\r\n    if (this.currentCalls < this.maxConcurrent) {\r\n      return this.runWithPermit(fn);\r\n    }\r\n\r\n    // Queue if room\r\n    if (this.waitQueue.length < this.maxWait) {\r\n      await this.waitForPermit();\r\n      return this.runWithPermit(fn);\r\n    }\r\n\r\n    // Reject immediately - bulkhead full\r\n    metrics.increment('bulkhead.rejected', { name: this.name });\r\n    throw new BulkheadFullError(\r\n      `Bulkhead ${this.name} is full (${this.maxConcurrent} concurrent, ${this.maxWait} queued)`\r\n    );\r\n  }\r\n\r\n  private async runWithPermit<T>(fn: () => Promise<T>): Promise<T> {\r\n    this.currentCalls++;\r\n    metrics.gauge('bulkhead.concurrent', this.currentCalls, { name: this.name });\r\n    \r\n    try {\r\n      return await fn();\r\n    } finally {\r\n      this.currentCalls--;\r\n      this.releaseWaiter();\r\n    }\r\n  }\r\n\r\n  private waitForPermit(): Promise<void> {\r\n    return new Promise((resolve, reject) => {\r\n      const timer = setTimeout(() => {\r\n        const idx = this.waitQueue.findIndex(w => w.resolve === resolve);\r\n        if (idx >= 0) this.waitQueue.splice(idx, 1);\r\n        reject(new BulkheadTimeoutError(`Timeout waiting for bulkhead ${this.name}`));\r\n      }, this.timeout);\r\n\r\n      this.waitQueue.push({\r\n        resolve: () => {\r\n          clearTimeout(timer);\r\n          resolve();\r\n        },\r\n        reject,\r\n      });\r\n      \r\n      metrics.gauge('bulkhead.queued', this.waitQueue.length, { name: this.name });\r\n    });\r\n  }\r\n\r\n  private releaseWaiter() {\r\n    const waiter = this.waitQueue.shift();\r\n    if (waiter) {\r\n      waiter.resolve();\r\n    }\r\n  }\r\n}\r\n\r\n// Usage\r\nconst paymentBulkhead = new Bulkhead('payment-service', 10, 50);\r\nconst inventoryBulkhead = new Bulkhead('inventory-service', 20, 100);\r\n\r\nasync function processOrder(order: Order) {\r\n  // Each downstream call is isolated\r\n  const [payment, inventory] = await Promise.all([\r\n    paymentBulkhead.execute(() => paymentService.charge(order)),\r\n    inventoryBulkhead.execute(() => inventoryService.reserve(order)),\r\n  ]);\r\n  \r\n  // If payment service is slow and exhausts its bulkhead,\r\n  // inventory service calls are unaffected\r\n  return { payment, inventory };\r\n}\r\n```\r\n\r\n### 3. Resilience4j-style Bulkhead (TypeScript)\r\n\r\n```typescript\r\nimport Bottleneck from 'bottleneck';\r\n\r\n// Bottleneck provides robust bulkhead/rate limiting\r\nconst bulkheads = {\r\n  // Payment: max 10 concurrent, max 50 queued\r\n  payment: new Bottleneck({\r\n    maxConcurrent: 10,\r\n    reservoir: 50,      // Max queued\r\n    reservoirRefreshInterval: 1000,\r\n    reservoirRefreshAmount: 50,\r\n  }),\r\n  \r\n  // External API: max 5 concurrent, 100ms between calls\r\n  externalApi: new Bottleneck({\r\n    maxConcurrent: 5,\r\n    minTime: 100,       // Minimum time between calls\r\n  }),\r\n  \r\n  // Heavy computation: max 2 concurrent\r\n  heavyCompute: new Bottleneck({\r\n    maxConcurrent: 2,\r\n  }),\r\n};\r\n\r\n// Wrap calls with bulkhead\r\nconst chargePayment = bulkheads.payment.wrap(async (order: Order) => {\r\n  return paymentService.charge(order);\r\n});\r\n\r\n// With events\r\nbulkheads.payment.on('failed', (error, jobInfo) => {\r\n  logger.error({ error, jobInfo }, 'Payment bulkhead job failed');\r\n});\r\n\r\nbulkheads.payment.on('dropped', (dropped) => {\r\n  metrics.increment('bulkhead.dropped', { service: 'payment' });\r\n  logger.warn({ dropped }, 'Payment request dropped by bulkhead');\r\n});\r\n```\r\n\r\n### 4. Connection Pool Bulkheads\r\n\r\n```typescript\r\n// Separate connection pools per service/tenant\r\nimport { Pool } from 'pg';\r\n\r\nclass DatabaseBulkhead {\r\n  private pools: Map<string, Pool> = new Map();\r\n  \r\n  constructor(\r\n    private baseConfig: PoolConfig,\r\n    private poolSizes: Record<string, number>,\r\n  ) {}\r\n\r\n  getPool(identifier: string): Pool {\r\n    if (!this.pools.has(identifier)) {\r\n      const size = this.poolSizes[identifier] || this.poolSizes.default || 5;\r\n      \r\n      this.pools.set(identifier, new Pool({\r\n        ...this.baseConfig,\r\n        max: size,\r\n        // Separate pool name for metrics\r\n        application_name: `app-${identifier}`,\r\n      }));\r\n    }\r\n    \r\n    return this.pools.get(identifier)!;\r\n  }\r\n}\r\n\r\n// Configuration\r\nconst dbBulkhead = new DatabaseBulkhead(\r\n  { host: 'localhost', database: 'mydb' },\r\n  {\r\n    default: 5,\r\n    'api-critical': 10,     // More connections for critical path\r\n    'api-reports': 3,       // Limited for heavy reports\r\n    'background-jobs': 5,   // Isolated from API\r\n    'tenant-enterprise': 8, // Premium tenant gets more\r\n    'tenant-free': 2,       // Free tier limited\r\n  }\r\n);\r\n\r\n// Usage - requests from different sources use isolated pools\r\nasync function handleApiRequest(req: Request) {\r\n  const pool = dbBulkhead.getPool('api-critical');\r\n  return pool.query('SELECT ...');\r\n}\r\n\r\nasync function generateReport(req: Request) {\r\n  const pool = dbBulkhead.getPool('api-reports');\r\n  return pool.query('SELECT ... (heavy aggregation)');\r\n}\r\n\r\nasync function processBackgroundJob(job: Job) {\r\n  const pool = dbBulkhead.getPool('background-jobs');\r\n  return pool.query('UPDATE ...');\r\n}\r\n\r\n// Tenant-based isolation\r\nasync function handleTenantRequest(req: Request) {\r\n  const tier = req.tenant.plan; // 'enterprise' or 'free'\r\n  const pool = dbBulkhead.getPool(`tenant-${tier}`);\r\n  return pool.query('SELECT ... WHERE tenant_id = $1', [req.tenant.id]);\r\n}\r\n```\r\n\r\n### 5. HTTP Client Bulkheads\r\n\r\n```typescript\r\nimport axios, { AxiosInstance } from 'axios';\r\nimport { Agent } from 'http';\r\n\r\n// Separate HTTP agents per downstream service\r\nfunction createBulkheadedClient(\r\n  name: string,\r\n  baseURL: string,\r\n  maxSockets: number,\r\n): AxiosInstance {\r\n  // Each client has its own connection pool\r\n  const agent = new Agent({\r\n    maxSockets,           // Max concurrent connections\r\n    maxFreeSockets: 10,   // Keep-alive connections\r\n    timeout: 30000,\r\n    keepAlive: true,\r\n  });\r\n\r\n  const client = axios.create({\r\n    baseURL,\r\n    httpAgent: agent,\r\n    timeout: 10000,\r\n  });\r\n\r\n  // Track metrics\r\n  client.interceptors.request.use((config) => {\r\n    metrics.increment('http.request', { service: name });\r\n    return config;\r\n  });\r\n\r\n  client.interceptors.response.use(\r\n    (response) => {\r\n      metrics.increment('http.response', { \r\n        service: name, \r\n        status: response.status,\r\n      });\r\n      return response;\r\n    },\r\n    (error) => {\r\n      metrics.increment('http.error', { service: name });\r\n      throw error;\r\n    }\r\n  );\r\n\r\n  return client;\r\n}\r\n\r\n// Create isolated clients\r\nconst httpClients = {\r\n  payment: createBulkheadedClient('payment', 'https://payment.internal', 10),\r\n  inventory: createBulkheadedClient('inventory', 'https://inventory.internal', 20),\r\n  shipping: createBulkheadedClient('shipping', 'https://shipping.internal', 15),\r\n  // External API with strict limits\r\n  externalApi: createBulkheadedClient('external', 'https://api.external.com', 5),\r\n};\r\n```\r\n\r\n### 6. Worker Thread Bulkheads (CPU-bound)\r\n\r\n```typescript\r\nimport { Worker } from 'worker_threads';\r\nimport { cpus } from 'os';\r\n\r\nclass WorkerPoolBulkhead {\r\n  private workers: Worker[] = [];\r\n  private taskQueue: Array<{\r\n    task: any;\r\n    resolve: (value: any) => void;\r\n    reject: (error: any) => void;\r\n  }> = [];\r\n  private busyWorkers = new Set<Worker>();\r\n\r\n  constructor(\r\n    private workerPath: string,\r\n    private poolSize: number = Math.max(1, cpus().length - 1),\r\n  ) {\r\n    this.initializeWorkers();\r\n  }\r\n\r\n  private initializeWorkers() {\r\n    for (let i = 0; i < this.poolSize; i++) {\r\n      const worker = new Worker(this.workerPath);\r\n      \r\n      worker.on('message', (result) => {\r\n        this.busyWorkers.delete(worker);\r\n        this.processQueue();\r\n        // Result handling done via Promise\r\n      });\r\n      \r\n      worker.on('error', (error) => {\r\n        this.busyWorkers.delete(worker);\r\n        // Replace crashed worker\r\n        this.workers = this.workers.filter(w => w !== worker);\r\n        this.workers.push(new Worker(this.workerPath));\r\n      });\r\n      \r\n      this.workers.push(worker);\r\n    }\r\n  }\r\n\r\n  async execute<T>(task: any): Promise<T> {\r\n    return new Promise((resolve, reject) => {\r\n      const availableWorker = this.workers.find(w => !this.busyWorkers.has(w));\r\n      \r\n      if (availableWorker) {\r\n        this.runOnWorker(availableWorker, task, resolve, reject);\r\n      } else {\r\n        // Queue the task\r\n        this.taskQueue.push({ task, resolve, reject });\r\n        metrics.gauge('worker_bulkhead.queued', this.taskQueue.length);\r\n      }\r\n    });\r\n  }\r\n\r\n  private runOnWorker(\r\n    worker: Worker,\r\n    task: any,\r\n    resolve: (value: any) => void,\r\n    reject: (error: any) => void,\r\n  ) {\r\n    this.busyWorkers.add(worker);\r\n    \r\n    const messageHandler = (result: any) => {\r\n      worker.off('message', messageHandler);\r\n      worker.off('error', errorHandler);\r\n      resolve(result);\r\n    };\r\n    \r\n    const errorHandler = (error: any) => {\r\n      worker.off('message', messageHandler);\r\n      worker.off('error', errorHandler);\r\n      reject(error);\r\n    };\r\n    \r\n    worker.on('message', messageHandler);\r\n    worker.on('error', errorHandler);\r\n    worker.postMessage(task);\r\n  }\r\n\r\n  private processQueue() {\r\n    if (this.taskQueue.length === 0) return;\r\n    \r\n    const availableWorker = this.workers.find(w => !this.busyWorkers.has(w));\r\n    if (!availableWorker) return;\r\n    \r\n    const { task, resolve, reject } = this.taskQueue.shift()!;\r\n    this.runOnWorker(availableWorker, task, resolve, reject);\r\n  }\r\n}\r\n\r\n// Usage - isolate CPU-heavy operations\r\nconst imageProcessingPool = new WorkerPoolBulkhead('./image-worker.js', 2);\r\nconst pdfGenerationPool = new WorkerPoolBulkhead('./pdf-worker.js', 2);\r\n\r\n// These won't block each other or the main thread\r\nawait Promise.all([\r\n  imageProcessingPool.execute({ type: 'resize', image: buffer }),\r\n  pdfGenerationPool.execute({ type: 'generate', data: reportData }),\r\n]);\r\n```\r\n\r\n### 7. Queue-Based Bulkheads\r\n\r\n```typescript\r\nimport Bull from 'bull';\r\n\r\n// Separate queues for different priority levels\r\nconst queues = {\r\n  critical: new Bull('critical-jobs', {\r\n    redis: { host: 'redis' },\r\n    limiter: {\r\n      max: 100,        // Max concurrent\r\n      duration: 1000,  // Per second\r\n    },\r\n  }),\r\n  \r\n  standard: new Bull('standard-jobs', {\r\n    redis: { host: 'redis' },\r\n    limiter: {\r\n      max: 50,\r\n      duration: 1000,\r\n    },\r\n  }),\r\n  \r\n  batch: new Bull('batch-jobs', {\r\n    redis: { host: 'redis' },\r\n    limiter: {\r\n      max: 10,         // Limited - don't overwhelm system\r\n      duration: 1000,\r\n    },\r\n  }),\r\n};\r\n\r\n// Process with isolated concurrency\r\nqueues.critical.process(20, async (job) => {\r\n  // Up to 20 concurrent critical jobs\r\n  return processCriticalJob(job.data);\r\n});\r\n\r\nqueues.standard.process(10, async (job) => {\r\n  // Up to 10 concurrent standard jobs\r\n  return processStandardJob(job.data);\r\n});\r\n\r\nqueues.batch.process(2, async (job) => {\r\n  // Only 2 concurrent batch jobs\r\n  return processBatchJob(job.data);\r\n});\r\n\r\n// Route jobs to appropriate queue\r\nfunction enqueueJob(job: Job) {\r\n  switch (job.priority) {\r\n    case 'critical':\r\n      return queues.critical.add(job.data, { priority: 1 });\r\n    case 'batch':\r\n      return queues.batch.add(job.data);\r\n    default:\r\n      return queues.standard.add(job.data);\r\n  }\r\n}\r\n```\r\n\r\n### 8. Multi-Tenant Bulkheads\r\n\r\n```typescript\r\n// Isolate tenants to prevent noisy neighbor\r\nclass TenantBulkhead {\r\n  private tenantLimiters = new Map<string, Bottleneck>();\r\n  \r\n  constructor(\r\n    private tierLimits: Record<string, { concurrent: number; perSecond: number }>,\r\n  ) {}\r\n\r\n  private getLimiter(tenantId: string, tier: string): Bottleneck {\r\n    const key = `${tenantId}:${tier}`;\r\n    \r\n    if (!this.tenantLimiters.has(key)) {\r\n      const limits = this.tierLimits[tier] || this.tierLimits.default;\r\n      \r\n      const limiter = new Bottleneck({\r\n        maxConcurrent: limits.concurrent,\r\n        reservoir: limits.perSecond,\r\n        reservoirRefreshInterval: 1000,\r\n        reservoirRefreshAmount: limits.perSecond,\r\n      });\r\n      \r\n      limiter.on('dropped', () => {\r\n        metrics.increment('tenant.rate_limited', { tenantId, tier });\r\n      });\r\n      \r\n      this.tenantLimiters.set(key, limiter);\r\n    }\r\n    \r\n    return this.tenantLimiters.get(key)!;\r\n  }\r\n\r\n  async execute<T>(\r\n    tenantId: string,\r\n    tier: string,\r\n    fn: () => Promise<T>,\r\n  ): Promise<T> {\r\n    const limiter = this.getLimiter(tenantId, tier);\r\n    return limiter.schedule(fn);\r\n  }\r\n}\r\n\r\nconst tenantBulkhead = new TenantBulkhead({\r\n  enterprise: { concurrent: 50, perSecond: 1000 },\r\n  pro: { concurrent: 20, perSecond: 200 },\r\n  free: { concurrent: 5, perSecond: 50 },\r\n  default: { concurrent: 10, perSecond: 100 },\r\n});\r\n\r\n// Middleware\r\nasync function tenantIsolation(req: Request, res: Response, next: NextFunction) {\r\n  try {\r\n    await tenantBulkhead.execute(\r\n      req.tenant.id,\r\n      req.tenant.plan,\r\n      () => Promise.resolve(), // Just acquire permit\r\n    );\r\n    next();\r\n  } catch (error) {\r\n    if (error.message.includes('dropped')) {\r\n      res.status(429).json({ error: 'Rate limit exceeded for your plan' });\r\n    } else {\r\n      next(error);\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n## Pitfalls\r\n\r\n| Pitfall | Impact | How to Avoid |\r\n|---------|--------|--------------|\r\n| Too small bulkheads | Rejecting valid requests | Size based on load testing |\r\n| Too large bulkheads | No isolation benefit | Start small, increase based on metrics |\r\n| No queue limit | Memory exhaustion | Always set max queue size |\r\n| No timeout on queue | Stuck requests | Set reasonable wait timeout |\r\n| Shared thread pool | Bulkhead bypass | Use true isolation |\r\n| Not monitoring | Can't tune | Track rejection/queue metrics |\r\n| No fallback | Hard failures | Combine with circuit breaker |\r\n\r\n## Checklist\r\n\r\n- [ ] Bulkheads defined for each downstream service\r\n- [ ] Bulkhead type chosen (semaphore/thread pool/connection)\r\n- [ ] Max concurrent limits configured\r\n- [ ] Queue limits and timeouts set\r\n- [ ] Metrics exposed (concurrent, queued, rejected)\r\n- [ ] Fallback behavior defined\r\n- [ ] Combined with circuit breaker\r\n- [ ] Multi-tenant isolation if applicable\r\n- [ ] Load tested to find optimal sizes\r\n- [ ] Alerts on high rejection rates\r\n\r\n## References\r\n\r\n- [Azure Bulkhead Pattern](https://docs.microsoft.com/en-us/azure/architecture/patterns/bulkhead)\r\n- [Resilience4j Bulkhead](https://resilience4j.readme.io/docs/bulkhead)\r\n- [AWS Avoiding Overload](https://aws.amazon.com/builders-library/avoiding-overload-in-distributed-systems/)\r\n- [Bottleneck Library](https://github.com/SGrondin/bottleneck)\r\n"
  },
  {
    "id": "rel-caching-strategies",
    "title": "Caching Strategies & Patterns",
    "tags": [
      "reliability",
      "caching",
      "performance",
      "redis",
      "distributed-systems"
    ],
    "level": "intermediate",
    "stacks": [
      "all"
    ],
    "scope": "reliability",
    "maturity": "stable",
    "works_with": [
      "all"
    ],
    "path": "D:/Code/backend-engineering-kit/.shared/production-backend-kit/patterns/rel.caching-strategies.md",
    "type": "pattern",
    "sections": [],
    "checklist": [],
    "sources": [],
    "rawContent": "\r\n# Caching Strategies & Patterns\r\n\r\n## Problem\r\n\r\nWithout proper caching:\r\n- Database overwhelmed with repeated identical queries\r\n- High latency for frequently accessed data\r\n- Unnecessary computation repeated\r\n- Poor user experience during traffic spikes\r\n- Higher infrastructure costs\r\n\r\nBut caching done wrong causes:\r\n- Stale data served to users\r\n- Cache stampedes crushing backend\r\n- Memory bloat from unbounded caches\r\n- Inconsistency between cache and source of truth\r\n\r\n## When to use\r\n\r\n- Read-heavy workloads (read:write ratio > 10:1)\r\n- Data that changes infrequently\r\n- Expensive computations (aggregations, ML inference)\r\n- External API responses\r\n- Session data and user preferences\r\n- Database query results\r\n- Static assets and configurations\r\n\r\n## Solution\r\n\r\n### 1. Caching Patterns Comparison\r\n\r\n```\r\n┌─────────────────────────────────────────────────────────────────────────────┐\r\n│ Pattern          │ Description              │ Best For                      │\r\n├──────────────────┼──────────────────────────┼───────────────────────────────┤\r\n│ Cache-Aside      │ App manages cache        │ General purpose, most common  │\r\n│ (Lazy Loading)   │ Load on miss             │ Read-heavy, tolerance to stale│\r\n├──────────────────┼──────────────────────────┼───────────────────────────────┤\r\n│ Read-Through     │ Cache loads from DB      │ Simpler app code              │\r\n│                  │ transparently            │ Cache library handles loading │\r\n├──────────────────┼──────────────────────────┼───────────────────────────────┤\r\n│ Write-Through    │ Write to cache + DB      │ Strong consistency needed     │\r\n│                  │ synchronously            │ Can't tolerate stale data     │\r\n├──────────────────┼──────────────────────────┼───────────────────────────────┤\r\n│ Write-Behind     │ Write to cache, async    │ High write throughput         │\r\n│ (Write-Back)     │ persist to DB            │ Can tolerate some data loss   │\r\n├──────────────────┼──────────────────────────┼───────────────────────────────┤\r\n│ Refresh-Ahead    │ Proactively refresh      │ Predictable access patterns   │\r\n│                  │ before expiry            │ Zero-latency cache hits       │\r\n└─────────────────────────────────────────────────────────────────────────────┘\r\n```\r\n\r\n### 2. Cache-Aside Pattern (Most Common)\r\n\r\n```typescript\r\n// The application is responsible for cache management\r\nclass UserService {\r\n  constructor(\r\n    private cache: Redis,\r\n    private db: Database\r\n  ) {}\r\n\r\n  async getUser(id: string): Promise<User | null> {\r\n    const cacheKey = `user:${id}`;\r\n    \r\n    // 1. Try cache first\r\n    const cached = await this.cache.get(cacheKey);\r\n    if (cached) {\r\n      metrics.increment('cache.hit', { entity: 'user' });\r\n      return JSON.parse(cached);\r\n    }\r\n    \r\n    // 2. Cache miss - load from DB\r\n    metrics.increment('cache.miss', { entity: 'user' });\r\n    const user = await this.db.users.findById(id);\r\n    \r\n    if (user) {\r\n      // 3. Populate cache for next time\r\n      await this.cache.setex(\r\n        cacheKey,\r\n        3600, // TTL: 1 hour\r\n        JSON.stringify(user)\r\n      );\r\n    }\r\n    \r\n    return user;\r\n  }\r\n\r\n  async updateUser(id: string, data: UpdateUserDto): Promise<User> {\r\n    // 1. Update database\r\n    const user = await this.db.users.update(id, data);\r\n    \r\n    // 2. Invalidate cache (not update - simpler, safer)\r\n    await this.cache.del(`user:${id}`);\r\n    \r\n    // 3. Also invalidate related caches\r\n    await this.cache.del(`user:${user.email}`);\r\n    await this.cache.del(`team:${user.teamId}:members`);\r\n    \r\n    return user;\r\n  }\r\n}\r\n```\r\n\r\n### 3. Cache Key Design\r\n\r\n```typescript\r\n// Good cache key patterns\r\nconst CACHE_KEYS = {\r\n  // Entity by ID\r\n  user: (id: string) => `user:${id}`,\r\n  \r\n  // Entity by unique field\r\n  userByEmail: (email: string) => `user:email:${email.toLowerCase()}`,\r\n  \r\n  // Collection with pagination\r\n  userList: (page: number, limit: number) => `users:list:p${page}:l${limit}`,\r\n  \r\n  // Filtered collection (hash the filters)\r\n  userSearch: (filters: object) => `users:search:${hashFilters(filters)}`,\r\n  \r\n  // Tenant-scoped\r\n  tenantUser: (tenantId: string, userId: string) => `t:${tenantId}:user:${userId}`,\r\n  \r\n  // Versioned (for cache busting)\r\n  config: (version: string) => `config:v${version}`,\r\n  \r\n  // Time-bucketed (for analytics)\r\n  dailyStats: (date: string) => `stats:daily:${date}`,\r\n};\r\n\r\n// Hash filters for consistent keys\r\nfunction hashFilters(filters: object): string {\r\n  const sorted = Object.keys(filters).sort().reduce((acc, key) => {\r\n    acc[key] = filters[key];\r\n    return acc;\r\n  }, {});\r\n  return crypto.createHash('md5').update(JSON.stringify(sorted)).digest('hex').slice(0, 8);\r\n}\r\n```\r\n\r\n### 4. TTL Strategies\r\n\r\n```typescript\r\nconst TTL = {\r\n  // Static/rarely changing\r\n  CONFIG: 24 * 60 * 60,        // 24 hours\r\n  FEATURE_FLAGS: 5 * 60,       // 5 minutes\r\n  \r\n  // User data\r\n  USER_PROFILE: 60 * 60,       // 1 hour\r\n  USER_SESSION: 30 * 60,       // 30 minutes\r\n  USER_PERMISSIONS: 5 * 60,    // 5 minutes (security-sensitive)\r\n  \r\n  // Dynamic data\r\n  PRODUCT_DETAILS: 15 * 60,    // 15 minutes\r\n  INVENTORY_COUNT: 60,         // 1 minute (frequently changing)\r\n  PRICE: 5 * 60,               // 5 minutes\r\n  \r\n  // Computed/expensive\r\n  ANALYTICS_REPORT: 60 * 60,   // 1 hour\r\n  SEARCH_RESULTS: 5 * 60,      // 5 minutes\r\n  \r\n  // External API\r\n  EXCHANGE_RATES: 60 * 60,     // 1 hour\r\n  WEATHER_DATA: 10 * 60,       // 10 minutes\r\n};\r\n\r\n// Add jitter to prevent thundering herd\r\nfunction ttlWithJitter(baseTtl: number, jitterPercent = 10): number {\r\n  const jitter = baseTtl * (jitterPercent / 100);\r\n  return Math.floor(baseTtl + (Math.random() * jitter * 2) - jitter);\r\n}\r\n```\r\n\r\n### 5. Cache Stampede Prevention\r\n\r\n```typescript\r\n// Problem: 1000 requests hit expired cache simultaneously\r\n// All 1000 go to database = stampede\r\n\r\n// Solution 1: Probabilistic Early Expiration (PER)\r\nasync function getWithPER<T>(\r\n  key: string,\r\n  fetchFn: () => Promise<T>,\r\n  ttl: number,\r\n  beta = 1 // Higher = more aggressive early recompute\r\n): Promise<T> {\r\n  const cached = await cache.get(key);\r\n  \r\n  if (cached) {\r\n    const { value, delta, expiry } = JSON.parse(cached);\r\n    const now = Date.now();\r\n    \r\n    // Probabilistically recompute before expiry\r\n    // xfetch algorithm: expiry - delta * beta * log(random())\r\n    const shouldRecompute = now - (delta * beta * Math.log(Math.random())) >= expiry;\r\n    \r\n    if (!shouldRecompute) {\r\n      return value;\r\n    }\r\n  }\r\n  \r\n  // Recompute\r\n  const start = Date.now();\r\n  const value = await fetchFn();\r\n  const delta = Date.now() - start;\r\n  \r\n  await cache.setex(key, ttl, JSON.stringify({\r\n    value,\r\n    delta,\r\n    expiry: Date.now() + (ttl * 1000),\r\n  }));\r\n  \r\n  return value;\r\n}\r\n\r\n// Solution 2: Distributed Lock\r\nasync function getWithLock<T>(\r\n  key: string,\r\n  fetchFn: () => Promise<T>,\r\n  ttl: number\r\n): Promise<T> {\r\n  const cached = await cache.get(key);\r\n  if (cached) return JSON.parse(cached);\r\n  \r\n  const lockKey = `lock:${key}`;\r\n  const lockAcquired = await cache.set(lockKey, '1', 'EX', 10, 'NX');\r\n  \r\n  if (lockAcquired) {\r\n    try {\r\n      // We got the lock - fetch and cache\r\n      const value = await fetchFn();\r\n      await cache.setex(key, ttl, JSON.stringify(value));\r\n      return value;\r\n    } finally {\r\n      await cache.del(lockKey);\r\n    }\r\n  } else {\r\n    // Someone else is fetching - wait and retry\r\n    await sleep(100);\r\n    return getWithLock(key, fetchFn, ttl);\r\n  }\r\n}\r\n\r\n// Solution 3: Stale-While-Revalidate\r\nasync function getWithSWR<T>(\r\n  key: string,\r\n  fetchFn: () => Promise<T>,\r\n  ttl: number,\r\n  staleTtl: number // How long stale data is acceptable\r\n): Promise<T> {\r\n  const cached = await cache.get(key);\r\n  \r\n  if (cached) {\r\n    const { value, cachedAt } = JSON.parse(cached);\r\n    const age = Date.now() - cachedAt;\r\n    \r\n    if (age < ttl * 1000) {\r\n      // Fresh - return immediately\r\n      return value;\r\n    }\r\n    \r\n    if (age < (ttl + staleTtl) * 1000) {\r\n      // Stale but acceptable - return stale, refresh in background\r\n      refreshInBackground(key, fetchFn, ttl); // Don't await!\r\n      return value;\r\n    }\r\n  }\r\n  \r\n  // Expired or missing - must fetch\r\n  return fetchAndCache(key, fetchFn, ttl);\r\n}\r\n```\r\n\r\n### 6. Cache Invalidation Strategies\r\n\r\n```typescript\r\n// Strategy 1: Event-Driven Invalidation\r\nclass CacheInvalidationService {\r\n  constructor(private cache: Redis, private events: EventEmitter) {\r\n    // Listen to domain events\r\n    events.on('user.updated', this.onUserUpdated.bind(this));\r\n    events.on('user.deleted', this.onUserDeleted.bind(this));\r\n    events.on('order.created', this.onOrderCreated.bind(this));\r\n  }\r\n\r\n  private async onUserUpdated(event: UserUpdatedEvent) {\r\n    const keys = [\r\n      `user:${event.userId}`,\r\n      `user:email:${event.oldEmail}`,\r\n      `user:email:${event.newEmail}`,\r\n    ];\r\n    \r\n    // Invalidate all related keys\r\n    await this.cache.del(...keys);\r\n    \r\n    // Invalidate patterns (use scan for large datasets)\r\n    await this.invalidatePattern(`user:${event.userId}:*`);\r\n  }\r\n\r\n  private async invalidatePattern(pattern: string) {\r\n    let cursor = '0';\r\n    do {\r\n      const [newCursor, keys] = await this.cache.scan(\r\n        cursor, 'MATCH', pattern, 'COUNT', 100\r\n      );\r\n      cursor = newCursor;\r\n      if (keys.length > 0) {\r\n        await this.cache.del(...keys);\r\n      }\r\n    } while (cursor !== '0');\r\n  }\r\n}\r\n\r\n// Strategy 2: Tag-Based Invalidation\r\nclass TaggedCache {\r\n  async set(key: string, value: any, ttl: number, tags: string[]) {\r\n    const multi = this.cache.multi();\r\n    \r\n    // Store value\r\n    multi.setex(key, ttl, JSON.stringify(value));\r\n    \r\n    // Add key to tag sets\r\n    for (const tag of tags) {\r\n      multi.sadd(`tag:${tag}`, key);\r\n      multi.expire(`tag:${tag}`, ttl + 3600); // Tag lives longer than content\r\n    }\r\n    \r\n    await multi.exec();\r\n  }\r\n\r\n  async invalidateTag(tag: string) {\r\n    const keys = await this.cache.smembers(`tag:${tag}`);\r\n    if (keys.length > 0) {\r\n      await this.cache.del(...keys, `tag:${tag}`);\r\n    }\r\n  }\r\n}\r\n\r\n// Usage\r\nawait taggedCache.set('user:123', userData, 3600, ['users', 'team:456']);\r\nawait taggedCache.set('user:124', userData, 3600, ['users', 'team:456']);\r\n\r\n// Invalidate all users in team 456\r\nawait taggedCache.invalidateTag('team:456');\r\n```\r\n\r\n### 7. Multi-Layer Caching\r\n\r\n```typescript\r\n// L1: In-memory (fastest, smallest)\r\n// L2: Redis (fast, shared across instances)\r\n// L3: Database (slow, source of truth)\r\n\r\nclass MultiLayerCache<T> {\r\n  private l1: LRUCache<string, T>;\r\n  private l2: Redis;\r\n  \r\n  constructor(l1MaxSize = 1000) {\r\n    this.l1 = new LRUCache({ max: l1MaxSize, ttl: 60 * 1000 }); // 1 min L1\r\n  }\r\n\r\n  async get(key: string, fetchFn: () => Promise<T>, ttl: number): Promise<T> {\r\n    // L1: Check in-memory\r\n    const l1Value = this.l1.get(key);\r\n    if (l1Value !== undefined) {\r\n      metrics.increment('cache.hit.l1');\r\n      return l1Value;\r\n    }\r\n\r\n    // L2: Check Redis\r\n    const l2Value = await this.l2.get(key);\r\n    if (l2Value) {\r\n      metrics.increment('cache.hit.l2');\r\n      const parsed = JSON.parse(l2Value);\r\n      this.l1.set(key, parsed); // Populate L1\r\n      return parsed;\r\n    }\r\n\r\n    // L3: Fetch from source\r\n    metrics.increment('cache.miss');\r\n    const value = await fetchFn();\r\n    \r\n    // Populate both layers\r\n    this.l1.set(key, value);\r\n    await this.l2.setex(key, ttl, JSON.stringify(value));\r\n    \r\n    return value;\r\n  }\r\n\r\n  async invalidate(key: string) {\r\n    this.l1.delete(key);\r\n    await this.l2.del(key);\r\n    \r\n    // Broadcast to other instances\r\n    await this.l2.publish('cache:invalidate', key);\r\n  }\r\n}\r\n```\r\n\r\n### 8. Cache Warming\r\n\r\n```typescript\r\n// Pre-populate cache on startup or schedule\r\nclass CacheWarmer {\r\n  async warmOnStartup() {\r\n    logger.info('Starting cache warm-up...');\r\n    \r\n    // Warm frequently accessed data\r\n    await Promise.all([\r\n      this.warmFeatureFlags(),\r\n      this.warmPopularProducts(),\r\n      this.warmActiveUsers(),\r\n    ]);\r\n    \r\n    logger.info('Cache warm-up complete');\r\n  }\r\n\r\n  private async warmFeatureFlags() {\r\n    const flags = await this.db.featureFlags.findAll({ active: true });\r\n    \r\n    const multi = this.cache.multi();\r\n    for (const flag of flags) {\r\n      multi.setex(`feature:${flag.key}`, TTL.FEATURE_FLAGS, JSON.stringify(flag));\r\n    }\r\n    await multi.exec();\r\n    \r\n    logger.info(`Warmed ${flags.length} feature flags`);\r\n  }\r\n\r\n  private async warmPopularProducts() {\r\n    // Top 100 most viewed products\r\n    const products = await this.db.products.findMany({\r\n      orderBy: { viewCount: 'desc' },\r\n      take: 100,\r\n    });\r\n    \r\n    for (const product of products) {\r\n      await this.cache.setex(\r\n        `product:${product.id}`,\r\n        TTL.PRODUCT_DETAILS,\r\n        JSON.stringify(product)\r\n      );\r\n    }\r\n  }\r\n\r\n  // Scheduled warming - run before cache expires\r\n  @Cron('*/5 * * * *') // Every 5 minutes\r\n  async warmScheduled() {\r\n    await this.warmFeatureFlags();\r\n  }\r\n}\r\n```\r\n\r\n## Pitfalls\r\n\r\n| Pitfall | Impact | How to Avoid |\r\n|---------|--------|--------------|\r\n| Cache stampede | Database overwhelmed | Use locks, PER, or stale-while-revalidate |\r\n| No TTL set | Memory bloat, stale forever | Always set TTL, use maxmemory policy |\r\n| Caching nulls | Repeated DB lookups for missing data | Cache negative results with short TTL |\r\n| Cache key collisions | Wrong data served | Include version/tenant in keys |\r\n| Over-caching | Memory waste, stale data | Cache only hot data, measure hit rate |\r\n| Inconsistent invalidation | Stale data after updates | Use event-driven invalidation |\r\n| Single cache instance | SPOF, no HA | Use Redis Cluster or Sentinel |\r\n| Caching sensitive data | Security risk | Encrypt or avoid caching PII |\r\n\r\n## Checklist\r\n\r\n- [ ] Caching strategy chosen (cache-aside, write-through, etc.)\r\n- [ ] TTLs defined for each data type\r\n- [ ] Cache key naming convention documented\r\n- [ ] Stampede prevention implemented\r\n- [ ] Invalidation strategy defined\r\n- [ ] Cache hit/miss metrics tracked\r\n- [ ] Memory limits configured (maxmemory + policy)\r\n- [ ] Cache warming for critical data\r\n- [ ] Null/empty results handled\r\n- [ ] Multi-tenant isolation in cache keys\r\n- [ ] Cache failure graceful degradation\r\n- [ ] Sensitive data encryption or exclusion\r\n- [ ] Redis Cluster/Sentinel for HA\r\n\r\n## References\r\n\r\n- [AWS ElastiCache Best Practices](https://aws.amazon.com/caching/best-practices/)\r\n- [Redis Patterns](https://redis.io/docs/manual/patterns/)\r\n- [Azure Cache-Aside Pattern](https://docs.microsoft.com/en-us/azure/architecture/patterns/cache-aside)\r\n- [XFetch Algorithm Paper](https://cseweb.ucsd.edu/~avattani/papers/cache_stampede.pdf)\r\n"
  },
  {
    "id": "rel-circuit-breaker",
    "title": "Circuit Breaker",
    "tags": [
      "reliability",
      "circuit-breaker",
      "resilience",
      "fault-tolerance",
      "bulkhead"
    ],
    "level": "advanced",
    "stacks": [
      "all"
    ],
    "scope": "reliability",
    "maturity": "stable",
    "works_with": [
      "all"
    ],
    "path": "D:/Code/backend-engineering-kit/.shared/production-backend-kit/patterns/rel.circuit-breaker.md",
    "type": "pattern",
    "sections": [],
    "checklist": [],
    "sources": [],
    "rawContent": "\r\n# Circuit Breaker\r\n\r\n## Problem\r\n\r\nWhen a downstream service fails, continuing to send requests wastes resources, increases latency, and can cause cascading failures. Without circuit breakers, a single failing dependency can bring down your entire system.\r\n\r\n## When to use\r\n\r\n- Calls to external services (APIs, databases)\r\n- Microservice-to-microservice communication\r\n- Any dependency that can fail or be slow\r\n- Preventing cascade failures\r\n- Protecting shared resources (connection pools, threads)\r\n\r\n## Solution\r\n\r\n### 1. Understand Circuit States\r\n\r\n```\r\n     ┌───────────────────────────────────────────┐\r\n     │                                           │\r\n     ▼                                           │\r\n  CLOSED ──(failure rate > threshold)──► OPEN   │\r\n     ▲                                    │      │\r\n     │                              (wait time)  │\r\n     │                                    │      │\r\n     │                                    ▼      │\r\n     └──(N successes)────────────── HALF-OPEN ──┘\r\n                                          │\r\n                                    (any failure)\r\n                                          │\r\n                                          └──► OPEN\r\n```\r\n\r\n| State | Behavior | Transitions |\r\n|-------|----------|-------------|\r\n| **CLOSED** | Normal operation, requests pass through | → OPEN when failure threshold breached |\r\n| **OPEN** | All requests fail fast immediately | → HALF-OPEN after wait duration |\r\n| **HALF-OPEN** | Limited probe requests allowed | → CLOSED if probes succeed, → OPEN if probe fails |\r\n\r\n### 2. Configuration Parameters\r\n\r\n| Parameter | Typical Value | Description |\r\n|-----------|---------------|-------------|\r\n| Failure Rate Threshold | 50% | % of failures to open circuit |\r\n| Slow Call Rate Threshold | 100% | % of slow calls to open |\r\n| Slow Call Duration | 2-5s | What counts as \"slow\" |\r\n| Minimum Calls | 10-20 | Min calls before calculating rate |\r\n| Wait Duration (Open) | 30-60s | Time before trying half-open |\r\n| Permitted Calls (Half-Open) | 3-5 | Probe requests in half-open |\r\n| Sliding Window Size | 10-100 | Calls to consider for rate |\r\n\r\n### 3. Implementation Examples\r\n\r\n**Resilience4j (Java/Kotlin):**\r\n```java\r\nCircuitBreakerConfig config = CircuitBreakerConfig.custom()\r\n    .failureRateThreshold(50)\r\n    .slowCallRateThreshold(100)\r\n    .slowCallDurationThreshold(Duration.ofSeconds(2))\r\n    .waitDurationInOpenState(Duration.ofSeconds(30))\r\n    .permittedNumberOfCallsInHalfOpenState(3)\r\n    .minimumNumberOfCalls(10)\r\n    .slidingWindowSize(20)\r\n    .build();\r\n\r\nCircuitBreaker circuitBreaker = CircuitBreaker.of(\"backendService\", config);\r\n\r\nSupplier<String> decoratedSupplier = CircuitBreaker\r\n    .decorateSupplier(circuitBreaker, backendService::call);\r\n```\r\n\r\n**Polly (.NET):**\r\n```csharp\r\nvar circuitBreakerPolicy = Policy\r\n    .Handle<HttpRequestException>()\r\n    .CircuitBreakerAsync(\r\n        exceptionsAllowedBeforeBreaking: 5,\r\n        durationOfBreak: TimeSpan.FromSeconds(30),\r\n        onBreak: (ex, breakDelay) => Log.Warn(\"Circuit opened\"),\r\n        onReset: () => Log.Info(\"Circuit closed\"),\r\n        onHalfOpen: () => Log.Info(\"Circuit half-open\")\r\n    );\r\n```\r\n\r\n**Node.js (opossum):**\r\n```typescript\r\nimport CircuitBreaker from 'opossum';\r\n\r\nconst breaker = new CircuitBreaker(asyncFunction, {\r\n  timeout: 3000,           // 3s timeout\r\n  errorThresholdPercentage: 50,\r\n  resetTimeout: 30000,     // 30s before half-open\r\n  volumeThreshold: 10,     // Min calls before tripping\r\n});\r\n\r\nbreaker.fallback(() => cachedResponse);\r\nbreaker.on('open', () => console.log('Circuit opened'));\r\n```\r\n\r\n## Pitfalls\r\n\r\n| Pitfall | How to Avoid |\r\n|---------|--------------|\r\n| Too sensitive thresholds | Tune based on normal error rates |\r\n| Too long open duration | Balance recovery time vs availability |\r\n| Not handling half-open properly | Limit probe requests in half-open |\r\n| Circuit per-instance, not per-dependency | Group by logical dependency |\r\n| No fallback behavior | Always define degraded response |\r\n\r\n## Checklist\r\n\r\n- [ ] Circuit breaker configured for external calls\r\n- [ ] Failure threshold defined (count/percentage)\r\n- [ ] Open state timeout configured\r\n- [ ] Half-open probe strategy defined\r\n- [ ] Fallback behavior implemented\r\n- [ ] Circuit state metrics exposed\r\n- [ ] Alerts on circuit open events\r\n- [ ] Different circuits per dependency\r\n- [ ] Circuit state logged\r\n- [ ] Recovery behavior tested\r\n\r\n## Snippets (Generic)\r\n\r\n```\r\nCircuit Breaker States:\r\n     ┌──────────────────────────────────────┐\r\n     │                                      │\r\n     ▼                                      │\r\n  CLOSED ──(failures > threshold)──▶ OPEN  │\r\n     ▲                                │     │\r\n     │                                │     │\r\n     │                         (timeout)    │\r\n     │                                │     │\r\n     │                                ▼     │\r\n     └──(successes > threshold)── HALF-OPEN┘\r\n                                      │\r\n                               (failure)\r\n                                      │\r\n                                      └──▶ OPEN\r\n\r\nConfiguration Example:\r\ncircuit_breaker = CircuitBreaker(\r\n  failure_threshold=5,        # Open after 5 failures\r\n  success_threshold=3,        # Close after 3 successes in half-open\r\n  timeout=30s,                # Time in open state before half-open\r\n  failure_rate_threshold=50%  # Or use percentage\r\n)\r\n\r\nUsage Pattern:\r\ndef call_external_service():\r\n  if circuit.is_open():\r\n    return fallback_response()\r\n  \r\n  try:\r\n    response = http.get(external_url)\r\n    circuit.record_success()\r\n    return response\r\n  except Exception as e:\r\n    circuit.record_failure()\r\n    if circuit.is_open():\r\n      log.warn(\"Circuit opened for external_service\")\r\n    raise\r\n\r\nFallback Strategies:\r\n- Return cached data\r\n- Return default/empty response\r\n- Call alternative service\r\n- Queue for later processing\r\n- Return error with context\r\n\r\nMetrics to Track:\r\n- Circuit state (closed/open/half-open)\r\n- Failure count and rate\r\n- Time spent in each state\r\n- Fallback invocation count\r\n```\r\n\r\n## Sources\r\n\r\n- Martin Fowler - Circuit Breaker: https://martinfowler.com/bliki/CircuitBreaker.html\r\n- Netflix Hystrix (archived but educational): https://github.com/Netflix/Hystrix/wiki\r\n- Resilience4j Circuit Breaker: https://resilience4j.readme.io/docs/circuitbreaker\r\n- Microsoft Circuit Breaker Pattern: https://learn.microsoft.com/en-us/azure/architecture/patterns/circuit-breaker\r\n"
  },
  {
    "id": "rel-dlq-basics",
    "title": "Dead Letter Queue Basics",
    "tags": [
      "reliability",
      "dlq",
      "messaging",
      "error-handling"
    ],
    "level": "intermediate",
    "stacks": [
      "all"
    ],
    "scope": "reliability",
    "maturity": "stable",
    "works_with": [
      "all"
    ],
    "path": "D:/Code/backend-engineering-kit/.shared/production-backend-kit/patterns/rel.dlq-basics.md",
    "type": "pattern",
    "sections": [],
    "checklist": [],
    "sources": [],
    "rawContent": "\r\n# Dead Letter Queue Basics\r\n\r\n## Problem\r\n\r\nMessages that repeatedly fail processing can block queues, causing backlog and preventing healthy messages from being processed. You need a way to isolate problematic messages for later analysis.\r\n\r\n## When to use\r\n\r\n- Any message queue or event-driven system\r\n- Background job processing\r\n- Webhook handling\r\n- Event sourcing\r\n- Async task execution\r\n\r\n## Solution\r\n\r\n1. **Configure DLQ routing**\r\n   - Route failed messages after N retries\r\n   - Preserve original message + metadata\r\n   - Add failure context (error, attempts, timestamp)\r\n\r\n2. **Define failure criteria**\r\n   - Max retry count exceeded\r\n   - Specific exception types\r\n   - Message expired (TTL)\r\n\r\n3. **Monitor and alert**\r\n   - Track DLQ depth\r\n   - Alert on sudden spikes\r\n   - Set up dashboards\r\n\r\n4. **Handle DLQ messages**\r\n   - Manual review and fix\r\n   - Automated replay after fix\r\n   - Archive or delete after analysis\r\n\r\n## Pitfalls\r\n\r\n| Pitfall | How to Avoid |\r\n|---------|--------------|\r\n| DLQ grows unbounded | Set retention/TTL, regular cleanup |\r\n| No alerting on DLQ | Monitor depth, alert on threshold |\r\n| Losing failure context | Store original message + error info |\r\n| No replay mechanism | Build tooling to replay from DLQ |\r\n| Retrying without fixing | Analyze root cause before replay |\r\n\r\n## Checklist\r\n\r\n- [ ] DLQ configured for each queue\r\n- [ ] Max retry count defined before DLQ\r\n- [ ] Original message preserved in DLQ\r\n- [ ] Failure context stored (error, traces)\r\n- [ ] DLQ depth monitored and alerted\r\n- [ ] Retention policy for DLQ messages\r\n- [ ] Replay mechanism available\r\n- [ ] DLQ messages auditable\r\n- [ ] Root cause analysis process defined\r\n- [ ] Runbook for DLQ handling\r\n\r\n## Snippets (Generic)\r\n\r\n```\r\nDLQ Message Structure:\r\n{\r\n  \"original_message\": {\r\n    \"type\": \"OrderCreated\",\r\n    \"payload\": { \"orderId\": 123, \"amount\": 100 }\r\n  },\r\n  \"failure_context\": {\r\n    \"queue\": \"orders\",\r\n    \"first_failure\": \"2026-01-14T10:00:00Z\",\r\n    \"last_failure\": \"2026-01-14T10:05:00Z\",\r\n    \"attempt_count\": 5,\r\n    \"last_error\": \"ValidationError: Invalid product ID\",\r\n    \"stack_trace\": \"...\",\r\n    \"correlation_id\": \"corr_abc123\"\r\n  }\r\n}\r\n\r\nMessage Flow:\r\nMain Queue ─┬─► Success ─► Done\r\n            │\r\n            └─► Failure ─► Retry (up to N times)\r\n                              │\r\n                              └─► DLQ (after N failures)\r\n\r\nDLQ Handling Process:\r\n1. Alert received: DLQ depth > threshold\r\n2. Investigate sample messages\r\n3. Identify root cause (bug, bad data, dependency)\r\n4. Fix the issue\r\n5. Replay messages from DLQ\r\n6. Monitor for success\r\n7. Archive processed DLQ messages\r\n\r\nAWS SQS DLQ Config (pseudo):\r\nmain_queue:\r\n  redrive_policy:\r\n    dead_letter_queue: dlq_arn\r\n    max_receive_count: 5\r\n\r\nRabbitMQ DLQ Config (pseudo):\r\nexchange: orders-exchange\r\nqueue: orders-queue\r\n  arguments:\r\n    x-dead-letter-exchange: orders-dlx\r\n    x-dead-letter-routing-key: orders-dlq\r\n\r\nDLQ Tooling Commands:\r\n# View DLQ messages\r\ndlq-tool list --queue orders-dlq --limit 10\r\n\r\n# Replay single message\r\ndlq-tool replay --queue orders-dlq --message-id msg_123\r\n\r\n# Replay all (after fix deployed)\r\ndlq-tool replay-all --queue orders-dlq --dry-run\r\n```\r\n\r\n## Sources\r\n\r\n- AWS SQS Dead Letter Queues: https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html\r\n- RabbitMQ Dead Letter Exchanges: https://www.rabbitmq.com/docs/dlx\r\n- Azure Service Bus DLQ: https://learn.microsoft.com/en-us/azure/service-bus-messaging/service-bus-dead-letter-queues\r\n- Kafka Error Handling: https://www.confluent.io/blog/error-handling-patterns-in-kafka/\r\n"
  },
  {
    "id": "rel-graceful-shutdown",
    "title": "Graceful Shutdown",
    "tags": [
      "reliability",
      "shutdown",
      "kubernetes",
      "devops",
      "deployment"
    ],
    "level": "intermediate",
    "stacks": [
      "all"
    ],
    "scope": "reliability",
    "maturity": "stable",
    "works_with": [
      "all"
    ],
    "path": "D:/Code/backend-engineering-kit/.shared/production-backend-kit/patterns/rel.graceful-shutdown.md",
    "type": "pattern",
    "sections": [],
    "checklist": [],
    "sources": [],
    "rawContent": "\r\n# Graceful Shutdown\r\n\r\n## Problem\r\n\r\nAbrupt process termination causes:\r\n- In-flight requests dropped (500 errors for users)\r\n- Database transactions left incomplete\r\n- Message queue messages lost\r\n- WebSocket connections cut without notice\r\n- Resource leaks (file handles, connections)\r\n- Data corruption in write operations\r\n\r\n## When to use\r\n\r\n- **Every production service** - no exceptions\r\n- Kubernetes deployments (required for zero-downtime)\r\n- Any service handling stateful requests\r\n- Background job processors\r\n- WebSocket/long-polling servers\r\n- Services with database transactions\r\n\r\n## Solution\r\n\r\n### 1. Shutdown Sequence\r\n\r\n```\r\n┌─────────────────────────────────────────────────────────────────────────────┐\r\n│                    GRACEFUL SHUTDOWN SEQUENCE                               │\r\n└─────────────────────────────────────────────────────────────────────────────┘\r\n\r\n   SIGTERM received\r\n         │\r\n         ▼\r\n   ┌─────────────────┐\r\n   │ 1. STOP ACCEPTING│   Stop receiving new requests\r\n   │    NEW REQUESTS  │   Return 503 for new connections\r\n   └────────┬────────┘   Unregister from service discovery\r\n            │\r\n            ▼\r\n   ┌─────────────────┐\r\n   │ 2. WAIT FOR     │   Let in-flight requests complete\r\n   │    IN-FLIGHT    │   Drain connection pools\r\n   └────────┬────────┘   Configurable timeout (15-30s)\r\n            │\r\n            ▼\r\n   ┌─────────────────┐\r\n   │ 3. CLOSE        │   Close database connections\r\n   │    CONNECTIONS  │   Close Redis/cache connections\r\n   └────────┬────────┘   Close message queue connections\r\n            │\r\n            ▼\r\n   ┌─────────────────┐\r\n   │ 4. FLUSH &      │   Flush logs to disk\r\n   │    CLEANUP      │   Flush metrics\r\n   └────────┬────────┘   Release file handles\r\n            │\r\n            ▼\r\n   ┌─────────────────┐\r\n   │ 5. EXIT         │   Exit with code 0 (success)\r\n   └─────────────────┘\r\n\r\n\r\nTimeline:\r\n0s          15s         30s\r\n│───────────│───────────│\r\n│ Draining  │ Force     │\r\n│ requests  │ shutdown  │\r\n│           │ (SIGKILL) │\r\n```\r\n\r\n### 2. Kubernetes Lifecycle\r\n\r\n```yaml\r\napiVersion: v1\r\nkind: Pod\r\nspec:\r\n  # Time between SIGTERM and SIGKILL\r\n  terminationGracePeriodSeconds: 30\r\n  \r\n  containers:\r\n    - name: app\r\n      lifecycle:\r\n        preStop:\r\n          exec:\r\n            # Give load balancer time to remove pod from pool\r\n            command: [\"sleep\", \"5\"]\r\n```\r\n\r\n```\r\nPod Termination Timeline:\r\n─────────────────────────────────────────────────────────────────────\r\n0s       5s                                    25s       30s\r\n│        │                                      │         │\r\n│ preStop│        Application shutdown          │ Timeout │\r\n│ hook   │        (handle SIGTERM)              │         │\r\n│        │                                      │         │\r\n│        │◀─────── Your shutdown window ───────▶│         │\r\n│        │                                      │         │\r\n│ LB removes pod from endpoints                 │ SIGKILL │\r\n─────────────────────────────────────────────────────────────────────\r\n```\r\n\r\n### 3. Signal Handling\r\n\r\n| Signal | Default Action | Graceful Action |\r\n|--------|---------------|-----------------|\r\n| `SIGTERM` | Terminate | Begin graceful shutdown |\r\n| `SIGINT` | Terminate | Begin graceful shutdown |\r\n| `SIGQUIT` | Core dump | Graceful shutdown + dump |\r\n| `SIGKILL` | Immediate kill | Cannot be caught! |\r\n| `SIGHUP` | Terminate | Reload configuration |\r\n\r\n## Pitfalls\r\n\r\n| Pitfall | Impact | How to Avoid |\r\n|---------|--------|--------------|\r\n| Not handling SIGTERM | Requests dropped on deploy | Always register signal handlers |\r\n| Too short grace period | Requests still dropped | Set appropriate timeout (15-30s) |\r\n| No preStop hook in K8s | Traffic during shutdown | Add sleep in preStop |\r\n| Infinite shutdown wait | Pod never terminates | Set hard timeout |\r\n| Not draining connections | Active connections dropped | Wait for in-flight requests |\r\n| Force-killing DB transactions | Data corruption | Await pending transactions |\r\n| Not closing message consumers | Lost messages | Stop consuming before exit |\r\n\r\n## Checklist\r\n\r\n- [ ] SIGTERM handler registered\r\n- [ ] SIGINT handler for local dev\r\n- [ ] Stop accepting new requests on shutdown\r\n- [ ] Wait for in-flight requests (with timeout)\r\n- [ ] Close database connection pools\r\n- [ ] Close cache connections (Redis)\r\n- [ ] Stop message queue consumers\r\n- [ ] Flush logs before exit\r\n- [ ] Kubernetes `terminationGracePeriodSeconds` set\r\n- [ ] preStop hook for load balancer sync\r\n- [ ] Health check returns 503 during shutdown\r\n- [ ] Hard timeout prevents infinite hang\r\n- [ ] Exit with code 0 on clean shutdown\r\n\r\n## Code Examples\r\n\r\n### TypeScript/Node.js (Express)\r\n\r\n```typescript\r\nimport express from 'express';\r\nimport { Server } from 'http';\r\nimport { Pool } from 'pg';\r\n\r\nclass GracefulShutdown {\r\n  private isShuttingDown = false;\r\n  private server: Server | null = null;\r\n  private connections = new Set<any>();\r\n  private shutdownTimeout = 30000; // 30 seconds\r\n  \r\n  constructor(\r\n    private app: express.Application,\r\n    private db: Pool,\r\n    private redis: any,\r\n  ) {}\r\n  \r\n  start(port: number): Server {\r\n    this.server = this.app.listen(port, () => {\r\n      console.log(`Server listening on port ${port}`);\r\n    });\r\n    \r\n    // Track connections for forced cleanup\r\n    this.server.on('connection', (conn) => {\r\n      this.connections.add(conn);\r\n      conn.on('close', () => this.connections.delete(conn));\r\n    });\r\n    \r\n    // Register signal handlers\r\n    process.on('SIGTERM', () => this.shutdown('SIGTERM'));\r\n    process.on('SIGINT', () => this.shutdown('SIGINT'));\r\n    \r\n    // Middleware to reject requests during shutdown\r\n    this.app.use((req, res, next) => {\r\n      if (this.isShuttingDown) {\r\n        res.status(503).json({\r\n          error: 'Service is shutting down',\r\n          retryAfter: 5,\r\n        });\r\n        return;\r\n      }\r\n      next();\r\n    });\r\n    \r\n    return this.server;\r\n  }\r\n  \r\n  async shutdown(signal: string): Promise<void> {\r\n    if (this.isShuttingDown) {\r\n      console.log('Shutdown already in progress');\r\n      return;\r\n    }\r\n    \r\n    console.log(`\\n${signal} received. Starting graceful shutdown...`);\r\n    this.isShuttingDown = true;\r\n    \r\n    // Set hard timeout\r\n    const forceShutdownTimer = setTimeout(() => {\r\n      console.error('Graceful shutdown timed out. Forcing exit.');\r\n      process.exit(1);\r\n    }, this.shutdownTimeout);\r\n    \r\n    try {\r\n      // 1. Stop accepting new connections\r\n      await this.closeServer();\r\n      console.log('✓ Server closed to new connections');\r\n      \r\n      // 2. Wait for existing requests to complete\r\n      await this.drainConnections();\r\n      console.log('✓ Active connections drained');\r\n      \r\n      // 3. Close database pool\r\n      await this.db.end();\r\n      console.log('✓ Database connections closed');\r\n      \r\n      // 4. Close Redis\r\n      await this.redis.quit();\r\n      console.log('✓ Redis connection closed');\r\n      \r\n      // 5. Flush logs\r\n      await this.flushLogs();\r\n      console.log('✓ Logs flushed');\r\n      \r\n      clearTimeout(forceShutdownTimer);\r\n      console.log('Graceful shutdown complete');\r\n      process.exit(0);\r\n      \r\n    } catch (error) {\r\n      console.error('Error during shutdown:', error);\r\n      clearTimeout(forceShutdownTimer);\r\n      process.exit(1);\r\n    }\r\n  }\r\n  \r\n  private closeServer(): Promise<void> {\r\n    return new Promise((resolve, reject) => {\r\n      if (!this.server) {\r\n        resolve();\r\n        return;\r\n      }\r\n      \r\n      this.server.close((err) => {\r\n        if (err) reject(err);\r\n        else resolve();\r\n      });\r\n    });\r\n  }\r\n  \r\n  private drainConnections(): Promise<void> {\r\n    return new Promise((resolve) => {\r\n      // Give connections time to finish\r\n      const checkInterval = setInterval(() => {\r\n        if (this.connections.size === 0) {\r\n          clearInterval(checkInterval);\r\n          resolve();\r\n        }\r\n      }, 100);\r\n      \r\n      // Set max drain time\r\n      setTimeout(() => {\r\n        clearInterval(checkInterval);\r\n        // Force close remaining connections\r\n        for (const conn of this.connections) {\r\n          conn.destroy();\r\n        }\r\n        this.connections.clear();\r\n        resolve();\r\n      }, 10000); // 10s max drain time\r\n    });\r\n  }\r\n  \r\n  private async flushLogs(): Promise<void> {\r\n    // If using async logging, ensure buffers are flushed\r\n    await new Promise(resolve => setTimeout(resolve, 100));\r\n  }\r\n}\r\n\r\n// Usage\r\nconst app = express();\r\nconst db = new Pool({ /* config */ });\r\nconst redis = new Redis({ /* config */ });\r\n\r\nconst graceful = new GracefulShutdown(app, db, redis);\r\n\r\n// Add routes\r\napp.get('/health/live', (req, res) => res.json({ status: 'ok' }));\r\napp.get('/health/ready', (req, res) => {\r\n  if (graceful.isShuttingDown) {\r\n    return res.status(503).json({ status: 'shutting_down' });\r\n  }\r\n  res.json({ status: 'ok' });\r\n});\r\n\r\ngraceful.start(3000);\r\n```\r\n\r\n### Python/FastAPI (with uvicorn)\r\n\r\n```python\r\nimport asyncio\r\nimport signal\r\nfrom contextlib import asynccontextmanager\r\nfrom fastapi import FastAPI, Request, Response\r\nfrom typing import Set\r\nimport logging\r\n\r\nlogger = logging.getLogger(__name__)\r\n\r\nclass ShutdownManager:\r\n    def __init__(self):\r\n        self.is_shutting_down = False\r\n        self.active_requests: Set[int] = set()\r\n        self._request_counter = 0\r\n        \r\n    def start_request(self) -> int:\r\n        self._request_counter += 1\r\n        request_id = self._request_counter\r\n        self.active_requests.add(request_id)\r\n        return request_id\r\n    \r\n    def end_request(self, request_id: int):\r\n        self.active_requests.discard(request_id)\r\n    \r\n    async def wait_for_requests(self, timeout: float = 10.0):\r\n        \"\"\"Wait for active requests to complete\"\"\"\r\n        start = asyncio.get_event_loop().time()\r\n        while self.active_requests:\r\n            if asyncio.get_event_loop().time() - start > timeout:\r\n                logger.warning(f\"Timeout waiting for {len(self.active_requests)} requests\")\r\n                break\r\n            await asyncio.sleep(0.1)\r\n\r\nshutdown_manager = ShutdownManager()\r\n\r\n@asynccontextmanager\r\nasync def lifespan(app: FastAPI):\r\n    # Startup\r\n    logger.info(\"Starting application\")\r\n    await db.connect()\r\n    await redis.connect()\r\n    \r\n    yield  # Application runs here\r\n    \r\n    # Shutdown\r\n    logger.info(\"Starting graceful shutdown\")\r\n    shutdown_manager.is_shutting_down = True\r\n    \r\n    # Wait for in-flight requests\r\n    await shutdown_manager.wait_for_requests(timeout=15.0)\r\n    logger.info(\"✓ Active requests drained\")\r\n    \r\n    # Close connections\r\n    await db.disconnect()\r\n    logger.info(\"✓ Database disconnected\")\r\n    \r\n    await redis.close()\r\n    logger.info(\"✓ Redis disconnected\")\r\n    \r\n    logger.info(\"Graceful shutdown complete\")\r\n\r\napp = FastAPI(lifespan=lifespan)\r\n\r\n@app.middleware(\"http\")\r\nasync def track_requests(request: Request, call_next):\r\n    # Reject new requests during shutdown\r\n    if shutdown_manager.is_shutting_down:\r\n        return Response(\r\n            content='{\"error\": \"Service shutting down\"}',\r\n            status_code=503,\r\n            media_type=\"application/json\",\r\n            headers={\"Retry-After\": \"5\"}\r\n        )\r\n    \r\n    request_id = shutdown_manager.start_request()\r\n    try:\r\n        response = await call_next(request)\r\n        return response\r\n    finally:\r\n        shutdown_manager.end_request(request_id)\r\n\r\n@app.get(\"/health/ready\")\r\nasync def readiness():\r\n    if shutdown_manager.is_shutting_down:\r\n        return Response(status_code=503, content='{\"status\": \"shutting_down\"}')\r\n    return {\"status\": \"ok\"}\r\n\r\n# For running with uvicorn\r\nif __name__ == \"__main__\":\r\n    import uvicorn\r\n    \r\n    config = uvicorn.Config(\r\n        app=app,\r\n        host=\"0.0.0.0\",\r\n        port=8000,\r\n        # Graceful shutdown timeout\r\n        timeout_graceful_shutdown=30,\r\n    )\r\n    server = uvicorn.Server(config)\r\n    \r\n    # Handle signals\r\n    loop = asyncio.get_event_loop()\r\n    \r\n    def handle_signal(sig):\r\n        logger.info(f\"Received {sig.name}\")\r\n        loop.create_task(server.shutdown())\r\n    \r\n    for sig in (signal.SIGTERM, signal.SIGINT):\r\n        loop.add_signal_handler(sig, handle_signal, sig)\r\n    \r\n    loop.run_until_complete(server.serve())\r\n```\r\n\r\n### Go\r\n\r\n```go\r\npackage main\r\n\r\nimport (\r\n    \"context\"\r\n    \"log\"\r\n    \"net/http\"\r\n    \"os\"\r\n    \"os/signal\"\r\n    \"sync\"\r\n    \"sync/atomic\"\r\n    \"syscall\"\r\n    \"time\"\r\n)\r\n\r\ntype Server struct {\r\n    httpServer      *http.Server\r\n    isShuttingDown  atomic.Bool\r\n    activeRequests  sync.WaitGroup\r\n    db              *sql.DB\r\n    redis           *redis.Client\r\n}\r\n\r\nfunc NewServer(addr string, db *sql.DB, redis *redis.Client) *Server {\r\n    s := &Server{\r\n        db:    db,\r\n        redis: redis,\r\n    }\r\n    \r\n    mux := http.NewServeMux()\r\n    \r\n    // Health endpoints\r\n    mux.HandleFunc(\"/health/live\", func(w http.ResponseWriter, r *http.Request) {\r\n        w.Write([]byte(`{\"status\":\"ok\"}`))\r\n    })\r\n    \r\n    mux.HandleFunc(\"/health/ready\", func(w http.ResponseWriter, r *http.Request) {\r\n        if s.isShuttingDown.Load() {\r\n            w.WriteHeader(http.StatusServiceUnavailable)\r\n            w.Write([]byte(`{\"status\":\"shutting_down\"}`))\r\n            return\r\n        }\r\n        w.Write([]byte(`{\"status\":\"ok\"}`))\r\n    })\r\n    \r\n    // Wrap handler to track requests\r\n    handler := http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\r\n        if s.isShuttingDown.Load() {\r\n            w.Header().Set(\"Retry-After\", \"5\")\r\n            http.Error(w, `{\"error\":\"Service shutting down\"}`, http.StatusServiceUnavailable)\r\n            return\r\n        }\r\n        \r\n        s.activeRequests.Add(1)\r\n        defer s.activeRequests.Done()\r\n        \r\n        mux.ServeHTTP(w, r)\r\n    })\r\n    \r\n    s.httpServer = &http.Server{\r\n        Addr:    addr,\r\n        Handler: handler,\r\n    }\r\n    \r\n    return s\r\n}\r\n\r\nfunc (s *Server) Start() error {\r\n    // Channel for shutdown signals\r\n    stop := make(chan os.Signal, 1)\r\n    signal.Notify(stop, syscall.SIGTERM, syscall.SIGINT)\r\n    \r\n    // Start server in goroutine\r\n    go func() {\r\n        log.Printf(\"Server starting on %s\", s.httpServer.Addr)\r\n        if err := s.httpServer.ListenAndServe(); err != http.ErrServerClosed {\r\n            log.Fatalf(\"Server error: %v\", err)\r\n        }\r\n    }()\r\n    \r\n    // Wait for shutdown signal\r\n    sig := <-stop\r\n    log.Printf(\"Received %v signal. Starting graceful shutdown...\", sig)\r\n    \r\n    return s.Shutdown()\r\n}\r\n\r\nfunc (s *Server) Shutdown() error {\r\n    s.isShuttingDown.Store(true)\r\n    \r\n    // Create context with timeout\r\n    ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)\r\n    defer cancel()\r\n    \r\n    // 1. Stop accepting new connections\r\n    if err := s.httpServer.Shutdown(ctx); err != nil {\r\n        log.Printf(\"HTTP server shutdown error: %v\", err)\r\n    }\r\n    log.Println(\"✓ Server closed to new connections\")\r\n    \r\n    // 2. Wait for active requests to complete\r\n    done := make(chan struct{})\r\n    go func() {\r\n        s.activeRequests.Wait()\r\n        close(done)\r\n    }()\r\n    \r\n    select {\r\n    case <-done:\r\n        log.Println(\"✓ Active requests drained\")\r\n    case <-ctx.Done():\r\n        log.Println(\"⚠ Timeout waiting for active requests\")\r\n    }\r\n    \r\n    // 3. Close database\r\n    if err := s.db.Close(); err != nil {\r\n        log.Printf(\"Database close error: %v\", err)\r\n    }\r\n    log.Println(\"✓ Database connection closed\")\r\n    \r\n    // 4. Close Redis\r\n    if err := s.redis.Close(); err != nil {\r\n        log.Printf(\"Redis close error: %v\", err)\r\n    }\r\n    log.Println(\"✓ Redis connection closed\")\r\n    \r\n    log.Println(\"Graceful shutdown complete\")\r\n    return nil\r\n}\r\n\r\nfunc main() {\r\n    db, _ := sql.Open(\"postgres\", os.Getenv(\"DATABASE_URL\"))\r\n    redis := redis.NewClient(&redis.Options{Addr: \"localhost:6379\"})\r\n    \r\n    server := NewServer(\":8080\", db, redis)\r\n    if err := server.Start(); err != nil {\r\n        log.Fatal(err)\r\n    }\r\n}\r\n```\r\n\r\n## References\r\n\r\n- [Kubernetes - Pod Lifecycle](https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination)\r\n- [AWS - Avoiding Overload in Distributed Systems](https://aws.amazon.com/builders-library/avoiding-overload-in-distributed-systems/)\r\n- [Node.js Graceful Shutdown in Kubernetes](https://blog.risingstack.com/graceful-shutdown-node-js-kubernetes/)\r\n- [Go - Graceful HTTP Server Shutdown](https://pkg.go.dev/net/http#Server.Shutdown)\r\n"
  },
  {
    "id": "rel-health-checks",
    "title": "Health Check Endpoints",
    "tags": [
      "reliability",
      "health-check",
      "monitoring",
      "kubernetes",
      "devops"
    ],
    "level": "intermediate",
    "stacks": [
      "all"
    ],
    "scope": "reliability",
    "maturity": "stable",
    "works_with": [
      "all"
    ],
    "path": "D:/Code/backend-engineering-kit/.shared/production-backend-kit/patterns/rel.health-checks.md",
    "type": "pattern",
    "sections": [],
    "checklist": [],
    "sources": [],
    "rawContent": "\r\n# Health Check Endpoints\r\n\r\n## Problem\r\n\r\nWithout health checks:\r\n- Load balancers send traffic to unhealthy instances\r\n- Container orchestrators don't know when to restart pods\r\n- Deployments can't verify successful rollout\r\n- Cascading failures spread undetected\r\n- Debugging production issues becomes guesswork\r\n\r\n## When to use\r\n\r\n- **Always** for any production service\r\n- Kubernetes/Docker deployments (required)\r\n- Behind load balancers\r\n- Microservices architectures\r\n- Any service with external dependencies\r\n\r\n## Solution\r\n\r\n### 1. Three Types of Health Checks\r\n\r\n```\r\n┌─────────────────────────────────────────────────────────────────────┐\r\n│ Type          │ Purpose              │ Failure Action              │\r\n├───────────────┼──────────────────────┼─────────────────────────────┤\r\n│ LIVENESS      │ Is the process alive?│ Restart container           │\r\n│ /health/live  │ Can it respond?      │ Kill and recreate           │\r\n├───────────────┼──────────────────────┼─────────────────────────────┤\r\n│ READINESS     │ Can it serve traffic?│ Remove from load balancer   │\r\n│ /health/ready │ Are deps available?  │ Stop sending requests       │\r\n├───────────────┼──────────────────────┼─────────────────────────────┤\r\n│ STARTUP       │ Has it started?      │ Wait longer before probes   │\r\n│ /health/start │ Initialization done? │ Extend startup time         │\r\n└─────────────────────────────────────────────────────────────────────┘\r\n\r\nKey Insight:\r\n- LIVENESS: \"Should this be killed and restarted?\"\r\n- READINESS: \"Should this receive traffic right now?\"\r\n- STARTUP: \"Has this finished booting?\"\r\n```\r\n\r\n### 2. What Each Check Should Verify\r\n\r\n```typescript\r\n// LIVENESS - Keep it simple!\r\n// Only check if the process can respond\r\n// DON'T check external dependencies here\r\nGET /health/live\r\nResponse: { \"status\": \"ok\" }\r\n\r\n// READINESS - Check dependencies\r\n// Verify the service can actually serve requests\r\nGET /health/ready\r\nResponse: {\r\n  \"status\": \"ok\",  // or \"degraded\" or \"unhealthy\"\r\n  \"checks\": {\r\n    \"database\": { \"status\": \"ok\", \"latency_ms\": 5 },\r\n    \"cache\": { \"status\": \"ok\", \"latency_ms\": 2 },\r\n    \"external_api\": { \"status\": \"degraded\", \"latency_ms\": 500 }\r\n  }\r\n}\r\n\r\n// STARTUP - One-time initialization\r\n// Check if app has finished starting\r\nGET /health/startup\r\nResponse: { \"status\": \"ok\", \"started_at\": \"2026-01-19T12:00:00Z\" }\r\n```\r\n\r\n### 3. Response Format\r\n\r\n```json\r\n// Healthy\r\nHTTP/1.1 200 OK\r\n{\r\n  \"status\": \"ok\",\r\n  \"version\": \"1.2.3\",\r\n  \"uptime_seconds\": 86400,\r\n  \"checks\": {\r\n    \"database\": {\r\n      \"status\": \"ok\",\r\n      \"latency_ms\": 5,\r\n      \"message\": \"Connected to primary\"\r\n    },\r\n    \"redis\": {\r\n      \"status\": \"ok\",\r\n      \"latency_ms\": 2\r\n    }\r\n  }\r\n}\r\n\r\n// Degraded (still serving, but not optimal)\r\nHTTP/1.1 200 OK\r\n{\r\n  \"status\": \"degraded\",\r\n  \"checks\": {\r\n    \"database\": { \"status\": \"ok\" },\r\n    \"redis\": { \r\n      \"status\": \"degraded\",\r\n      \"message\": \"Failover to replica\"\r\n    }\r\n  }\r\n}\r\n\r\n// Unhealthy\r\nHTTP/1.1 503 Service Unavailable\r\n{\r\n  \"status\": \"unhealthy\",\r\n  \"checks\": {\r\n    \"database\": {\r\n      \"status\": \"unhealthy\",\r\n      \"error\": \"Connection refused\",\r\n      \"last_check\": \"2026-01-19T12:00:00Z\"\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n### 4. Kubernetes Configuration\r\n\r\n```yaml\r\napiVersion: v1\r\nkind: Pod\r\nspec:\r\n  containers:\r\n    - name: app\r\n      image: myapp:1.0\r\n      ports:\r\n        - containerPort: 8080\r\n      \r\n      # STARTUP probe - runs first during boot\r\n      startupProbe:\r\n        httpGet:\r\n          path: /health/startup\r\n          port: 8080\r\n        initialDelaySeconds: 5\r\n        periodSeconds: 5\r\n        failureThreshold: 30  # Allow up to 150s for startup\r\n      \r\n      # LIVENESS probe - runs after startup passes\r\n      livenessProbe:\r\n        httpGet:\r\n          path: /health/live\r\n          port: 8080\r\n        initialDelaySeconds: 0\r\n        periodSeconds: 10\r\n        timeoutSeconds: 5\r\n        failureThreshold: 3  # Restart after 3 failures\r\n      \r\n      # READINESS probe - determines if receives traffic\r\n      readinessProbe:\r\n        httpGet:\r\n          path: /health/ready\r\n          port: 8080\r\n        initialDelaySeconds: 0\r\n        periodSeconds: 5\r\n        timeoutSeconds: 3\r\n        failureThreshold: 1  # Remove immediately on failure\r\n```\r\n\r\n## Pitfalls\r\n\r\n| Pitfall | Impact | How to Avoid |\r\n|---------|--------|--------------|\r\n| External deps in liveness | Restart loops when DB down | Only check process health |\r\n| Missing readiness check | Traffic to unready instances | Always implement readiness |\r\n| Slow health checks | Probe timeouts, false failures | Set timeout, cache results |\r\n| No startup probe | Slow apps killed during boot | Use startup for slow init |\r\n| Exposing sensitive info | Security risk | Don't include credentials/paths |\r\n| Missing dependency check | Serving errors silently | Check all critical deps |\r\n| Health check on same thread | Blocked by slow requests | Use dedicated thread/async |\r\n\r\n## Checklist\r\n\r\n- [ ] `/health/live` endpoint returns 200 quickly (no dep checks)\r\n- [ ] `/health/ready` checks all critical dependencies\r\n- [ ] `/health/startup` for slow-starting applications\r\n- [ ] Response time under timeout threshold\r\n- [ ] Appropriate HTTP status codes (200/503)\r\n- [ ] Dependency checks have individual timeouts\r\n- [ ] Health checks don't expose sensitive info\r\n- [ ] Kubernetes/ELB probes configured correctly\r\n- [ ] Failed checks logged with context\r\n- [ ] Health check metrics exported\r\n- [ ] Degraded state supported (not just ok/fail)\r\n- [ ] Health checks are authenticated (if needed)\r\n\r\n## Code Examples\r\n\r\n### TypeScript/Node.js (Express)\r\n\r\n```typescript\r\nimport { Router } from 'express';\r\nimport { Pool } from 'pg';\r\nimport Redis from 'ioredis';\r\n\r\ninterface HealthCheck {\r\n  status: 'ok' | 'degraded' | 'unhealthy';\r\n  latency_ms?: number;\r\n  message?: string;\r\n  error?: string;\r\n}\r\n\r\ninterface HealthResponse {\r\n  status: 'ok' | 'degraded' | 'unhealthy';\r\n  version: string;\r\n  uptime_seconds: number;\r\n  checks?: Record<string, HealthCheck>;\r\n}\r\n\r\nconst startTime = Date.now();\r\nlet isReady = false;\r\n\r\n// Dependency health checkers\r\nasync function checkDatabase(pool: Pool): Promise<HealthCheck> {\r\n  const start = Date.now();\r\n  try {\r\n    await pool.query('SELECT 1');\r\n    return { status: 'ok', latency_ms: Date.now() - start };\r\n  } catch (error) {\r\n    return { \r\n      status: 'unhealthy', \r\n      error: (error as Error).message,\r\n      latency_ms: Date.now() - start \r\n    };\r\n  }\r\n}\r\n\r\nasync function checkRedis(redis: Redis): Promise<HealthCheck> {\r\n  const start = Date.now();\r\n  try {\r\n    await redis.ping();\r\n    return { status: 'ok', latency_ms: Date.now() - start };\r\n  } catch (error) {\r\n    return { \r\n      status: 'degraded', // Redis might be optional\r\n      error: (error as Error).message,\r\n      latency_ms: Date.now() - start \r\n    };\r\n  }\r\n}\r\n\r\n// Health router\r\nexport function createHealthRouter(pool: Pool, redis: Redis): Router {\r\n  const router = Router();\r\n  \r\n  // LIVENESS - Simple, fast, no external calls\r\n  router.get('/health/live', (req, res) => {\r\n    res.json({ status: 'ok' });\r\n  });\r\n  \r\n  // STARTUP - Check initialization complete\r\n  router.get('/health/startup', (req, res) => {\r\n    if (isReady) {\r\n      res.json({ \r\n        status: 'ok',\r\n        started_at: new Date(startTime).toISOString()\r\n      });\r\n    } else {\r\n      res.status(503).json({ \r\n        status: 'unhealthy',\r\n        message: 'Application still starting'\r\n      });\r\n    }\r\n  });\r\n  \r\n  // READINESS - Check all dependencies\r\n  router.get('/health/ready', async (req, res) => {\r\n    const checks: Record<string, HealthCheck> = {};\r\n    \r\n    // Run checks in parallel with timeout\r\n    const timeout = 3000;\r\n    const checkPromises = [\r\n      Promise.race([\r\n        checkDatabase(pool),\r\n        new Promise<HealthCheck>(resolve => \r\n          setTimeout(() => resolve({ status: 'unhealthy', error: 'Timeout' }), timeout)\r\n        )\r\n      ]).then(result => { checks.database = result; }),\r\n      \r\n      Promise.race([\r\n        checkRedis(redis),\r\n        new Promise<HealthCheck>(resolve => \r\n          setTimeout(() => resolve({ status: 'degraded', error: 'Timeout' }), timeout)\r\n        )\r\n      ]).then(result => { checks.redis = result; }),\r\n    ];\r\n    \r\n    await Promise.all(checkPromises);\r\n    \r\n    // Determine overall status\r\n    const hasUnhealthy = Object.values(checks).some(c => c.status === 'unhealthy');\r\n    const hasDegraded = Object.values(checks).some(c => c.status === 'degraded');\r\n    \r\n    const response: HealthResponse = {\r\n      status: hasUnhealthy ? 'unhealthy' : hasDegraded ? 'degraded' : 'ok',\r\n      version: process.env.APP_VERSION || '1.0.0',\r\n      uptime_seconds: Math.floor((Date.now() - startTime) / 1000),\r\n      checks,\r\n    };\r\n    \r\n    res.status(hasUnhealthy ? 503 : 200).json(response);\r\n  });\r\n  \r\n  return router;\r\n}\r\n\r\n// Mark as ready after initialization\r\nexport function markReady(): void {\r\n  isReady = true;\r\n}\r\n\r\n// Usage\r\napp.use(createHealthRouter(pgPool, redisClient));\r\n\r\n// After all initialization is done\r\nPromise.all([\r\n  runMigrations(),\r\n  warmUpCaches(),\r\n]).then(() => {\r\n  markReady();\r\n  console.log('Application ready');\r\n});\r\n```\r\n\r\n### Python/FastAPI\r\n\r\n```python\r\nfrom fastapi import FastAPI, Response\r\nfrom datetime import datetime, timezone\r\nfrom typing import Dict, Optional, Literal\r\nfrom pydantic import BaseModel\r\nimport asyncio\r\nimport time\r\n\r\napp = FastAPI()\r\nstart_time = time.time()\r\nis_ready = False\r\n\r\nclass HealthCheck(BaseModel):\r\n    status: Literal['ok', 'degraded', 'unhealthy']\r\n    latency_ms: Optional[float] = None\r\n    message: Optional[str] = None\r\n    error: Optional[str] = None\r\n\r\nclass HealthResponse(BaseModel):\r\n    status: Literal['ok', 'degraded', 'unhealthy']\r\n    version: str = \"1.0.0\"\r\n    uptime_seconds: int\r\n    checks: Optional[Dict[str, HealthCheck]] = None\r\n\r\nasync def check_database() -> HealthCheck:\r\n    start = time.time()\r\n    try:\r\n        async with db_pool.acquire() as conn:\r\n            await conn.execute(\"SELECT 1\")\r\n        return HealthCheck(\r\n            status='ok',\r\n            latency_ms=(time.time() - start) * 1000\r\n        )\r\n    except Exception as e:\r\n        return HealthCheck(\r\n            status='unhealthy',\r\n            error=str(e),\r\n            latency_ms=(time.time() - start) * 1000\r\n        )\r\n\r\nasync def check_redis() -> HealthCheck:\r\n    start = time.time()\r\n    try:\r\n        await redis.ping()\r\n        return HealthCheck(\r\n            status='ok',\r\n            latency_ms=(time.time() - start) * 1000\r\n        )\r\n    except Exception as e:\r\n        return HealthCheck(\r\n            status='degraded',  # Redis might be optional\r\n            error=str(e),\r\n            latency_ms=(time.time() - start) * 1000\r\n        )\r\n\r\n@app.get(\"/health/live\")\r\nasync def liveness():\r\n    \"\"\"Liveness probe - just check if process responds\"\"\"\r\n    return {\"status\": \"ok\"}\r\n\r\n@app.get(\"/health/startup\")\r\nasync def startup(response: Response):\r\n    \"\"\"Startup probe - check if app finished initializing\"\"\"\r\n    if is_ready:\r\n        return {\r\n            \"status\": \"ok\",\r\n            \"started_at\": datetime.fromtimestamp(start_time, tz=timezone.utc).isoformat()\r\n        }\r\n    response.status_code = 503\r\n    return {\"status\": \"unhealthy\", \"message\": \"Application still starting\"}\r\n\r\n@app.get(\"/health/ready\", response_model=HealthResponse)\r\nasync def readiness(response: Response):\r\n    \"\"\"Readiness probe - check all dependencies\"\"\"\r\n    \r\n    # Run checks in parallel with timeout\r\n    async def check_with_timeout(coro, timeout=3.0, default_status='unhealthy'):\r\n        try:\r\n            return await asyncio.wait_for(coro, timeout=timeout)\r\n        except asyncio.TimeoutError:\r\n            return HealthCheck(status=default_status, error='Timeout')\r\n    \r\n    db_check, redis_check = await asyncio.gather(\r\n        check_with_timeout(check_database()),\r\n        check_with_timeout(check_redis(), default_status='degraded'),\r\n    )\r\n    \r\n    checks = {\r\n        \"database\": db_check,\r\n        \"redis\": redis_check,\r\n    }\r\n    \r\n    # Determine overall status\r\n    statuses = [c.status for c in checks.values()]\r\n    if 'unhealthy' in statuses:\r\n        overall_status = 'unhealthy'\r\n        response.status_code = 503\r\n    elif 'degraded' in statuses:\r\n        overall_status = 'degraded'\r\n    else:\r\n        overall_status = 'ok'\r\n    \r\n    return HealthResponse(\r\n        status=overall_status,\r\n        uptime_seconds=int(time.time() - start_time),\r\n        checks=checks\r\n    )\r\n\r\ndef mark_ready():\r\n    global is_ready\r\n    is_ready = True\r\n\r\n# Startup event\r\n@app.on_event(\"startup\")\r\nasync def on_startup():\r\n    await run_migrations()\r\n    await warm_caches()\r\n    mark_ready()\r\n```\r\n\r\n### Go\r\n\r\n```go\r\npackage health\r\n\r\nimport (\r\n    \"context\"\r\n    \"encoding/json\"\r\n    \"net/http\"\r\n    \"sync\"\r\n    \"time\"\r\n)\r\n\r\ntype Status string\r\n\r\nconst (\r\n    StatusOK        Status = \"ok\"\r\n    StatusDegraded  Status = \"degraded\"\r\n    StatusUnhealthy Status = \"unhealthy\"\r\n)\r\n\r\ntype Check struct {\r\n    Status    Status  `json:\"status\"`\r\n    LatencyMs float64 `json:\"latency_ms,omitempty\"`\r\n    Message   string  `json:\"message,omitempty\"`\r\n    Error     string  `json:\"error,omitempty\"`\r\n}\r\n\r\ntype Response struct {\r\n    Status        Status           `json:\"status\"`\r\n    Version       string           `json:\"version\"`\r\n    UptimeSeconds int64            `json:\"uptime_seconds\"`\r\n    Checks        map[string]Check `json:\"checks,omitempty\"`\r\n}\r\n\r\ntype Checker func(ctx context.Context) Check\r\n\r\ntype HealthHandler struct {\r\n    startTime time.Time\r\n    isReady   bool\r\n    mu        sync.RWMutex\r\n    checkers  map[string]Checker\r\n    version   string\r\n}\r\n\r\nfunc NewHealthHandler(version string) *HealthHandler {\r\n    return &HealthHandler{\r\n        startTime: time.Now(),\r\n        checkers:  make(map[string]Checker),\r\n        version:   version,\r\n    }\r\n}\r\n\r\nfunc (h *HealthHandler) AddChecker(name string, checker Checker) {\r\n    h.checkers[name] = checker\r\n}\r\n\r\nfunc (h *HealthHandler) SetReady(ready bool) {\r\n    h.mu.Lock()\r\n    defer h.mu.Unlock()\r\n    h.isReady = ready\r\n}\r\n\r\nfunc (h *HealthHandler) LivenessHandler(w http.ResponseWriter, r *http.Request) {\r\n    w.Header().Set(\"Content-Type\", \"application/json\")\r\n    json.NewEncoder(w).Encode(map[string]string{\"status\": \"ok\"})\r\n}\r\n\r\nfunc (h *HealthHandler) StartupHandler(w http.ResponseWriter, r *http.Request) {\r\n    h.mu.RLock()\r\n    ready := h.isReady\r\n    h.mu.RUnlock()\r\n\r\n    w.Header().Set(\"Content-Type\", \"application/json\")\r\n    if ready {\r\n        json.NewEncoder(w).Encode(map[string]interface{}{\r\n            \"status\":     \"ok\",\r\n            \"started_at\": h.startTime.Format(time.RFC3339),\r\n        })\r\n    } else {\r\n        w.WriteHeader(http.StatusServiceUnavailable)\r\n        json.NewEncoder(w).Encode(map[string]string{\r\n            \"status\":  \"unhealthy\",\r\n            \"message\": \"Application still starting\",\r\n        })\r\n    }\r\n}\r\n\r\nfunc (h *HealthHandler) ReadinessHandler(w http.ResponseWriter, r *http.Request) {\r\n    ctx, cancel := context.WithTimeout(r.Context(), 3*time.Second)\r\n    defer cancel()\r\n\r\n    checks := make(map[string]Check)\r\n    var wg sync.WaitGroup\r\n    var mu sync.Mutex\r\n\r\n    for name, checker := range h.checkers {\r\n        wg.Add(1)\r\n        go func(name string, checker Checker) {\r\n            defer wg.Done()\r\n            check := checker(ctx)\r\n            mu.Lock()\r\n            checks[name] = check\r\n            mu.Unlock()\r\n        }(name, checker)\r\n    }\r\n    wg.Wait()\r\n\r\n    // Determine overall status\r\n    overallStatus := StatusOK\r\n    for _, check := range checks {\r\n        if check.Status == StatusUnhealthy {\r\n            overallStatus = StatusUnhealthy\r\n            break\r\n        }\r\n        if check.Status == StatusDegraded {\r\n            overallStatus = StatusDegraded\r\n        }\r\n    }\r\n\r\n    response := Response{\r\n        Status:        overallStatus,\r\n        Version:       h.version,\r\n        UptimeSeconds: int64(time.Since(h.startTime).Seconds()),\r\n        Checks:        checks,\r\n    }\r\n\r\n    w.Header().Set(\"Content-Type\", \"application/json\")\r\n    if overallStatus == StatusUnhealthy {\r\n        w.WriteHeader(http.StatusServiceUnavailable)\r\n    }\r\n    json.NewEncoder(w).Encode(response)\r\n}\r\n\r\n// Database checker example\r\nfunc DatabaseChecker(db *sql.DB) Checker {\r\n    return func(ctx context.Context) Check {\r\n        start := time.Now()\r\n        err := db.PingContext(ctx)\r\n        latency := float64(time.Since(start).Milliseconds())\r\n        \r\n        if err != nil {\r\n            return Check{\r\n                Status:    StatusUnhealthy,\r\n                LatencyMs: latency,\r\n                Error:     err.Error(),\r\n            }\r\n        }\r\n        return Check{Status: StatusOK, LatencyMs: latency}\r\n    }\r\n}\r\n\r\n// Usage\r\nfunc main() {\r\n    health := NewHealthHandler(\"1.0.0\")\r\n    health.AddChecker(\"database\", DatabaseChecker(db))\r\n    health.AddChecker(\"redis\", RedisChecker(redis))\r\n    \r\n    http.HandleFunc(\"/health/live\", health.LivenessHandler)\r\n    http.HandleFunc(\"/health/startup\", health.StartupHandler)\r\n    http.HandleFunc(\"/health/ready\", health.ReadinessHandler)\r\n    \r\n    // After initialization\r\n    go func() {\r\n        runMigrations()\r\n        warmCaches()\r\n        health.SetReady(true)\r\n    }()\r\n    \r\n    http.ListenAndServe(\":8080\", nil)\r\n}\r\n```\r\n\r\n## References\r\n\r\n- [Kubernetes - Configure Liveness, Readiness and Startup Probes](https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/)\r\n- [AWS - Implementing Health Checks](https://aws.amazon.com/builders-library/implementing-health-checks/)\r\n- [Microsoft - Health Endpoint Monitoring Pattern](https://docs.microsoft.com/en-us/azure/architecture/patterns/health-endpoint-monitoring)\r\n- [Google SRE - Monitoring Distributed Systems](https://sre.google/sre-book/monitoring-distributed-systems/)\r\n"
  },
  {
    "id": "rel-load-shedding",
    "title": "Load Shedding and Graceful Degradation",
    "tags": [
      "reliability",
      "load-shedding",
      "graceful-degradation",
      "resilience",
      "overload"
    ],
    "level": "advanced",
    "stacks": [
      "nodejs",
      "python",
      "go"
    ],
    "scope": "reliability",
    "maturity": "stable",
    "works_with": [
      "nodejs",
      "python",
      "go"
    ],
    "path": "D:/Code/backend-engineering-kit/.shared/production-backend-kit/patterns/rel.load-shedding.md",
    "type": "pattern",
    "sections": [],
    "checklist": [],
    "sources": [],
    "rawContent": "\r\n# Load Shedding and Graceful Degradation\r\n\r\n## Problem\r\n\r\nWhen a service is overloaded:\r\n- **Without load shedding**: All requests slow down, timeouts cascade, system crashes\r\n- **With load shedding**: Some requests rejected fast, others served normally\r\n\r\n```\r\nTraffic Spike (3x capacity)\r\n    │\r\n    ▼\r\n┌─────────────────────────────────────┐\r\n│ Without Load Shedding               │\r\n│                                     │\r\n│  Request 1 ────────────────→ Slow   │\r\n│  Request 2 ────────────────→ Slow   │\r\n│  Request 3 ────────────────→ Slow   │\r\n│  Request 4 ────────────────→ Timeout│\r\n│  Request 5 ────────────────→ Timeout│\r\n│                                     │\r\n│  → 0% served at acceptable latency  │\r\n└─────────────────────────────────────┘\r\n\r\n┌─────────────────────────────────────┐\r\n│ With Load Shedding                  │\r\n│                                     │\r\n│  Request 1 ───────→ OK (fast)       │\r\n│  Request 2 → 503 (shed immediately) │\r\n│  Request 3 ───────→ OK (fast)       │\r\n│  Request 4 → 503 (shed immediately) │\r\n│  Request 5 ───────→ OK (fast)       │\r\n│                                     │\r\n│  → 60% served at acceptable latency │\r\n└─────────────────────────────────────┘\r\n```\r\n\r\n## When to use\r\n\r\n- Services with variable traffic patterns\r\n- During known traffic spikes (sales, events)\r\n- Dependencies become slow/unavailable\r\n- Protecting critical paths from non-critical traffic\r\n- Multi-tenant systems with noisy neighbors\r\n\r\n## Solution\r\n\r\n### 1. Queue-Based Load Shedding\r\n\r\n```typescript\r\nimport { EventEmitter } from 'events';\r\n\r\ninterface Request {\r\n  id: string;\r\n  priority: 'critical' | 'high' | 'normal' | 'low';\r\n  enqueuedAt: number;\r\n  deadline: number;\r\n  handler: () => Promise<any>;\r\n  resolve: (value: any) => void;\r\n  reject: (error: Error) => void;\r\n}\r\n\r\nclass LoadSheddingQueue extends EventEmitter {\r\n  private queues: Map<string, Request[]> = new Map([\r\n    ['critical', []],\r\n    ['high', []],\r\n    ['normal', []],\r\n    ['low', []],\r\n  ]);\r\n  \r\n  private processing = 0;\r\n  private config = {\r\n    maxConcurrent: 100,\r\n    maxQueueSize: 1000,\r\n    maxWaitTime: 5000,  // Max time in queue before shedding\r\n    shedLowPriorityAt: 0.7,   // Start shedding 'low' at 70% capacity\r\n    shedNormalPriorityAt: 0.9, // Start shedding 'normal' at 90% capacity\r\n  };\r\n\r\n  async enqueue<T>(\r\n    handler: () => Promise<T>,\r\n    options: { priority?: Request['priority']; timeout?: number } = {}\r\n  ): Promise<T> {\r\n    const { priority = 'normal', timeout = 30000 } = options;\r\n    \r\n    // Check if we should shed this request immediately\r\n    if (this.shouldShed(priority)) {\r\n      metrics.increment('load_shedding.shed', { priority, reason: 'capacity' });\r\n      throw new LoadSheddedError('Service at capacity');\r\n    }\r\n    \r\n    // Check queue size\r\n    const totalQueued = this.getTotalQueued();\r\n    if (totalQueued >= this.config.maxQueueSize) {\r\n      metrics.increment('load_shedding.shed', { priority, reason: 'queue_full' });\r\n      throw new LoadSheddedError('Queue full');\r\n    }\r\n    \r\n    return new Promise((resolve, reject) => {\r\n      const request: Request = {\r\n        id: crypto.randomUUID(),\r\n        priority,\r\n        enqueuedAt: Date.now(),\r\n        deadline: Date.now() + timeout,\r\n        handler,\r\n        resolve,\r\n        reject,\r\n      };\r\n      \r\n      this.queues.get(priority)!.push(request);\r\n      metrics.gauge('load_shedding.queue_size', this.getTotalQueued());\r\n      \r\n      this.processNext();\r\n    });\r\n  }\r\n\r\n  private shouldShed(priority: Request['priority']): boolean {\r\n    const utilization = this.processing / this.config.maxConcurrent;\r\n    \r\n    switch (priority) {\r\n      case 'critical':\r\n        return false;  // Never shed critical\r\n      case 'high':\r\n        return utilization >= 1.0;  // Only shed when fully loaded\r\n      case 'normal':\r\n        return utilization >= this.config.shedNormalPriorityAt;\r\n      case 'low':\r\n        return utilization >= this.config.shedLowPriorityAt;\r\n    }\r\n  }\r\n\r\n  private async processNext() {\r\n    if (this.processing >= this.config.maxConcurrent) {\r\n      return;\r\n    }\r\n    \r\n    const request = this.dequeueNext();\r\n    if (!request) {\r\n      return;\r\n    }\r\n    \r\n    // Check if request has expired in queue\r\n    if (Date.now() > request.deadline) {\r\n      metrics.increment('load_shedding.shed', { \r\n        priority: request.priority, \r\n        reason: 'deadline_exceeded' \r\n      });\r\n      request.reject(new LoadSheddedError('Request deadline exceeded'));\r\n      this.processNext();\r\n      return;\r\n    }\r\n    \r\n    // Check if request waited too long\r\n    const waitTime = Date.now() - request.enqueuedAt;\r\n    if (waitTime > this.config.maxWaitTime) {\r\n      metrics.increment('load_shedding.shed', { \r\n        priority: request.priority, \r\n        reason: 'wait_timeout' \r\n      });\r\n      request.reject(new LoadSheddedError('Wait time exceeded'));\r\n      this.processNext();\r\n      return;\r\n    }\r\n    \r\n    this.processing++;\r\n    metrics.gauge('load_shedding.processing', this.processing);\r\n    \r\n    try {\r\n      const result = await request.handler();\r\n      request.resolve(result);\r\n    } catch (error) {\r\n      request.reject(error as Error);\r\n    } finally {\r\n      this.processing--;\r\n      metrics.gauge('load_shedding.processing', this.processing);\r\n      this.processNext();\r\n    }\r\n  }\r\n\r\n  private dequeueNext(): Request | null {\r\n    // Process in priority order: critical → high → normal → low\r\n    for (const priority of ['critical', 'high', 'normal', 'low']) {\r\n      const queue = this.queues.get(priority)!;\r\n      if (queue.length > 0) {\r\n        return queue.shift()!;\r\n      }\r\n    }\r\n    return null;\r\n  }\r\n\r\n  private getTotalQueued(): number {\r\n    let total = 0;\r\n    for (const queue of this.queues.values()) {\r\n      total += queue.length;\r\n    }\r\n    return total;\r\n  }\r\n}\r\n\r\nclass LoadSheddedError extends Error {\r\n  constructor(message: string) {\r\n    super(message);\r\n    this.name = 'LoadSheddedError';\r\n  }\r\n}\r\n```\r\n\r\n### 2. Adaptive Concurrency Limits\r\n\r\n```typescript\r\n// Based on Netflix's adaptive concurrency limiter\r\n// Dynamically adjusts limits based on latency\r\n\r\nclass AdaptiveConcurrencyLimiter {\r\n  private limit: number;\r\n  private inFlight = 0;\r\n  private minLimit: number;\r\n  private maxLimit: number;\r\n  private latencyTarget: number;  // Target latency in ms\r\n  \r\n  // Exponential moving average of latency\r\n  private avgLatency = 0;\r\n  private latencyAlpha = 0.1;\r\n  \r\n  // Gradient tracking\r\n  private rttNoLoad = Infinity;  // Min observed latency (estimate of latency without queueing)\r\n\r\n  constructor(options: {\r\n    initialLimit?: number;\r\n    minLimit?: number;\r\n    maxLimit?: number;\r\n    latencyTarget?: number;\r\n  } = {}) {\r\n    this.limit = options.initialLimit || 20;\r\n    this.minLimit = options.minLimit || 5;\r\n    this.maxLimit = options.maxLimit || 200;\r\n    this.latencyTarget = options.latencyTarget || 100;\r\n  }\r\n\r\n  async execute<T>(fn: () => Promise<T>): Promise<T> {\r\n    if (this.inFlight >= this.limit) {\r\n      metrics.increment('adaptive_limiter.rejected');\r\n      throw new LoadSheddedError('Concurrency limit reached');\r\n    }\r\n    \r\n    this.inFlight++;\r\n    const startTime = Date.now();\r\n    \r\n    try {\r\n      const result = await fn();\r\n      this.onSuccess(Date.now() - startTime);\r\n      return result;\r\n    } catch (error) {\r\n      this.onError();\r\n      throw error;\r\n    } finally {\r\n      this.inFlight--;\r\n    }\r\n  }\r\n\r\n  private onSuccess(latency: number) {\r\n    // Update RTT no-load estimate\r\n    this.rttNoLoad = Math.min(this.rttNoLoad, latency);\r\n    \r\n    // Update average latency (EMA)\r\n    this.avgLatency = this.latencyAlpha * latency + (1 - this.latencyAlpha) * this.avgLatency;\r\n    \r\n    // Gradient: how much latency increased due to queueing\r\n    const gradient = Math.max(1, this.rttNoLoad / this.avgLatency);\r\n    \r\n    // New limit based on Little's Law adjusted for gradient\r\n    // Limit ≈ gradient * inFlight\r\n    const newLimit = Math.floor(gradient * this.inFlight);\r\n    \r\n    // Smooth adjustment\r\n    if (this.avgLatency < this.latencyTarget) {\r\n      // Under target - can increase limit\r\n      this.limit = Math.min(this.maxLimit, Math.max(this.limit, newLimit) + 1);\r\n    } else if (this.avgLatency > this.latencyTarget * 2) {\r\n      // Way over target - decrease aggressively\r\n      this.limit = Math.max(this.minLimit, Math.floor(this.limit * 0.8));\r\n    } else if (this.avgLatency > this.latencyTarget) {\r\n      // Over target - decrease gradually\r\n      this.limit = Math.max(this.minLimit, this.limit - 1);\r\n    }\r\n    \r\n    metrics.gauge('adaptive_limiter.limit', this.limit);\r\n    metrics.gauge('adaptive_limiter.latency_avg', this.avgLatency);\r\n  }\r\n\r\n  private onError() {\r\n    // Reduce limit on errors\r\n    this.limit = Math.max(this.minLimit, Math.floor(this.limit * 0.9));\r\n    metrics.gauge('adaptive_limiter.limit', this.limit);\r\n  }\r\n\r\n  getStats() {\r\n    return {\r\n      limit: this.limit,\r\n      inFlight: this.inFlight,\r\n      avgLatency: this.avgLatency,\r\n      rttNoLoad: this.rttNoLoad,\r\n    };\r\n  }\r\n}\r\n```\r\n\r\n### 3. Request Priority Classification\r\n\r\n```typescript\r\n// Classify requests by importance for shedding decisions\r\n\r\nenum RequestPriority {\r\n  CRITICAL = 4,  // Health checks, auth, payment completion\r\n  HIGH = 3,      // User-facing primary flows\r\n  NORMAL = 2,    // Standard requests\r\n  LOW = 1,       // Background tasks, prefetch, analytics\r\n  SHED = 0,      // Always shed under load\r\n}\r\n\r\ninterface PriorityConfig {\r\n  path: string;\r\n  method?: string;\r\n  priority: RequestPriority;\r\n  canDegrade?: boolean;  // Can serve degraded response\r\n}\r\n\r\nconst priorityConfig: PriorityConfig[] = [\r\n  // Critical - never shed\r\n  { path: '/health', priority: RequestPriority.CRITICAL },\r\n  { path: '/auth/token', method: 'POST', priority: RequestPriority.CRITICAL },\r\n  { path: '/payments/complete', priority: RequestPriority.CRITICAL },\r\n  \r\n  // High - only shed under extreme load\r\n  { path: '/api/users/*', method: 'GET', priority: RequestPriority.HIGH },\r\n  { path: '/api/orders', method: 'POST', priority: RequestPriority.HIGH },\r\n  \r\n  // Normal - standard shedding\r\n  { path: '/api/products', priority: RequestPriority.NORMAL, canDegrade: true },\r\n  { path: '/api/search', priority: RequestPriority.NORMAL, canDegrade: true },\r\n  \r\n  // Low - shed first\r\n  { path: '/api/recommendations', priority: RequestPriority.LOW, canDegrade: true },\r\n  { path: '/api/analytics', priority: RequestPriority.LOW },\r\n  \r\n  // Always shed under any load\r\n  { path: '/api/export', priority: RequestPriority.SHED },\r\n];\r\n\r\nfunction classifyRequest(req: Request): { priority: RequestPriority; canDegrade: boolean } {\r\n  for (const config of priorityConfig) {\r\n    if (matchPath(req.path, config.path)) {\r\n      if (!config.method || config.method === req.method) {\r\n        return {\r\n          priority: config.priority,\r\n          canDegrade: config.canDegrade ?? false,\r\n        };\r\n      }\r\n    }\r\n  }\r\n  \r\n  // Default: normal priority, no degradation\r\n  return { priority: RequestPriority.NORMAL, canDegrade: false };\r\n}\r\n\r\nfunction matchPath(path: string, pattern: string): boolean {\r\n  const regex = pattern\r\n    .replace(/\\*/g, '.*')\r\n    .replace(/\\//g, '\\\\/');\r\n  return new RegExp(`^${regex}$`).test(path);\r\n}\r\n```\r\n\r\n### 4. Graceful Degradation Strategies\r\n\r\n```typescript\r\n// Serve degraded responses instead of errors\r\n\r\ninterface DegradedResponse<T> {\r\n  data: T;\r\n  degraded: boolean;\r\n  degradationReason?: string;\r\n}\r\n\r\nclass GracefulDegradation {\r\n  private cache: Map<string, { data: any; timestamp: number }> = new Map();\r\n  private cacheMaxAge = 300000;  // 5 minutes\r\n\r\n  // Strategy 1: Return cached data\r\n  async withCacheFallback<T>(\r\n    key: string,\r\n    freshDataFn: () => Promise<T>,\r\n    options: { maxAge?: number } = {}\r\n  ): Promise<DegradedResponse<T>> {\r\n    try {\r\n      const data = await freshDataFn();\r\n      this.cache.set(key, { data, timestamp: Date.now() });\r\n      return { data, degraded: false };\r\n    } catch (error) {\r\n      const cached = this.cache.get(key);\r\n      if (cached && Date.now() - cached.timestamp < (options.maxAge || this.cacheMaxAge)) {\r\n        metrics.increment('degradation.cache_fallback');\r\n        return {\r\n          data: cached.data,\r\n          degraded: true,\r\n          degradationReason: 'Serving cached data',\r\n        };\r\n      }\r\n      throw error;\r\n    }\r\n  }\r\n\r\n  // Strategy 2: Return partial data\r\n  async withPartialData<T>(\r\n    dataFetchers: { key: string; fn: () => Promise<any>; required: boolean }[]\r\n  ): Promise<DegradedResponse<Partial<T>>> {\r\n    const results: Partial<T> = {};\r\n    let degraded = false;\r\n    const errors: string[] = [];\r\n\r\n    await Promise.all(\r\n      dataFetchers.map(async ({ key, fn, required }) => {\r\n        try {\r\n          results[key as keyof T] = await fn();\r\n        } catch (error) {\r\n          if (required) {\r\n            throw error;  // Critical data - fail the request\r\n          }\r\n          degraded = true;\r\n          errors.push(key);\r\n          metrics.increment('degradation.partial_data', { field: key });\r\n        }\r\n      })\r\n    );\r\n\r\n    return {\r\n      data: results,\r\n      degraded,\r\n      degradationReason: degraded ? `Missing: ${errors.join(', ')}` : undefined,\r\n    };\r\n  }\r\n\r\n  // Strategy 3: Simplified response\r\n  async withSimplifiedResponse<T>(\r\n    fullFn: () => Promise<T>,\r\n    simpleFn: () => Promise<Partial<T>>\r\n  ): Promise<DegradedResponse<T | Partial<T>>> {\r\n    try {\r\n      const data = await fullFn();\r\n      return { data, degraded: false };\r\n    } catch (error) {\r\n      metrics.increment('degradation.simplified_response');\r\n      const data = await simpleFn();\r\n      return {\r\n        data,\r\n        degraded: true,\r\n        degradationReason: 'Serving simplified response',\r\n      };\r\n    }\r\n  }\r\n\r\n  // Strategy 4: Static fallback\r\n  async withStaticFallback<T>(\r\n    fn: () => Promise<T>,\r\n    fallback: T\r\n  ): Promise<DegradedResponse<T>> {\r\n    try {\r\n      const data = await fn();\r\n      return { data, degraded: false };\r\n    } catch (error) {\r\n      metrics.increment('degradation.static_fallback');\r\n      return {\r\n        data: fallback,\r\n        degraded: true,\r\n        degradationReason: 'Serving static fallback',\r\n      };\r\n    }\r\n  }\r\n}\r\n\r\n// Example: Product page with graceful degradation\r\napp.get('/api/products/:id', async (req, res) => {\r\n  const degradation = new GracefulDegradation();\r\n  const productId = req.params.id;\r\n\r\n  const result = await degradation.withPartialData<ProductResponse>([\r\n    // Required - will fail request if unavailable\r\n    {\r\n      key: 'product',\r\n      fn: () => productService.getProduct(productId),\r\n      required: true,\r\n    },\r\n    // Optional - will degrade gracefully\r\n    {\r\n      key: 'reviews',\r\n      fn: () => reviewService.getReviews(productId),\r\n      required: false,\r\n    },\r\n    {\r\n      key: 'recommendations',\r\n      fn: () => recommendationService.getRecommendations(productId),\r\n      required: false,\r\n    },\r\n    {\r\n      key: 'inventory',\r\n      fn: () => inventoryService.getStock(productId),\r\n      required: false,\r\n    },\r\n  ]);\r\n\r\n  // Set header to indicate degradation\r\n  if (result.degraded) {\r\n    res.setHeader('X-Degraded', 'true');\r\n    res.setHeader('X-Degradation-Reason', result.degradationReason!);\r\n  }\r\n\r\n  res.json(result.data);\r\n});\r\n```\r\n\r\n### 5. Load Shedding Middleware\r\n\r\n```typescript\r\nimport { Request, Response, NextFunction } from 'express';\r\n\r\nclass LoadSheddingMiddleware {\r\n  private limiter: AdaptiveConcurrencyLimiter;\r\n  private queue: LoadSheddingQueue;\r\n  private degradation: GracefulDegradation;\r\n  private currentLoad = 0;\r\n  private loadThreshold = 0.8;  // Start shedding at 80% load\r\n\r\n  constructor() {\r\n    this.limiter = new AdaptiveConcurrencyLimiter({\r\n      initialLimit: 100,\r\n      latencyTarget: 200,\r\n    });\r\n    this.queue = new LoadSheddingQueue();\r\n    this.degradation = new GracefulDegradation();\r\n  }\r\n\r\n  middleware() {\r\n    return async (req: Request, res: Response, next: NextFunction) => {\r\n      const { priority, canDegrade } = classifyRequest(req);\r\n      \r\n      // Update load metrics\r\n      this.currentLoad = this.limiter.getStats().inFlight / this.limiter.getStats().limit;\r\n      metrics.gauge('load_shedding.load_factor', this.currentLoad);\r\n      \r\n      // Check if we should shed this request\r\n      if (this.shouldShed(priority)) {\r\n        metrics.increment('load_shedding.rejected', {\r\n          path: req.path,\r\n          priority: RequestPriority[priority],\r\n        });\r\n        \r\n        // Return 503 with Retry-After\r\n        res.setHeader('Retry-After', '5');\r\n        return res.status(503).json({\r\n          error: 'Service temporarily unavailable',\r\n          code: 'LOAD_SHEDDING',\r\n          retryAfter: 5,\r\n        });\r\n      }\r\n      \r\n      // Check if we should degrade\r\n      if (canDegrade && this.currentLoad > 0.6) {\r\n        req.headers['x-degrade-response'] = 'true';\r\n      }\r\n      \r\n      // Process with concurrency limiter\r\n      try {\r\n        await this.limiter.execute(async () => {\r\n          return new Promise<void>((resolve, reject) => {\r\n            res.on('finish', resolve);\r\n            res.on('error', reject);\r\n            next();\r\n          });\r\n        });\r\n      } catch (error) {\r\n        if (error instanceof LoadSheddedError) {\r\n          res.setHeader('Retry-After', '5');\r\n          return res.status(503).json({\r\n            error: 'Service temporarily unavailable',\r\n            code: 'LOAD_SHEDDING',\r\n          });\r\n        }\r\n        next(error);\r\n      }\r\n    };\r\n  }\r\n\r\n  private shouldShed(priority: RequestPriority): boolean {\r\n    if (priority === RequestPriority.CRITICAL) return false;\r\n    if (priority === RequestPriority.SHED) return true;\r\n    \r\n    const thresholds = {\r\n      [RequestPriority.HIGH]: 0.95,\r\n      [RequestPriority.NORMAL]: 0.85,\r\n      [RequestPriority.LOW]: 0.70,\r\n    };\r\n    \r\n    return this.currentLoad > thresholds[priority];\r\n  }\r\n}\r\n\r\n// Usage\r\nconst loadShedding = new LoadSheddingMiddleware();\r\napp.use(loadShedding.middleware());\r\n```\r\n\r\n### 6. Feature Flags for Degradation\r\n\r\n```typescript\r\n// Runtime toggles for degradation strategies\r\n\r\ninterface FeatureFlags {\r\n  recommendationsEnabled: boolean;\r\n  searchEnabled: boolean;\r\n  cacheOnlyMode: boolean;\r\n  simplifiedResponses: boolean;\r\n  readOnlyMode: boolean;\r\n}\r\n\r\nclass DegradationFlags {\r\n  private flags: FeatureFlags = {\r\n    recommendationsEnabled: true,\r\n    searchEnabled: true,\r\n    cacheOnlyMode: false,\r\n    simplifiedResponses: false,\r\n    readOnlyMode: false,\r\n  };\r\n  \r\n  private listeners: ((flags: FeatureFlags) => void)[] = [];\r\n\r\n  // Called by ops/automation during incidents\r\n  enableCacheOnlyMode() {\r\n    this.flags.cacheOnlyMode = true;\r\n    this.notifyListeners();\r\n    logger.warn('Degradation: Cache-only mode enabled');\r\n  }\r\n\r\n  enableReadOnlyMode() {\r\n    this.flags.readOnlyMode = true;\r\n    this.notifyListeners();\r\n    logger.warn('Degradation: Read-only mode enabled');\r\n  }\r\n\r\n  disableNonEssential() {\r\n    this.flags.recommendationsEnabled = false;\r\n    this.flags.searchEnabled = false;\r\n    this.flags.simplifiedResponses = true;\r\n    this.notifyListeners();\r\n    logger.warn('Degradation: Non-essential features disabled');\r\n  }\r\n\r\n  restoreNormal() {\r\n    this.flags = {\r\n      recommendationsEnabled: true,\r\n      searchEnabled: true,\r\n      cacheOnlyMode: false,\r\n      simplifiedResponses: false,\r\n      readOnlyMode: false,\r\n    };\r\n    this.notifyListeners();\r\n    logger.info('Degradation: Normal mode restored');\r\n  }\r\n\r\n  getFlags(): Readonly<FeatureFlags> {\r\n    return { ...this.flags };\r\n  }\r\n\r\n  subscribe(listener: (flags: FeatureFlags) => void) {\r\n    this.listeners.push(listener);\r\n  }\r\n\r\n  private notifyListeners() {\r\n    for (const listener of this.listeners) {\r\n      listener(this.flags);\r\n    }\r\n  }\r\n}\r\n\r\n// Usage in route handlers\r\nconst degradationFlags = new DegradationFlags();\r\n\r\napp.post('/api/orders', async (req, res) => {\r\n  const flags = degradationFlags.getFlags();\r\n  \r\n  if (flags.readOnlyMode) {\r\n    return res.status(503).json({\r\n      error: 'Service in read-only mode',\r\n      code: 'READ_ONLY_MODE',\r\n    });\r\n  }\r\n  \r\n  // Process order...\r\n});\r\n\r\napp.get('/api/products/:id', async (req, res) => {\r\n  const flags = degradationFlags.getFlags();\r\n  \r\n  const product = await productService.getProduct(req.params.id);\r\n  \r\n  const response: any = { product };\r\n  \r\n  if (flags.recommendationsEnabled && !flags.simplifiedResponses) {\r\n    try {\r\n      response.recommendations = await recommendationService.get(req.params.id);\r\n    } catch {\r\n      // Ignore - gracefully degrade\r\n    }\r\n  }\r\n  \r\n  res.json(response);\r\n});\r\n\r\n// Ops API for toggling degradation\r\napp.post('/ops/degradation/:mode', authOps, async (req, res) => {\r\n  switch (req.params.mode) {\r\n    case 'cache-only':\r\n      degradationFlags.enableCacheOnlyMode();\r\n      break;\r\n    case 'read-only':\r\n      degradationFlags.enableReadOnlyMode();\r\n      break;\r\n    case 'minimal':\r\n      degradationFlags.disableNonEssential();\r\n      break;\r\n    case 'normal':\r\n      degradationFlags.restoreNormal();\r\n      break;\r\n    default:\r\n      return res.status(400).json({ error: 'Unknown mode' });\r\n  }\r\n  \r\n  res.json({ status: 'ok', flags: degradationFlags.getFlags() });\r\n});\r\n```\r\n\r\n## Pitfalls\r\n\r\n| Pitfall | Impact | How to Avoid |\r\n|---------|--------|--------------|\r\n| Shedding critical requests | Breaking essential flows | Classify requests by priority |\r\n| No retry guidance | Clients retry immediately | Return Retry-After header |\r\n| All-or-nothing shedding | Poor UX | Implement partial degradation |\r\n| Static limits | Poor adaptation | Use adaptive concurrency limits |\r\n| No observability | Can't tune | Track shed rate, latency, capacity |\r\n| Shedding at wrong layer | Ineffective | Shed as early as possible (LB/gateway) |\r\n| No manual controls | Can't react to incidents | Add ops API for degradation flags |\r\n\r\n## Checklist\r\n\r\n- [ ] Request priority classification defined\r\n- [ ] Load shedding middleware in place\r\n- [ ] Adaptive concurrency limits\r\n- [ ] Graceful degradation for non-critical data\r\n- [ ] 503 responses include Retry-After header\r\n- [ ] Metrics for shed rate, latency, capacity\r\n- [ ] Manual degradation toggles for ops\r\n- [ ] Load testing to validate behavior\r\n- [ ] Alerting on high shed rates\r\n- [ ] Documentation for priority levels\r\n\r\n## References\r\n\r\n- [Google SRE: Handling Overload](https://sre.google/sre-book/handling-overload/)\r\n- [AWS: Using Load Shedding](https://aws.amazon.com/builders-library/using-load-shedding-to-avoid-overload/)\r\n- [Netflix: Performance Under Load](https://netflixtechblog.medium.com/performance-under-load-3e6fa9a60581)\r\n- [Cloudflare: Graceful Degradation](https://blog.cloudflare.com/the-problem-with-graceful-degradation/)\r\n"
  },
  {
    "id": "rel-outbox-pattern",
    "title": "Outbox Pattern",
    "tags": [
      "reliability",
      "outbox",
      "eventual-consistency",
      "distributed"
    ],
    "level": "advanced",
    "stacks": [
      "all"
    ],
    "scope": "reliability",
    "maturity": "stable",
    "works_with": [
      "all"
    ],
    "path": "D:/Code/backend-engineering-kit/.shared/production-backend-kit/patterns/rel.outbox-pattern.md",
    "type": "pattern",
    "sections": [],
    "checklist": [],
    "sources": [],
    "rawContent": "\r\n# Outbox Pattern\r\n\r\n## Problem\r\n\r\nIn distributed systems, you often need to update a database AND publish an event atomically. Without coordination, you risk data inconsistency—either the event is lost or published without the DB change.\r\n\r\n## When to use\r\n\r\n- Microservices event-driven architecture\r\n- Database change + message publish\r\n- Ensuring eventual consistency\r\n- Reliable event delivery\r\n- Avoiding dual-write problem\r\n\r\n## Solution\r\n\r\n1. **Write event to outbox table**\r\n   - Same transaction as business data\r\n   - Store event payload, status, created_at\r\n   - Guarantees atomicity\r\n\r\n2. **Publish from outbox**\r\n   - Background worker reads pending events\r\n   - Publishes to message broker\r\n   - Marks as processed\r\n\r\n3. **Handle failures**\r\n   - Retry failed publications\r\n   - Dead letter after max retries\r\n   - Idempotent consumers required\r\n\r\n4. **Cleanup processed events**\r\n   - Archive or delete old events\r\n   - Prevent table bloat\r\n\r\n## Pitfalls\r\n\r\n| Pitfall | How to Avoid |\r\n|---------|--------------|\r\n| Dual-write (DB + broker separately) | Always use transactional outbox |\r\n| Consumer not idempotent | Design consumers for at-least-once |\r\n| Outbox table grows unbounded | Regular cleanup/archival |\r\n| Publisher not fault-tolerant | Retry with backoff, use DLQ |\r\n| Wrong event ordering | Order by created_at, use sequence IDs |\r\n\r\n## Checklist\r\n\r\n- [ ] Outbox table in same database as business data\r\n- [ ] Event written in same transaction as business change\r\n- [ ] Background worker publishes pending events\r\n- [ ] Published events marked as processed\r\n- [ ] Failed events retried with backoff\r\n- [ ] Dead letter queue for failed events\r\n- [ ] Consumers are idempotent\r\n- [ ] Cleanup job removes old events\r\n- [ ] Outbox processing monitored\r\n- [ ] Event ordering preserved\r\n\r\n## Snippets (Generic)\r\n\r\n```\r\nOutbox Table Schema:\r\nCREATE TABLE outbox (\r\n  id UUID PRIMARY KEY,\r\n  aggregate_type VARCHAR(100) NOT NULL,\r\n  aggregate_id UUID NOT NULL,\r\n  event_type VARCHAR(100) NOT NULL,\r\n  payload JSONB NOT NULL,\r\n  status VARCHAR(20) DEFAULT 'pending',\r\n  retry_count INT DEFAULT 0,\r\n  created_at TIMESTAMPTZ DEFAULT NOW(),\r\n  processed_at TIMESTAMPTZ NULL\r\n);\r\n\r\nCREATE INDEX idx_outbox_pending ON outbox(created_at) \r\n  WHERE status = 'pending';\r\n\r\nBusiness Transaction:\r\nBEGIN;\r\n  -- Update business data\r\n  UPDATE orders SET status = 'paid' WHERE id = 123;\r\n  \r\n  -- Write event to outbox (same transaction)\r\n  INSERT INTO outbox (aggregate_type, aggregate_id, event_type, payload)\r\n  VALUES ('order', 123, 'OrderPaid', '{\"orderId\": 123, \"amount\": 100}');\r\nCOMMIT;\r\n\r\nOutbox Publisher (Background Worker):\r\nwhile true:\r\n  events = db.query(\"SELECT * FROM outbox WHERE status = 'pending' \r\n                     ORDER BY created_at LIMIT 100\")\r\n  \r\n  for event in events:\r\n    try:\r\n      broker.publish(event.event_type, event.payload)\r\n      db.update(\"UPDATE outbox SET status = 'processed', \r\n                 processed_at = NOW() WHERE id = ?\", event.id)\r\n    except PublishError:\r\n      db.update(\"UPDATE outbox SET retry_count = retry_count + 1 \r\n                 WHERE id = ?\", event.id)\r\n      if event.retry_count >= MAX_RETRIES:\r\n        move_to_dlq(event)\r\n  \r\n  sleep(poll_interval)\r\n```\r\n\r\n## Sources\r\n\r\n- Microservices Patterns (Chris Richardson) - Outbox: https://microservices.io/patterns/data/transactional-outbox.html\r\n- Debezium Outbox Pattern: https://debezium.io/documentation/reference/transformations/outbox-event-router.html\r\n- AWS Building Event-Driven Architectures: https://aws.amazon.com/event-driven-architecture/\r\n- Martin Kleppmann - Designing Data-Intensive Applications: https://dataintensive.net/\r\n"
  },
  {
    "id": "rel-retries-backoff",
    "title": "Retries with Exponential Backoff",
    "tags": [
      "reliability",
      "retries",
      "backoff",
      "resilience",
      "jitter"
    ],
    "level": "intermediate",
    "stacks": [
      "all"
    ],
    "scope": "reliability",
    "maturity": "stable",
    "works_with": [
      "all"
    ],
    "path": "D:/Code/backend-engineering-kit/.shared/production-backend-kit/patterns/rel.retries-backoff.md",
    "type": "pattern",
    "sections": [],
    "checklist": [],
    "sources": [],
    "rawContent": "\r\n# Retries with Exponential Backoff\r\n\r\n## Problem\r\n\r\nTransient failures (network blips, temporary overload) cause unnecessary errors if not retried. But aggressive retries without backoff create thundering herd problems that worsen outages and prevent recovery.\r\n\r\n## When to use\r\n\r\n- Network calls to external services\r\n- Database connection failures\r\n- Distributed system communication\r\n- Idempotent operations only\r\n- Queue message processing\r\n- Any I/O that may fail transiently\r\n\r\n## Solution\r\n\r\n### 1. Exponential Backoff Algorithm\r\n\r\n```\r\ndelay = min(maxDelay, baseDelay * 2^attempt)\r\n```\r\n\r\n| Attempt | Base 100ms | Base 1s |\r\n|---------|------------|----------|\r\n| 1 | 100ms | 1s |\r\n| 2 | 200ms | 2s |\r\n| 3 | 400ms | 4s |\r\n| 4 | 800ms | 8s |\r\n| 5 | 1600ms | 16s |\r\n\r\n### 2. Add Jitter (Critical!)\r\n\r\nWithout jitter, all clients retry at the same time (thundering herd).\r\n\r\n| Jitter Type | Formula | Use Case |\r\n|-------------|---------|----------|\r\n| **Full Jitter** | `random(0, delay)` | Most cases |\r\n| **Equal Jitter** | `delay/2 + random(0, delay/2)` | Guaranteed minimum wait |\r\n| **Decorrelated** | `min(cap, random(base, prevDelay * 3))` | AWS recommended |\r\n\r\n**Full Jitter (Recommended):**\r\n```typescript\r\nfunction fullJitterBackoff(\r\n  attempt: number,\r\n  baseDelay: number = 100,\r\n  maxDelay: number = 60000\r\n): number {\r\n  const expDelay = Math.min(maxDelay, baseDelay * Math.pow(2, attempt));\r\n  return Math.random() * expDelay;\r\n}\r\n```\r\n\r\n**Decorrelated Jitter (AWS):**\r\n```typescript\r\nlet prevDelay = baseDelay;\r\nfunction decorrelatedJitter(): number {\r\n  prevDelay = Math.min(maxDelay, random(baseDelay, prevDelay * 3));\r\n  return prevDelay;\r\n}\r\n```\r\n\r\n### 3. Identify Retryable Errors\r\n\r\n| Retry | Don't Retry |\r\n|-------|-------------|\r\n| 500 Internal Server Error | 400 Bad Request |\r\n| 502 Bad Gateway | 401 Unauthorized |\r\n| 503 Service Unavailable | 403 Forbidden |\r\n| 504 Gateway Timeout | 404 Not Found |\r\n| 429 Too Many Requests | 409 Conflict |\r\n| Connection timeout | 422 Unprocessable |\r\n| DNS resolution failure | Business logic errors |\r\n\r\n### 4. Respect Retry-After Header\r\n\r\n```http\r\nHTTP/1.1 429 Too Many Requests\r\nRetry-After: 30\r\n\r\nHTTP/1.1 503 Service Unavailable\r\nRetry-After: Wed, 21 Oct 2026 07:28:00 GMT\r\n```\r\n\r\n```typescript\r\nfunction getRetryDelay(response: Response, calculatedDelay: number): number {\r\n  const retryAfter = response.headers.get('Retry-After');\r\n  if (retryAfter) {\r\n    const seconds = parseInt(retryAfter, 10);\r\n    if (!isNaN(seconds)) return seconds * 1000;\r\n    const date = Date.parse(retryAfter);\r\n    if (!isNaN(date)) return date - Date.now();\r\n  }\r\n  return calculatedDelay;\r\n}\r\n```\r\n\r\n## Pitfalls\r\n\r\n| Pitfall | How to Avoid |\r\n|---------|--------------|\r\n| Retrying non-idempotent ops | Only retry safe/idempotent operations |\r\n| No jitter (thundering herd) | Always add random jitter |\r\n| Retrying client errors (4xx) | Only retry transient/server errors |\r\n| Unbounded retries | Set max attempts or time budget |\r\n| Ignoring Retry-After header | Respect server guidance |\r\n\r\n## Checklist\r\n\r\n- [ ] Exponential backoff implemented\r\n- [ ] Random jitter added\r\n- [ ] Maximum retry count configured\r\n- [ ] Only transient errors retried\r\n- [ ] Retry-After header respected\r\n- [ ] Total retry budget defined\r\n- [ ] Idempotency ensured for retry\r\n- [ ] Retry metrics tracked\r\n- [ ] Final failure handled gracefully\r\n- [ ] Retry logic tested\r\n\r\n## Snippets (Generic)\r\n\r\n```\r\nExponential Backoff with Jitter:\r\nbase_delay = 100ms\r\nmax_delay = 60s\r\nmax_attempts = 5\r\n\r\nfor attempt in 1..max_attempts:\r\n  try:\r\n    return make_request()\r\n  except RetryableError:\r\n    if attempt == max_attempts:\r\n      raise\r\n    \r\n    # Exponential: 100ms, 200ms, 400ms, 800ms, 1600ms...\r\n    exp_delay = base_delay * (2 ^ (attempt - 1))\r\n    \r\n    # Cap at maximum\r\n    delay = min(exp_delay, max_delay)\r\n    \r\n    # Add jitter (full jitter)\r\n    jitter_delay = random(0, delay)\r\n    \r\n    sleep(jitter_delay)\r\n\r\nJitter Strategies:\r\n- Full jitter: random(0, delay)\r\n- Equal jitter: delay/2 + random(0, delay/2)\r\n- Decorrelated jitter: min(cap, random(base, prev_delay * 3))\r\n\r\nRetryable vs Non-Retryable:\r\nRetry:\r\n  - 500, 502, 503, 504 (server errors)\r\n  - 429 (rate limit - respect Retry-After)\r\n  - Connection timeout\r\n  - Network errors\r\n\r\nDon't Retry:\r\n  - 400 (bad request)\r\n  - 401, 403 (auth errors)\r\n  - 404 (not found)\r\n  - 422 (validation)\r\n\r\nRetry-After Header:\r\nHTTP/1.1 429 Too Many Requests\r\nRetry-After: 30\r\n\r\n# Wait at least 30 seconds before retry\r\n```\r\n\r\n## Sources\r\n\r\n- AWS Exponential Backoff and Jitter: https://aws.amazon.com/blogs/architecture/exponential-backoff-and-jitter/\r\n- Google API Client Retry: https://cloud.google.com/storage/docs/retry-strategy\r\n- Azure Retry Guidance: https://learn.microsoft.com/en-us/azure/architecture/best-practices/retry-service-specific\r\n- Polly .NET Resilience: https://github.com/App-vNext/Polly\r\n"
  },
  {
    "id": "rel-timeout-strategies",
    "title": "Timeout Strategies and Deadline Propagation",
    "tags": [
      "reliability",
      "timeout",
      "deadline",
      "resilience",
      "latency"
    ],
    "level": "intermediate",
    "stacks": [
      "nodejs",
      "python",
      "go"
    ],
    "scope": "reliability",
    "maturity": "stable",
    "works_with": [
      "nodejs",
      "python",
      "go"
    ],
    "path": "D:/Code/backend-engineering-kit/.shared/production-backend-kit/patterns/rel.timeout-strategies.md",
    "type": "pattern",
    "sections": [],
    "checklist": [],
    "sources": [],
    "rawContent": "\r\n# Timeout Strategies and Deadline Propagation\r\n\r\n## Problem\r\n\r\nWithout proper timeouts:\r\n```\r\n┌─────────┐    ┌─────────┐    ┌─────────┐    ┌─────────┐\r\n│ Client  │───▶│ API     │───▶│ Service │───▶│   DB    │\r\n│         │    │ Gateway │    │    A    │    │  (hung) │\r\n└─────────┘    └─────────┘    └─────────┘    └─────────┘\r\n     │              │              │              │\r\n     │ waiting...   │ waiting...   │ waiting...   │ stuck!\r\n     │ 30s default  │ 30s default  │ 30s default  │\r\n     ▼              ▼              ▼              ▼\r\n   User gives up    Thread blocked  Thread blocked  \r\n   (bad UX)        (exhausted pool) (exhausted pool)\r\n```\r\n\r\n- Resources held indefinitely\r\n- Thread/connection pool exhaustion\r\n- Cascading failures\r\n- User waiting with no feedback\r\n\r\n## When to use\r\n\r\n- **Every** external call (HTTP, DB, cache, queue)\r\n- Internal service-to-service calls\r\n- Any I/O operation\r\n- Background job processing\r\n- User-facing request handlers\r\n\r\n## Solution\r\n\r\n### 1. Timeout Hierarchy\r\n\r\n```typescript\r\n/**\r\n * Timeout hierarchy (from outer to inner):\r\n * \r\n * 1. Request timeout (30s) - Total time for entire request\r\n *    └── 2. Service call timeout (10s) - Call to downstream service\r\n *        └── 3. Connection timeout (5s) - Establishing connection\r\n *            └── 4. Socket timeout (30s) - Idle socket\r\n * \r\n * Each inner timeout should be less than its parent\r\n */\r\n\r\ninterface TimeoutConfig {\r\n  // HTTP client timeouts\r\n  http: {\r\n    connect: number;    // Time to establish connection\r\n    socket: number;     // Time between data packets\r\n    request: number;    // Total request time\r\n  };\r\n  \r\n  // Database timeouts\r\n  db: {\r\n    connect: number;    // Connection acquisition\r\n    query: number;      // Query execution\r\n    transaction: number; // Transaction duration\r\n  };\r\n  \r\n  // Cache timeouts\r\n  cache: {\r\n    connect: number;\r\n    operation: number;\r\n  };\r\n  \r\n  // Request-level\r\n  request: {\r\n    total: number;      // Total request processing time\r\n    graceful: number;   // Time for graceful shutdown\r\n  };\r\n}\r\n\r\nconst timeoutConfig: TimeoutConfig = {\r\n  http: {\r\n    connect: 5000,      // 5s to connect\r\n    socket: 30000,      // 30s idle\r\n    request: 10000,     // 10s total for downstream calls\r\n  },\r\n  db: {\r\n    connect: 5000,      // 5s to get connection\r\n    query: 10000,       // 10s query timeout\r\n    transaction: 30000, // 30s transaction timeout\r\n  },\r\n  cache: {\r\n    connect: 1000,      // 1s to connect\r\n    operation: 500,     // 500ms per operation\r\n  },\r\n  request: {\r\n    total: 30000,       // 30s for entire request\r\n    graceful: 10000,    // 10s for graceful shutdown\r\n  },\r\n};\r\n```\r\n\r\n### 2. HTTP Client with Timeouts\r\n\r\n```typescript\r\nimport axios, { AxiosInstance } from 'axios';\r\n\r\nfunction createHttpClient(config: {\r\n  baseURL: string;\r\n  timeout?: number;\r\n  connectTimeout?: number;\r\n}): AxiosInstance {\r\n  const client = axios.create({\r\n    baseURL: config.baseURL,\r\n    timeout: config.timeout || 10000,\r\n    \r\n    // Per-request timeout\r\n    signal: AbortSignal.timeout(config.timeout || 10000),\r\n  });\r\n\r\n  // Connection timeout via custom agent\r\n  const http = require('http');\r\n  const https = require('https');\r\n  \r\n  const agentOptions = {\r\n    timeout: config.connectTimeout || 5000,\r\n    keepAlive: true,\r\n    keepAliveMsecs: 30000,\r\n    maxSockets: 100,\r\n    maxFreeSockets: 10,\r\n  };\r\n  \r\n  client.defaults.httpAgent = new http.Agent(agentOptions);\r\n  client.defaults.httpsAgent = new https.Agent(agentOptions);\r\n\r\n  // Add request timing\r\n  client.interceptors.request.use((config) => {\r\n    config.metadata = { startTime: Date.now() };\r\n    return config;\r\n  });\r\n\r\n  client.interceptors.response.use(\r\n    (response) => {\r\n      const duration = Date.now() - response.config.metadata.startTime;\r\n      metrics.histogram('http.client.duration', duration, {\r\n        host: new URL(response.config.url!).hostname,\r\n        status: String(response.status),\r\n      });\r\n      return response;\r\n    },\r\n    (error) => {\r\n      if (axios.isCancel(error) || error.code === 'ECONNABORTED') {\r\n        metrics.increment('http.client.timeout');\r\n        throw new TimeoutError(`Request timeout: ${error.config?.url}`);\r\n      }\r\n      throw error;\r\n    }\r\n  );\r\n\r\n  return client;\r\n}\r\n\r\n// Usage with per-request timeout\r\nconst userService = createHttpClient({\r\n  baseURL: 'http://user-service',\r\n  timeout: 5000,\r\n});\r\n\r\nasync function getUser(id: string, deadline?: number) {\r\n  const timeout = deadline || 5000;\r\n  \r\n  const response = await userService.get(`/users/${id}`, {\r\n    timeout,\r\n    signal: AbortSignal.timeout(timeout),\r\n  });\r\n  \r\n  return response.data;\r\n}\r\n```\r\n\r\n### 3. Deadline Propagation\r\n\r\n```typescript\r\nimport { AsyncLocalStorage } from 'async_hooks';\r\n\r\ninterface RequestDeadline {\r\n  deadline: number;      // Absolute timestamp when request must complete\r\n  remaining(): number;   // Remaining time in ms\r\n  isExpired(): boolean;\r\n  getChildDeadline(maxTime: number): number;  // For downstream calls\r\n}\r\n\r\nconst deadlineStorage = new AsyncLocalStorage<RequestDeadline>();\r\n\r\nclass Deadline implements RequestDeadline {\r\n  constructor(public deadline: number) {}\r\n  \r\n  remaining(): number {\r\n    return Math.max(0, this.deadline - Date.now());\r\n  }\r\n  \r\n  isExpired(): boolean {\r\n    return Date.now() >= this.deadline;\r\n  }\r\n  \r\n  getChildDeadline(maxTime: number): number {\r\n    // Child deadline is min of remaining time and maxTime\r\n    // Leave buffer for processing response\r\n    const buffer = 100;  // 100ms buffer\r\n    return Math.min(this.remaining() - buffer, maxTime);\r\n  }\r\n}\r\n\r\n// Middleware to set deadline\r\nexport function deadlineMiddleware(maxRequestTime: number = 30000) {\r\n  return (req: Request, res: Response, next: NextFunction) => {\r\n    // Check for propagated deadline header\r\n    const deadlineHeader = req.get('X-Deadline');\r\n    let deadline: number;\r\n    \r\n    if (deadlineHeader) {\r\n      deadline = parseInt(deadlineHeader, 10);\r\n      // Don't accept deadlines beyond our max\r\n      deadline = Math.min(deadline, Date.now() + maxRequestTime);\r\n    } else {\r\n      deadline = Date.now() + maxRequestTime;\r\n    }\r\n    \r\n    const requestDeadline = new Deadline(deadline);\r\n    \r\n    // Set timeout for response\r\n    const timeoutId = setTimeout(() => {\r\n      if (!res.headersSent) {\r\n        res.status(504).json({\r\n          error: 'Request timeout',\r\n          code: 'DEADLINE_EXCEEDED',\r\n        });\r\n      }\r\n    }, requestDeadline.remaining());\r\n    \r\n    res.on('finish', () => clearTimeout(timeoutId));\r\n    \r\n    deadlineStorage.run(requestDeadline, () => next());\r\n  };\r\n}\r\n\r\n// Get current deadline\r\nexport function getDeadline(): RequestDeadline | undefined {\r\n  return deadlineStorage.getStore();\r\n}\r\n\r\n// Helper to check deadline before expensive operations\r\nexport function checkDeadline(): void {\r\n  const deadline = getDeadline();\r\n  if (deadline?.isExpired()) {\r\n    throw new DeadlineExceededError('Request deadline exceeded');\r\n  }\r\n}\r\n\r\n// Propagate deadline to downstream service\r\nexport async function callDownstream<T>(\r\n  fn: (timeout: number) => Promise<T>,\r\n  maxTime: number\r\n): Promise<T> {\r\n  const deadline = getDeadline();\r\n  const timeout = deadline?.getChildDeadline(maxTime) ?? maxTime;\r\n  \r\n  if (timeout <= 0) {\r\n    throw new DeadlineExceededError('Insufficient time remaining');\r\n  }\r\n  \r\n  return fn(timeout);\r\n}\r\n\r\n// Usage\r\napp.get('/api/orders/:id', deadlineMiddleware(30000), async (req, res) => {\r\n  const orderId = req.params.id;\r\n  \r\n  // Check deadline before starting\r\n  checkDeadline();\r\n  \r\n  // Call user service with propagated deadline\r\n  const user = await callDownstream(\r\n    (timeout) => userService.get(`/users/${req.user.id}`, {\r\n      timeout,\r\n      headers: {\r\n        'X-Deadline': String(getDeadline()!.deadline),\r\n      },\r\n    }),\r\n    5000\r\n  );\r\n  \r\n  checkDeadline();\r\n  \r\n  // Call order service\r\n  const order = await callDownstream(\r\n    (timeout) => orderService.get(`/orders/${orderId}`, {\r\n      timeout,\r\n      headers: {\r\n        'X-Deadline': String(getDeadline()!.deadline),\r\n      },\r\n    }),\r\n    5000\r\n  );\r\n  \r\n  res.json({ order, user: user.data });\r\n});\r\n```\r\n\r\n### 4. Database Query Timeouts\r\n\r\n```typescript\r\nimport { Pool, PoolConfig } from 'pg';\r\n\r\n// PostgreSQL with timeouts\r\nconst poolConfig: PoolConfig = {\r\n  host: process.env.DB_HOST,\r\n  database: process.env.DB_NAME,\r\n  \r\n  // Connection timeout\r\n  connectionTimeoutMillis: 5000,\r\n  \r\n  // Query timeout (default for all queries)\r\n  statement_timeout: 10000,\r\n  \r\n  // Idle in transaction timeout\r\n  idle_in_transaction_session_timeout: 30000,\r\n  \r\n  // Lock timeout\r\n  lock_timeout: 5000,\r\n};\r\n\r\nconst pool = new Pool(poolConfig);\r\n\r\n// Query with specific timeout\r\nasync function queryWithTimeout<T>(\r\n  sql: string,\r\n  params: any[],\r\n  timeout: number\r\n): Promise<T[]> {\r\n  const client = await pool.connect();\r\n  \r\n  try {\r\n    // Set statement timeout for this session\r\n    await client.query(`SET statement_timeout = ${timeout}`);\r\n    \r\n    const result = await client.query(sql, params);\r\n    return result.rows;\r\n  } finally {\r\n    // Reset timeout\r\n    await client.query('RESET statement_timeout');\r\n    client.release();\r\n  }\r\n}\r\n\r\n// Transaction with timeout\r\nasync function transactionWithTimeout<T>(\r\n  fn: (client: PoolClient) => Promise<T>,\r\n  timeout: number\r\n): Promise<T> {\r\n  const client = await pool.connect();\r\n  \r\n  try {\r\n    await client.query('BEGIN');\r\n    await client.query(`SET LOCAL statement_timeout = ${timeout}`);\r\n    await client.query(`SET LOCAL lock_timeout = ${Math.floor(timeout / 2)}`);\r\n    \r\n    const result = await fn(client);\r\n    \r\n    await client.query('COMMIT');\r\n    return result;\r\n  } catch (error) {\r\n    await client.query('ROLLBACK');\r\n    throw error;\r\n  } finally {\r\n    client.release();\r\n  }\r\n}\r\n\r\n// With deadline propagation\r\nasync function queryWithDeadline<T>(sql: string, params: any[]): Promise<T[]> {\r\n  const deadline = getDeadline();\r\n  const timeout = deadline?.remaining() ?? 10000;\r\n  \r\n  if (timeout < 100) {\r\n    throw new DeadlineExceededError('Insufficient time for database query');\r\n  }\r\n  \r\n  return queryWithTimeout(sql, params, timeout - 100);  // 100ms buffer\r\n}\r\n```\r\n\r\n### 5. Promise Timeout Wrapper\r\n\r\n```typescript\r\n// Generic timeout wrapper for any promise\r\n\r\nclass TimeoutError extends Error {\r\n  constructor(message: string) {\r\n    super(message);\r\n    this.name = 'TimeoutError';\r\n  }\r\n}\r\n\r\nasync function withTimeout<T>(\r\n  promise: Promise<T>,\r\n  timeout: number,\r\n  operation: string = 'Operation'\r\n): Promise<T> {\r\n  let timeoutId: NodeJS.Timeout;\r\n  \r\n  const timeoutPromise = new Promise<never>((_, reject) => {\r\n    timeoutId = setTimeout(() => {\r\n      reject(new TimeoutError(`${operation} timed out after ${timeout}ms`));\r\n    }, timeout);\r\n  });\r\n  \r\n  try {\r\n    return await Promise.race([promise, timeoutPromise]);\r\n  } finally {\r\n    clearTimeout(timeoutId!);\r\n  }\r\n}\r\n\r\n// With AbortController for cancellation\r\nasync function withTimeoutAndCancel<T>(\r\n  fn: (signal: AbortSignal) => Promise<T>,\r\n  timeout: number,\r\n  operation: string = 'Operation'\r\n): Promise<T> {\r\n  const controller = new AbortController();\r\n  const timeoutId = setTimeout(() => controller.abort(), timeout);\r\n  \r\n  try {\r\n    return await fn(controller.signal);\r\n  } catch (error) {\r\n    if (error.name === 'AbortError') {\r\n      throw new TimeoutError(`${operation} timed out after ${timeout}ms`);\r\n    }\r\n    throw error;\r\n  } finally {\r\n    clearTimeout(timeoutId);\r\n  }\r\n}\r\n\r\n// Usage\r\nconst result = await withTimeout(\r\n  expensiveOperation(),\r\n  5000,\r\n  'Expensive operation'\r\n);\r\n\r\nconst data = await withTimeoutAndCancel(\r\n  async (signal) => {\r\n    const response = await fetch(url, { signal });\r\n    return response.json();\r\n  },\r\n  5000,\r\n  'API call'\r\n);\r\n```\r\n\r\n### 6. Timeout with Fallback\r\n\r\n```typescript\r\n// Return fallback value on timeout instead of throwing\r\n\r\nasync function withFallback<T>(\r\n  fn: () => Promise<T>,\r\n  timeout: number,\r\n  fallback: T | (() => T),\r\n  options: {\r\n    logTimeout?: boolean;\r\n    operation?: string;\r\n  } = {}\r\n): Promise<T> {\r\n  try {\r\n    return await withTimeout(fn(), timeout, options.operation);\r\n  } catch (error) {\r\n    if (error instanceof TimeoutError) {\r\n      if (options.logTimeout) {\r\n        logger.warn({\r\n          operation: options.operation,\r\n          timeout,\r\n        }, 'Operation timed out, using fallback');\r\n      }\r\n      \r\n      return typeof fallback === 'function' ? (fallback as () => T)() : fallback;\r\n    }\r\n    throw error;\r\n  }\r\n}\r\n\r\n// Usage for non-critical operations\r\nconst recommendations = await withFallback(\r\n  () => recommendationService.getForUser(userId),\r\n  500,  // 500ms timeout\r\n  [],   // Return empty array on timeout\r\n  { logTimeout: true, operation: 'getRecommendations' }\r\n);\r\n\r\n// Cached fallback\r\nconst userProfile = await withFallback(\r\n  () => userService.getProfile(userId),\r\n  1000,\r\n  () => cache.get(`user:${userId}`) || DEFAULT_PROFILE,\r\n  { logTimeout: true, operation: 'getUserProfile' }\r\n);\r\n```\r\n\r\n### 7. Timeout Configuration by Environment\r\n\r\n```typescript\r\n// Different timeouts for different environments\r\n\r\ninterface TimeoutProfile {\r\n  http: { connect: number; request: number };\r\n  db: { query: number; transaction: number };\r\n  cache: { operation: number };\r\n  request: { total: number };\r\n}\r\n\r\nconst timeoutProfiles: Record<string, TimeoutProfile> = {\r\n  development: {\r\n    http: { connect: 10000, request: 30000 },\r\n    db: { query: 30000, transaction: 60000 },\r\n    cache: { operation: 5000 },\r\n    request: { total: 120000 },  // Long timeouts for debugging\r\n  },\r\n  \r\n  staging: {\r\n    http: { connect: 5000, request: 15000 },\r\n    db: { query: 15000, transaction: 45000 },\r\n    cache: { operation: 1000 },\r\n    request: { total: 60000 },\r\n  },\r\n  \r\n  production: {\r\n    http: { connect: 3000, request: 10000 },\r\n    db: { query: 10000, transaction: 30000 },\r\n    cache: { operation: 500 },\r\n    request: { total: 30000 },  // Strict timeouts\r\n  },\r\n};\r\n\r\nconst env = process.env.NODE_ENV || 'development';\r\nexport const timeouts = timeoutProfiles[env] || timeoutProfiles.production;\r\n```\r\n\r\n### 8. Monitoring Timeout Metrics\r\n\r\n```typescript\r\n// Track timeout occurrences and near-misses\r\n\r\nclass TimeoutMonitor {\r\n  private thresholds = {\r\n    warning: 0.8,  // Warn at 80% of timeout\r\n    critical: 0.95, // Critical at 95%\r\n  };\r\n\r\n  wrapWithMonitoring<T>(\r\n    fn: () => Promise<T>,\r\n    timeout: number,\r\n    operation: string\r\n  ): Promise<T> {\r\n    const startTime = Date.now();\r\n    \r\n    return fn()\r\n      .then(result => {\r\n        const duration = Date.now() - startTime;\r\n        this.recordDuration(operation, duration, timeout, 'success');\r\n        return result;\r\n      })\r\n      .catch(error => {\r\n        const duration = Date.now() - startTime;\r\n        const status = error instanceof TimeoutError ? 'timeout' : 'error';\r\n        this.recordDuration(operation, duration, timeout, status);\r\n        throw error;\r\n      });\r\n  }\r\n\r\n  private recordDuration(\r\n    operation: string,\r\n    duration: number,\r\n    timeout: number,\r\n    status: string\r\n  ) {\r\n    const ratio = duration / timeout;\r\n    \r\n    // Record metrics\r\n    metrics.histogram(`operation.duration`, duration, { operation, status });\r\n    metrics.gauge(`operation.timeout_ratio`, ratio, { operation });\r\n    \r\n    // Alert on near-misses\r\n    if (status === 'success') {\r\n      if (ratio >= this.thresholds.critical) {\r\n        logger.warn({\r\n          operation,\r\n          duration,\r\n          timeout,\r\n          ratio,\r\n        }, 'Operation nearly timed out');\r\n      } else if (ratio >= this.thresholds.warning) {\r\n        logger.info({\r\n          operation,\r\n          duration,\r\n          timeout,\r\n          ratio,\r\n        }, 'Operation approaching timeout threshold');\r\n      }\r\n    }\r\n    \r\n    // Track timeout rate\r\n    if (status === 'timeout') {\r\n      metrics.increment('operation.timeout', { operation });\r\n    }\r\n  }\r\n}\r\n\r\n// Usage\r\nconst monitor = new TimeoutMonitor();\r\n\r\nconst result = await monitor.wrapWithMonitoring(\r\n  () => userService.getUser(id),\r\n  5000,\r\n  'getUser'\r\n);\r\n```\r\n\r\n## Pitfalls\r\n\r\n| Pitfall | Impact | How to Avoid |\r\n|---------|--------|--------------|\r\n| No timeout | Resource exhaustion | Always set timeouts |\r\n| Timeout too long | Slow failure, bad UX | Match business requirements |\r\n| Timeout too short | False failures | Test under load |\r\n| No deadline propagation | Wasted work downstream | Propagate via headers |\r\n| Ignoring remaining time | Waste resources | Check deadline before operations |\r\n| Same timeout everywhere | Suboptimal | Tune per operation |\r\n| No fallback strategy | All-or-nothing | Implement graceful degradation |\r\n\r\n## Checklist\r\n\r\n- [ ] Connection timeout set for all clients\r\n- [ ] Request timeout set for all HTTP calls\r\n- [ ] Database query timeout configured\r\n- [ ] Deadline propagation implemented\r\n- [ ] Timeout varies by operation criticality\r\n- [ ] Metrics track timeout rate and near-misses\r\n- [ ] Fallback strategy for non-critical operations\r\n- [ ] Different timeouts per environment\r\n- [ ] Child timeouts less than parent\r\n- [ ] Buffer time for response processing\r\n\r\n## References\r\n\r\n- [Google SRE: Addressing Cascading Failures](https://sre.google/sre-book/addressing-cascading-failures/)\r\n- [AWS: Timeouts, Retries, and Backoff](https://aws.amazon.com/builders-library/timeouts-retries-and-backoff-with-jitter/)\r\n- [gRPC Deadlines](https://grpc.io/docs/guides/deadlines/)\r\n- [Azure Timeout Pattern](https://docs.microsoft.com/en-us/azure/architecture/patterns/timeout)\r\n"
  },
  {
    "id": "rel-timeouts",
    "title": "Timeouts",
    "tags": [
      "reliability",
      "timeouts",
      "resilience",
      "latency",
      "sla"
    ],
    "level": "intermediate",
    "stacks": [
      "all"
    ],
    "scope": "reliability",
    "maturity": "stable",
    "works_with": [
      "all"
    ],
    "path": "D:/Code/backend-engineering-kit/.shared/production-backend-kit/patterns/rel.timeouts.md",
    "type": "pattern",
    "sections": [],
    "checklist": [],
    "sources": [],
    "rawContent": "\r\n# Timeouts\r\n\r\n## Problem\r\n\r\nWithout proper timeouts, a slow or unresponsive service can block resources indefinitely, causing cascading failures, thread pool exhaustion, and system-wide outages. \"Waiting forever\" is never acceptable in production.\r\n\r\n## When to use\r\n\r\n- Any external HTTP calls\r\n- Database queries\r\n- Message queue operations\r\n- Cache operations\r\n- File I/O operations\r\n- Any I/O that could hang\r\n\r\n## Solution\r\n\r\n### 1. Timeout Types\r\n\r\n| Timeout Type | What It Controls | Typical Values |\r\n|--------------|------------------|----------------|\r\n| **Connection** | Time to establish connection | 1-5s |\r\n| **Read/Socket** | Time waiting for data chunks | 5-30s |\r\n| **Request/Total** | End-to-end operation time | 10-60s |\r\n| **Idle** | Time connection sits unused | 60-300s |\r\n| **Pool Checkout** | Time waiting for connection from pool | 1-5s |\r\n\r\n### 2. Timeout Hierarchy (Critical!)\r\n\r\n```\r\nClient (Browser) → Gateway → Service A → Service B → Database\r\n      60s            50s         30s          15s         5s\r\n      \r\n↓ Direction of shorter timeouts ↓\r\n\r\nRule: Inner services MUST have shorter timeouts than outer services.\r\nOtherwise: Outer service times out, inner continues wasting resources.\r\n```\r\n\r\n**Example Hierarchy:**\r\n```yaml\r\napi-gateway:\r\n  timeout: 60s  # Longest\r\n\r\norder-service:\r\n  client_timeout: 50s  # Gateway has buffer\r\n  db_timeout: 30s\r\n  cache_timeout: 1s\r\n\r\npayment-service:\r\n  client_timeout: 45s\r\n  external_api_timeout: 30s\r\n  db_timeout: 15s\r\n\r\ndatabase:\r\n  statement_timeout: 30s\r\n  lock_timeout: 10s\r\n```\r\n\r\n### 3. Implementation Examples\r\n\r\n**HTTP Client (Node.js with Axios):**\r\n```typescript\r\nimport axios from 'axios';\r\nimport { AbortController } from 'abort-controller';\r\n\r\nconst httpClient = axios.create({\r\n  timeout: 10000,  // 10s total request timeout\r\n  // Note: Axios timeout is for entire request, not connection\r\n});\r\n\r\n// With AbortController for more control\r\nasync function fetchWithTimeout<T>(\r\n  url: string,\r\n  timeoutMs: number = 10000\r\n): Promise<T> {\r\n  const controller = new AbortController();\r\n  const timeoutId = setTimeout(() => controller.abort(), timeoutMs);\r\n  \r\n  try {\r\n    const response = await axios.get<T>(url, {\r\n      signal: controller.signal,\r\n    });\r\n    return response.data;\r\n  } catch (error) {\r\n    if (error.name === 'AbortError' || error.code === 'ECONNABORTED') {\r\n      throw new TimeoutError(`Request to ${url} timed out after ${timeoutMs}ms`);\r\n    }\r\n    throw error;\r\n  } finally {\r\n    clearTimeout(timeoutId);\r\n  }\r\n}\r\n```\r\n\r\n**Database (PostgreSQL):**\r\n```sql\r\n-- Session level\r\nSET statement_timeout = '30s';\r\nSET lock_timeout = '10s';\r\n\r\n-- Per query (PostgreSQL 9.3+)\r\nSELECT /*+ statement_timeout(5000) */ * FROM large_table;\r\n```\r\n\r\n**Prisma:**\r\n```typescript\r\nconst prisma = new PrismaClient({\r\n  datasources: {\r\n    db: {\r\n      url: process.env.DATABASE_URL + '?connection_timeout=5&pool_timeout=10',\r\n    },\r\n  },\r\n});\r\n\r\n// Per-query timeout\r\nawait prisma.$queryRawUnsafe(\r\n  `SET LOCAL statement_timeout = '5s'; SELECT * FROM orders WHERE id = $1`,\r\n  orderId\r\n);\r\n```\r\n\r\n**Connection Pool (Generic):**\r\n```typescript\r\nconst pool = new Pool({\r\n  connectionTimeoutMillis: 5000,  // Time to establish new connection\r\n  idleTimeoutMillis: 30000,       // Close idle connections after 30s\r\n  query_timeout: 30000,           // Query timeout\r\n  statement_timeout: 30000,       // PostgreSQL statement timeout\r\n});\r\n```\r\n\r\n### 4. Timeout Budget Pattern\r\n\r\n```typescript\r\nclass TimeoutBudget {\r\n  private readonly startTime: number;\r\n  private readonly totalBudget: number;\r\n  \r\n  constructor(totalBudgetMs: number) {\r\n    this.startTime = Date.now();\r\n    this.totalBudget = totalBudgetMs;\r\n  }\r\n  \r\n  remaining(): number {\r\n    return Math.max(0, this.totalBudget - (Date.now() - this.startTime));\r\n  }\r\n  \r\n  isExpired(): boolean {\r\n    return this.remaining() <= 0;\r\n  }\r\n  \r\n  checkExpired(): void {\r\n    if (this.isExpired()) {\r\n      throw new TimeoutError('Request budget exhausted');\r\n    }\r\n  }\r\n}\r\n\r\n// Usage\r\nasync function processOrder(orderId: string): Promise<Order> {\r\n  const budget = new TimeoutBudget(30000); // 30s total budget\r\n  \r\n  // Step 1: Fetch order (use part of budget)\r\n  budget.checkExpired();\r\n  const order = await fetchWithTimeout(\r\n    `/orders/${orderId}`,\r\n    Math.min(budget.remaining(), 10000)  // Max 10s, or remaining\r\n  );\r\n  \r\n  // Step 2: Validate inventory\r\n  budget.checkExpired();\r\n  await validateInventory(order.items, budget.remaining());\r\n  \r\n  // Step 3: Process payment (with remaining budget)\r\n  budget.checkExpired();\r\n  await processPayment(order, budget.remaining());\r\n  \r\n  return order;\r\n}\r\n```\r\n\r\n### 5. Timeout Guidelines by Operation\r\n\r\n| Operation | Recommended | Notes |\r\n|-----------|-------------|-------|\r\n| Cache read (Redis) | 100-500ms | Should be fast, fail to source |\r\n| Cache write | 200-1000ms | Slightly more tolerance |\r\n| Internal service call | 1-10s | Depends on operation |\r\n| External API call | 5-30s | Third party SLAs vary |\r\n| Database query (simple) | 1-5s | OLTP queries |\r\n| Database query (complex) | 5-60s | Reports, aggregations |\r\n| File upload | 30s-5min | Depends on size |\r\n| Webhook delivery | 5-30s | Recipient must respond quickly |\r\n| Background job step | 30s-5min | Per step, not total job |\r\n\r\n## Pitfalls\r\n\r\n| Pitfall | How to Avoid |\r\n|---------|--------------|\r\n| No timeout set (infinite wait) | Always configure explicit timeouts |\r\n| Timeout too long | Base on P99 + buffer, not worst case |\r\n| Timeout too aggressive | Allow for realistic response times |\r\n| Inner > outer timeout | Ensure inner < outer for proper cascade |\r\n| Not considering retry budget | Total time = (attempts × timeout) |\r\n\r\n## Checklist\r\n\r\n- [ ] Connection timeout configured\r\n- [ ] Read/request timeout configured\r\n- [ ] Database query timeout set\r\n- [ ] Message queue operation timeout set\r\n- [ ] Inner timeouts shorter than outer\r\n- [ ] Timeout values documented\r\n- [ ] Timeout errors logged with context\r\n- [ ] Retry budget considers timeout × attempts\r\n- [ ] Circuit breaker monitors timeout rate\r\n- [ ] Timeouts tested in integration tests\r\n\r\n## Snippets (Generic)\r\n\r\n```\r\nTimeout Layers:\r\nFrontend (30s) → API Gateway (25s) → Service (20s) → Database (5s)\r\n\r\nHTTP Client Configuration:\r\nclient = HttpClient(\r\n  connect_timeout=5s,\r\n  read_timeout=10s,\r\n  total_timeout=15s\r\n)\r\n\r\nDatabase Configuration:\r\nconnection_timeout=5s\r\nquery_timeout=30s\r\npool_checkout_timeout=3s\r\n\r\nTimeout Guidelines:\r\n- API calls to external services: 5-30s\r\n- Database queries: 5-60s (depends on query)\r\n- Cache reads: 100-500ms\r\n- Internal microservice calls: 1-10s\r\n- Background job steps: varies (minutes for complex)\r\n\r\nLayered Timeout Example:\r\n# Total budget: 25 seconds\r\n# - Initial attempt: 10s timeout\r\n# - First retry: 7s timeout  \r\n# - Second retry: 5s timeout\r\n# - Remaining buffer: 3s\r\n\r\nTimeout Error Handling:\r\ntry:\r\n  response = http.get(url, timeout=10s)\r\nexcept TimeoutError:\r\n  log.warn(\"Request to {url} timed out\", context)\r\n  if should_retry:\r\n    return retry_with_backoff()\r\n  else:\r\n    raise ServiceUnavailable()\r\n```\r\n\r\n## Sources\r\n\r\n- AWS Best Practices for Timeouts: https://aws.amazon.com/builders-library/timeouts-retries-and-backoff-with-jitter/\r\n- Netflix Hystrix Wiki: https://github.com/Netflix/Hystrix/wiki\r\n- Google SRE Book - Handling Overload: https://sre.google/sre-book/handling-overload/\r\n- Release It! (Michael Nygard): https://pragprog.com/titles/mnee2/release-it-second-edition/\r\n"
  },
  {
    "id": "sec-api-security-headers",
    "title": "HTTP Security Headers",
    "tags": [
      "security",
      "http",
      "headers",
      "csp",
      "cors"
    ],
    "level": "beginner",
    "stacks": [
      "all"
    ],
    "scope": "security",
    "maturity": "stable",
    "works_with": [
      "all"
    ],
    "path": "D:/Code/backend-engineering-kit/.shared/production-backend-kit/patterns/sec.api-security-headers.md",
    "type": "pattern",
    "sections": [],
    "checklist": [],
    "sources": [],
    "rawContent": "\r\n# HTTP Security Headers\r\n\r\n## Problem\r\n\r\nMissing or misconfigured HTTP headers expose applications to:\r\n- Cross-Site Scripting (XSS) attacks\r\n- Clickjacking via iframe embedding\r\n- MIME-type sniffing exploits\r\n- Downgrade attacks to HTTP\r\n- Information leakage about server/framework\r\n- Cross-site data theft\r\n\r\n**Headers are your first line of defense - configure them properly!**\r\n\r\n## When to use\r\n\r\n- **Every web application** - no exceptions\r\n- REST/GraphQL APIs\r\n- Single Page Applications\r\n- Server-rendered websites\r\n- Any HTTP-based service\r\n\r\n## Solution\r\n\r\n### 1. Essential Security Headers\r\n\r\n```\r\n┌─────────────────────────────────────────────────────────────────────────────┐\r\n│                      SECURITY HEADERS QUICK REFERENCE                       │\r\n├─────────────────────────────────────────────────────────────────────────────┤\r\n│ Header                          │ Purpose                                   │\r\n├─────────────────────────────────┼───────────────────────────────────────────┤\r\n│ Content-Security-Policy         │ Prevent XSS, injection attacks            │\r\n│ Strict-Transport-Security       │ Force HTTPS                               │\r\n│ X-Content-Type-Options          │ Prevent MIME sniffing                     │\r\n│ X-Frame-Options                 │ Prevent clickjacking                      │\r\n│ Referrer-Policy                 │ Control referrer information              │\r\n│ Permissions-Policy              │ Control browser features                  │\r\n│ Cross-Origin-Opener-Policy      │ Isolate browsing context                  │\r\n│ Cross-Origin-Resource-Policy    │ Control resource sharing                  │\r\n│ Cross-Origin-Embedder-Policy    │ Enable cross-origin isolation             │\r\n├─────────────────────────────────┼───────────────────────────────────────────┤\r\n│ REMOVE THESE:                   │                                           │\r\n│ X-Powered-By                    │ Reveals server technology                 │\r\n│ Server                          │ Reveals server software                   │\r\n└─────────────────────────────────────────────────────────────────────────────┘\r\n```\r\n\r\n### 2. Content-Security-Policy (CSP)\r\n\r\n```typescript\r\n// CSP is the most important security header\r\n// It controls what resources the browser can load\r\n\r\n// Strict CSP for SPAs\r\nconst strictCSP = [\r\n  \"default-src 'self'\",                    // Default: only same origin\r\n  \"script-src 'self'\",                     // Scripts: only same origin\r\n  \"style-src 'self' 'unsafe-inline'\",      // Styles: same origin + inline (for CSS-in-JS)\r\n  \"img-src 'self' data: https:\",           // Images: self, data URIs, any HTTPS\r\n  \"font-src 'self'\",                       // Fonts: same origin\r\n  \"connect-src 'self' https://api.example.com\", // XHR/Fetch: self + API\r\n  \"frame-ancestors 'none'\",                // Prevent embedding (clickjacking)\r\n  \"base-uri 'self'\",                       // Restrict <base> tag\r\n  \"form-action 'self'\",                    // Restrict form submissions\r\n  \"upgrade-insecure-requests\",             // Auto-upgrade HTTP to HTTPS\r\n].join('; ');\r\n\r\n// CSP for API (simpler)\r\nconst apiCSP = [\r\n  \"default-src 'none'\",                    // APIs shouldn't load any resources\r\n  \"frame-ancestors 'none'\",\r\n].join('; ');\r\n\r\n// CSP with nonce for inline scripts (more secure)\r\nfunction generateCSPWithNonce(): { header: string; nonce: string } {\r\n  const nonce = crypto.randomBytes(16).toString('base64');\r\n  \r\n  const header = [\r\n    \"default-src 'self'\",\r\n    `script-src 'self' 'nonce-${nonce}'`,  // Only scripts with this nonce\r\n    \"style-src 'self' 'unsafe-inline'\",\r\n    \"img-src 'self' data: https:\",\r\n    \"connect-src 'self' https://api.example.com\",\r\n    \"frame-ancestors 'none'\",\r\n    \"base-uri 'self'\",\r\n    \"form-action 'self'\",\r\n  ].join('; ');\r\n  \r\n  return { header, nonce };\r\n}\r\n\r\n// Usage in HTML:\r\n// <script nonce=\"<%= nonce %>\">...</script>\r\n```\r\n\r\n### 3. Complete Header Configuration\r\n\r\n```typescript\r\nimport helmet from 'helmet';\r\nimport express from 'express';\r\n\r\nconst app = express();\r\n\r\n// Using Helmet (recommended)\r\napp.use(helmet({\r\n  // Content-Security-Policy\r\n  contentSecurityPolicy: {\r\n    directives: {\r\n      defaultSrc: [\"'self'\"],\r\n      scriptSrc: [\"'self'\"],\r\n      styleSrc: [\"'self'\", \"'unsafe-inline'\"],\r\n      imgSrc: [\"'self'\", \"data:\", \"https:\"],\r\n      connectSrc: [\"'self'\", \"https://api.example.com\"],\r\n      fontSrc: [\"'self'\"],\r\n      objectSrc: [\"'none'\"],\r\n      frameAncestors: [\"'none'\"],\r\n      baseUri: [\"'self'\"],\r\n      formAction: [\"'self'\"],\r\n      upgradeInsecureRequests: [],\r\n    },\r\n  },\r\n  \r\n  // Strict-Transport-Security\r\n  hsts: {\r\n    maxAge: 31536000,        // 1 year\r\n    includeSubDomains: true,\r\n    preload: true,           // Submit to HSTS preload list\r\n  },\r\n  \r\n  // X-Content-Type-Options: nosniff\r\n  noSniff: true,\r\n  \r\n  // X-Frame-Options: DENY (or SAMEORIGIN)\r\n  frameguard: { action: 'deny' },\r\n  \r\n  // Referrer-Policy\r\n  referrerPolicy: { policy: 'strict-origin-when-cross-origin' },\r\n  \r\n  // X-XSS-Protection: 0 (disabled - rely on CSP instead)\r\n  xssFilter: false, // Modern browsers don't need this, CSP is better\r\n  \r\n  // X-Powered-By: removed\r\n  hidePoweredBy: true,\r\n  \r\n  // Cross-Origin-Opener-Policy\r\n  crossOriginOpenerPolicy: { policy: 'same-origin' },\r\n  \r\n  // Cross-Origin-Resource-Policy\r\n  crossOriginResourcePolicy: { policy: 'same-origin' },\r\n  \r\n  // Cross-Origin-Embedder-Policy\r\n  crossOriginEmbedderPolicy: { policy: 'require-corp' },\r\n}));\r\n\r\n// Additional headers not covered by Helmet\r\napp.use((req, res, next) => {\r\n  // Permissions-Policy (formerly Feature-Policy)\r\n  res.setHeader('Permissions-Policy', [\r\n    'accelerometer=()',\r\n    'camera=()',\r\n    'geolocation=()',\r\n    'gyroscope=()',\r\n    'magnetometer=()',\r\n    'microphone=()',\r\n    'payment=()',\r\n    'usb=()',\r\n  ].join(', '));\r\n  \r\n  // Cache-Control for sensitive endpoints\r\n  if (req.path.startsWith('/api/')) {\r\n    res.setHeader('Cache-Control', 'no-store, no-cache, must-revalidate, private');\r\n    res.setHeader('Pragma', 'no-cache');\r\n  }\r\n  \r\n  next();\r\n});\r\n```\r\n\r\n### 4. CORS Configuration\r\n\r\n```typescript\r\nimport cors from 'cors';\r\n\r\n// CORS for APIs\r\nconst corsOptions: cors.CorsOptions = {\r\n  // Allowed origins\r\n  origin: (origin, callback) => {\r\n    const allowedOrigins = [\r\n      'https://app.example.com',\r\n      'https://admin.example.com',\r\n    ];\r\n    \r\n    // Allow requests with no origin (mobile apps, Postman)\r\n    if (!origin) {\r\n      return callback(null, true);\r\n    }\r\n    \r\n    if (allowedOrigins.includes(origin)) {\r\n      return callback(null, true);\r\n    }\r\n    \r\n    // Dev environment - allow localhost\r\n    if (process.env.NODE_ENV === 'development' && \r\n        origin.match(/^http:\\/\\/localhost:\\d+$/)) {\r\n      return callback(null, true);\r\n    }\r\n    \r\n    callback(new Error('Not allowed by CORS'));\r\n  },\r\n  \r\n  // Allowed methods\r\n  methods: ['GET', 'POST', 'PUT', 'PATCH', 'DELETE', 'OPTIONS'],\r\n  \r\n  // Allowed headers\r\n  allowedHeaders: [\r\n    'Content-Type',\r\n    'Authorization',\r\n    'X-Requested-With',\r\n    'X-Correlation-ID',\r\n  ],\r\n  \r\n  // Exposed headers (accessible to JavaScript)\r\n  exposedHeaders: [\r\n    'X-Correlation-ID',\r\n    'X-RateLimit-Limit',\r\n    'X-RateLimit-Remaining',\r\n  ],\r\n  \r\n  // Allow credentials (cookies, authorization headers)\r\n  credentials: true,\r\n  \r\n  // Preflight cache duration\r\n  maxAge: 86400, // 24 hours\r\n  \r\n  // Handle preflight success\r\n  optionsSuccessStatus: 204,\r\n};\r\n\r\napp.use(cors(corsOptions));\r\n\r\n// For specific routes with different CORS\r\napp.use('/api/public', cors({\r\n  origin: '*', // Public API - allow all\r\n  methods: ['GET'],\r\n}));\r\n```\r\n\r\n### 5. Python/FastAPI Headers\r\n\r\n```python\r\nfrom fastapi import FastAPI, Request, Response\r\nfrom fastapi.middleware.cors import CORSMiddleware\r\nfrom starlette.middleware.base import BaseHTTPMiddleware\r\n\r\napp = FastAPI()\r\n\r\n# Security headers middleware\r\nclass SecurityHeadersMiddleware(BaseHTTPMiddleware):\r\n    async def dispatch(self, request: Request, call_next):\r\n        response: Response = await call_next(request)\r\n        \r\n        # Content-Security-Policy\r\n        response.headers[\"Content-Security-Policy\"] = \"; \".join([\r\n            \"default-src 'self'\",\r\n            \"script-src 'self'\",\r\n            \"style-src 'self' 'unsafe-inline'\",\r\n            \"img-src 'self' data: https:\",\r\n            \"connect-src 'self' https://api.example.com\",\r\n            \"frame-ancestors 'none'\",\r\n            \"base-uri 'self'\",\r\n            \"form-action 'self'\",\r\n        ])\r\n        \r\n        # Strict-Transport-Security\r\n        response.headers[\"Strict-Transport-Security\"] = (\r\n            \"max-age=31536000; includeSubDomains; preload\"\r\n        )\r\n        \r\n        # Other security headers\r\n        response.headers[\"X-Content-Type-Options\"] = \"nosniff\"\r\n        response.headers[\"X-Frame-Options\"] = \"DENY\"\r\n        response.headers[\"Referrer-Policy\"] = \"strict-origin-when-cross-origin\"\r\n        response.headers[\"Permissions-Policy\"] = (\r\n            \"accelerometer=(), camera=(), geolocation=(), microphone=()\"\r\n        )\r\n        response.headers[\"Cross-Origin-Opener-Policy\"] = \"same-origin\"\r\n        response.headers[\"Cross-Origin-Resource-Policy\"] = \"same-origin\"\r\n        \r\n        # Remove server info\r\n        if \"server\" in response.headers:\r\n            del response.headers[\"server\"]\r\n        \r\n        return response\r\n\r\napp.add_middleware(SecurityHeadersMiddleware)\r\n\r\n# CORS\r\napp.add_middleware(\r\n    CORSMiddleware,\r\n    allow_origins=[\"https://app.example.com\"],\r\n    allow_credentials=True,\r\n    allow_methods=[\"GET\", \"POST\", \"PUT\", \"DELETE\"],\r\n    allow_headers=[\"Authorization\", \"Content-Type\", \"X-Correlation-ID\"],\r\n    expose_headers=[\"X-Correlation-ID\"],\r\n    max_age=86400,\r\n)\r\n```\r\n\r\n### 6. Nginx Configuration\r\n\r\n```nginx\r\n# /etc/nginx/conf.d/security-headers.conf\r\n\r\n# Content-Security-Policy\r\nadd_header Content-Security-Policy \"default-src 'self'; script-src 'self'; style-src 'self' 'unsafe-inline'; img-src 'self' data: https:; connect-src 'self' https://api.example.com; frame-ancestors 'none'; base-uri 'self'; form-action 'self';\" always;\r\n\r\n# Strict-Transport-Security\r\nadd_header Strict-Transport-Security \"max-age=31536000; includeSubDomains; preload\" always;\r\n\r\n# X-Content-Type-Options\r\nadd_header X-Content-Type-Options \"nosniff\" always;\r\n\r\n# X-Frame-Options\r\nadd_header X-Frame-Options \"DENY\" always;\r\n\r\n# Referrer-Policy\r\nadd_header Referrer-Policy \"strict-origin-when-cross-origin\" always;\r\n\r\n# Permissions-Policy\r\nadd_header Permissions-Policy \"accelerometer=(), camera=(), geolocation=(), microphone=()\" always;\r\n\r\n# Cross-Origin policies\r\nadd_header Cross-Origin-Opener-Policy \"same-origin\" always;\r\nadd_header Cross-Origin-Resource-Policy \"same-origin\" always;\r\nadd_header Cross-Origin-Embedder-Policy \"require-corp\" always;\r\n\r\n# Remove server info\r\nserver_tokens off;\r\nmore_clear_headers Server;\r\n\r\n# API-specific location\r\nlocation /api/ {\r\n    # Stricter CSP for API\r\n    add_header Content-Security-Policy \"default-src 'none'; frame-ancestors 'none';\" always;\r\n    \r\n    # No caching for API responses\r\n    add_header Cache-Control \"no-store, no-cache, must-revalidate, private\" always;\r\n    add_header Pragma \"no-cache\" always;\r\n    \r\n    # ... proxy settings\r\n}\r\n```\r\n\r\n### 7. Header Validation & Testing\r\n\r\n```typescript\r\n// Test your headers with this endpoint (dev only!)\r\napp.get('/debug/headers', (req, res) => {\r\n  if (process.env.NODE_ENV !== 'development') {\r\n    return res.status(404).send('Not found');\r\n  }\r\n  \r\n  // List all response headers that will be sent\r\n  const headers = {\r\n    sent: {}, // Will be populated after response\r\n    expected: {\r\n      'Content-Security-Policy': 'should contain default-src',\r\n      'Strict-Transport-Security': 'should have max-age >= 31536000',\r\n      'X-Content-Type-Options': 'nosniff',\r\n      'X-Frame-Options': 'DENY or SAMEORIGIN',\r\n      'Referrer-Policy': 'strict-origin-when-cross-origin',\r\n    },\r\n  };\r\n  \r\n  res.json(headers);\r\n});\r\n\r\n// Automated header check\r\nasync function checkSecurityHeaders(url: string): Promise<{\r\n  passed: string[];\r\n  failed: string[];\r\n  warnings: string[];\r\n}> {\r\n  const response = await fetch(url, { method: 'HEAD' });\r\n  const headers = response.headers;\r\n  \r\n  const checks = [\r\n    {\r\n      name: 'Content-Security-Policy',\r\n      check: () => headers.has('content-security-policy'),\r\n      severity: 'failed',\r\n    },\r\n    {\r\n      name: 'Strict-Transport-Security',\r\n      check: () => {\r\n        const hsts = headers.get('strict-transport-security');\r\n        if (!hsts) return false;\r\n        const maxAge = parseInt(hsts.match(/max-age=(\\d+)/)?.[1] || '0');\r\n        return maxAge >= 31536000;\r\n      },\r\n      severity: 'failed',\r\n    },\r\n    {\r\n      name: 'X-Content-Type-Options',\r\n      check: () => headers.get('x-content-type-options') === 'nosniff',\r\n      severity: 'failed',\r\n    },\r\n    {\r\n      name: 'X-Frame-Options',\r\n      check: () => ['DENY', 'SAMEORIGIN'].includes(\r\n        headers.get('x-frame-options')?.toUpperCase() || ''\r\n      ),\r\n      severity: 'warning', // CSP frame-ancestors is preferred\r\n    },\r\n    {\r\n      name: 'No X-Powered-By',\r\n      check: () => !headers.has('x-powered-by'),\r\n      severity: 'warning',\r\n    },\r\n    {\r\n      name: 'No Server version',\r\n      check: () => {\r\n        const server = headers.get('server');\r\n        return !server || !server.match(/\\d+\\.\\d+/);\r\n      },\r\n      severity: 'warning',\r\n    },\r\n  ];\r\n  \r\n  const results = { passed: [], failed: [], warnings: [] };\r\n  \r\n  for (const check of checks) {\r\n    if (check.check()) {\r\n      results.passed.push(check.name);\r\n    } else {\r\n      results[check.severity === 'failed' ? 'failed' : 'warnings'].push(check.name);\r\n    }\r\n  }\r\n  \r\n  return results;\r\n}\r\n```\r\n\r\n### 8. CSP Report-Only & Violation Reporting\r\n\r\n```typescript\r\n// Start with report-only to test CSP without breaking things\r\napp.use((req, res, next) => {\r\n  const csp = \"default-src 'self'; script-src 'self'; report-uri /api/csp-report\";\r\n  \r\n  // Report-only mode - violations reported but not enforced\r\n  res.setHeader('Content-Security-Policy-Report-Only', csp);\r\n  \r\n  // Once tested, switch to enforcing:\r\n  // res.setHeader('Content-Security-Policy', csp);\r\n  \r\n  next();\r\n});\r\n\r\n// CSP violation report endpoint\r\napp.post('/api/csp-report', express.json({ type: 'application/csp-report' }), (req, res) => {\r\n  const report = req.body['csp-report'];\r\n  \r\n  logger.warn({\r\n    event: 'CSP_VIOLATION',\r\n    documentUri: report['document-uri'],\r\n    violatedDirective: report['violated-directive'],\r\n    blockedUri: report['blocked-uri'],\r\n    sourceFile: report['source-file'],\r\n    lineNumber: report['line-number'],\r\n  });\r\n  \r\n  // Track violations in metrics\r\n  metrics.increment('security.csp_violation', {\r\n    directive: report['violated-directive'],\r\n  });\r\n  \r\n  res.status(204).send();\r\n});\r\n\r\n// Modern Reporting API (successor to report-uri)\r\napp.use((req, res, next) => {\r\n  res.setHeader('Report-To', JSON.stringify({\r\n    group: 'csp-endpoint',\r\n    max_age: 10886400,\r\n    endpoints: [\r\n      { url: 'https://api.example.com/reports/csp' }\r\n    ],\r\n  }));\r\n  \r\n  res.setHeader('Content-Security-Policy', \r\n    \"default-src 'self'; report-to csp-endpoint\"\r\n  );\r\n  \r\n  next();\r\n});\r\n```\r\n\r\n## Pitfalls\r\n\r\n| Pitfall | Impact | How to Avoid |\r\n|---------|--------|--------------|\r\n| No CSP | XSS attacks possible | Always configure CSP |\r\n| `unsafe-inline` in script-src | XSS via inline scripts | Use nonces or hashes |\r\n| `*` in CORS origin | Any site can call API | Explicit allowlist |\r\n| Missing HSTS | Downgrade attacks | Enable with preload |\r\n| Exposing server version | Targeted attacks | Remove/hide version |\r\n| Overly permissive CSP | Ineffective protection | Start strict, relax only if needed |\r\n| CORS credentials + `*` origin | Security error | Can't combine these |\r\n| Forgetting `always` in nginx | Headers missing on errors | Use `always` directive |\r\n\r\n## Checklist\r\n\r\n- [ ] Content-Security-Policy configured\r\n- [ ] CSP tested in report-only mode first\r\n- [ ] Strict-Transport-Security with long max-age\r\n- [ ] HSTS preload submitted (if applicable)\r\n- [ ] X-Content-Type-Options: nosniff\r\n- [ ] X-Frame-Options or CSP frame-ancestors\r\n- [ ] Referrer-Policy configured\r\n- [ ] Permissions-Policy restricts unused features\r\n- [ ] CORS origins explicitly whitelisted\r\n- [ ] CORS credentials properly configured\r\n- [ ] Server/X-Powered-By headers removed\r\n- [ ] Cache-Control for sensitive responses\r\n- [ ] Headers tested with securityheaders.com\r\n- [ ] CSP violations monitored\r\n\r\n## References\r\n\r\n- [OWASP HTTP Headers Cheat Sheet](https://cheatsheetseries.owasp.org/cheatsheets/HTTP_Headers_Cheat_Sheet.html)\r\n- [MDN Security Headers](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers#security)\r\n- [SecurityHeaders.com](https://securityheaders.com/)\r\n- [CSP Evaluator](https://csp-evaluator.withgoogle.com/)\r\n- [Helmet.js](https://helmetjs.github.io/)\r\n- [HSTS Preload](https://hstspreload.org/)\r\n"
  },
  {
    "id": "sec-authn-authz-boundary",
    "title": "Authentication vs Authorization Boundary",
    "tags": [
      "security",
      "authentication",
      "authorization",
      "rbac",
      "access-control"
    ],
    "level": "intermediate",
    "stacks": [
      "all"
    ],
    "scope": "security",
    "maturity": "stable",
    "works_with": [
      "all"
    ],
    "path": "D:/Code/backend-engineering-kit/.shared/production-backend-kit/patterns/sec.authn-authz-boundary.md",
    "type": "pattern",
    "sections": [],
    "checklist": [],
    "sources": [],
    "rawContent": "\r\n# Authentication vs Authorization Boundary\r\n\r\n## Problem\r\n\r\nConfusing authentication (who are you?) with authorization (what can you do?) leads to security holes, inconsistent access controls, and hard-to-maintain permission systems.\r\n\r\n## When to use\r\n\r\n- Any application with user accounts\r\n- Multi-tenant systems\r\n- APIs with different access levels\r\n- Role-based or attribute-based access\r\n- Resource-level permissions\r\n\r\n## Solution\r\n\r\n1. **Separate concerns clearly**\r\n   - Authentication: Verify identity (login, tokens)\r\n   - Authorization: Check permissions (roles, policies)\r\n   - Handle in separate middleware/layers\r\n\r\n2. **Authentication layer**\r\n   - Validate credentials (password, OAuth, SSO)\r\n   - Issue tokens (JWT, session)\r\n   - Attach identity to request context\r\n   - Return 401 Unauthorized if fails\r\n\r\n3. **Authorization layer**\r\n   - Extract permissions from identity\r\n   - Check against required permissions\r\n   - Return 403 Forbidden if denied\r\n   - Use consistent permission model\r\n\r\n4. **Choose authorization model**\r\n   - RBAC: Role-Based Access Control\r\n   - ABAC: Attribute-Based Access Control\r\n   - ReBAC: Relationship-Based Access Control\r\n\r\n## Pitfalls\r\n\r\n| Pitfall | How to Avoid |\r\n|---------|--------------|\r\n| Mixing 401 and 403 | 401 = who are you?, 403 = you can't do that |\r\n| Checking perms in business logic | Use middleware/decorators |\r\n| Hardcoding roles everywhere | Centralize permission checks |\r\n| Not logging access denials | Always log for security audit |\r\n| Over-privileged defaults | Default deny, explicit allow |\r\n\r\n## Checklist\r\n\r\n- [ ] Authentication and authorization are separate layers\r\n- [ ] 401 used for authentication failures\r\n- [ ] 403 used for authorization failures\r\n- [ ] Identity attached to request context after auth\r\n- [ ] Permissions checked before business logic\r\n- [ ] Role/permission model documented\r\n- [ ] Default-deny policy enforced\r\n- [ ] Access denials logged with context\r\n- [ ] Admin functions protected separately\r\n- [ ] Permission changes audited\r\n\r\n## Snippets (Generic)\r\n\r\n```\r\nRequest Flow:\r\n1. Request arrives\r\n2. Authentication middleware:\r\n   - Extract token from header\r\n   - Verify token signature & expiry\r\n   - Attach user identity to context\r\n   - If invalid → 401 Unauthorized\r\n3. Authorization middleware:\r\n   - Extract required permissions for route\r\n   - Check user roles/permissions\r\n   - If denied → 403 Forbidden\r\n4. Business logic executes\r\n\r\nHTTP Status Codes:\r\n- 401 Unauthorized: \"I don't know who you are\"\r\n  - Missing token, invalid token, expired token\r\n- 403 Forbidden: \"I know who you are, but no access\"\r\n  - Valid user, insufficient permissions\r\n\r\nRBAC Model:\r\nUser → Roles → Permissions\r\nExample: user.roles = ['editor']\r\n         editor.permissions = ['read', 'write']\r\n         admin.permissions = ['read', 'write', 'delete']\r\n\r\nMiddleware Pattern:\r\n@authenticate  # First: verify identity\r\n@authorize('write')  # Second: check permission\r\ndef update_article(id):\r\n  # Business logic here\r\n```\r\n\r\n## Sources\r\n\r\n- OWASP Authentication Cheat Sheet: https://cheatsheetseries.owasp.org/cheatsheets/Authentication_Cheat_Sheet.html\r\n- OWASP Authorization Cheat Sheet: https://cheatsheetseries.owasp.org/cheatsheets/Authorization_Cheat_Sheet.html\r\n- NIST RBAC Model: https://csrc.nist.gov/projects/role-based-access-control\r\n- Google Zanzibar Paper: https://research.google/pubs/pub48190/\r\n"
  },
  {
    "id": "sec.billing-integration",
    "title": "Billing/Stripe Integration Lite",
    "tags": [
      "security",
      "billing",
      "stripe",
      "webhooks"
    ],
    "level": "intermediate",
    "stacks": [
      "nodejs",
      "python",
      "go",
      "all"
    ],
    "scope": "security",
    "maturity": "stable",
    "works_with": [
      "nodejs",
      "python",
      "go",
      "all"
    ],
    "path": "D:/Code/backend-engineering-kit/.shared/production-backend-kit/patterns/sec.billing-integration.md",
    "type": "pattern",
    "sections": [],
    "checklist": [],
    "sources": [],
    "rawContent": "\r\n# Billing/Stripe Integration Lite\r\n\r\n## Problem\r\n\r\nSaaS billing requires secure webhook handling, proper signature verification, and idempotent processing to avoid double-charges or missed events.\r\n\r\n## When to use\r\n\r\n- Integrating Stripe (or similar) for subscriptions\r\n- Need webhook processing for payment events\r\n- Want to sync billing state with app database\r\n- Require idempotent payment processing\r\n\r\n## Solution\r\n\r\n### 1. Webhook Signature Verification\r\n\r\n```typescript\r\nimport Stripe from 'stripe';\r\n\r\nconst stripe = new Stripe(process.env.STRIPE_SECRET_KEY!);\r\nconst webhookSecret = process.env.STRIPE_WEBHOOK_SECRET!;\r\n\r\napp.post('/api/webhooks/stripe',\r\n  express.raw({ type: 'application/json' }), // Raw body required!\r\n  async (req, res) => {\r\n    const signature = req.headers['stripe-signature'] as string;\r\n    \r\n    let event: Stripe.Event;\r\n    try {\r\n      event = stripe.webhooks.constructEvent(\r\n        req.body,\r\n        signature,\r\n        webhookSecret\r\n      );\r\n    } catch (err) {\r\n      logger.warn({ event: 'WEBHOOK_SIGNATURE_FAILED', error: err.message });\r\n      return res.status(400).send('Webhook signature verification failed');\r\n    }\r\n    \r\n    // Process event\r\n    await handleStripeEvent(event);\r\n    \r\n    res.json({ received: true });\r\n  }\r\n);\r\n```\r\n\r\n### 2. Idempotent Event Processing\r\n\r\n```typescript\r\nasync function handleStripeEvent(event: Stripe.Event) {\r\n  // Check if already processed\r\n  const existing = await db.webhookEvents.findOne({ stripeEventId: event.id });\r\n  if (existing) {\r\n    logger.info({ event: 'WEBHOOK_DUPLICATE', stripeEventId: event.id });\r\n    return; // Already processed\r\n  }\r\n  \r\n  // Store event first (with processing status)\r\n  await db.webhookEvents.create({\r\n    stripeEventId: event.id,\r\n    type: event.type,\r\n    data: event.data,\r\n    status: 'processing',\r\n    receivedAt: new Date(),\r\n  });\r\n  \r\n  try {\r\n    switch (event.type) {\r\n      case 'checkout.session.completed':\r\n        await handleCheckoutComplete(event.data.object);\r\n        break;\r\n      case 'customer.subscription.updated':\r\n        await handleSubscriptionUpdate(event.data.object);\r\n        break;\r\n      case 'customer.subscription.deleted':\r\n        await handleSubscriptionCanceled(event.data.object);\r\n        break;\r\n      case 'invoice.payment_failed':\r\n        await handlePaymentFailed(event.data.object);\r\n        break;\r\n    }\r\n    \r\n    await db.webhookEvents.update(\r\n      { stripeEventId: event.id },\r\n      { status: 'completed', processedAt: new Date() }\r\n    );\r\n  } catch (error) {\r\n    await db.webhookEvents.update(\r\n      { stripeEventId: event.id },\r\n      { status: 'failed', error: error.message }\r\n    );\r\n    throw error; // Stripe will retry\r\n  }\r\n}\r\n```\r\n\r\n### 3. Subscription State Sync\r\n\r\n```typescript\r\nasync function handleSubscriptionUpdate(subscription: Stripe.Subscription) {\r\n  const customerId = subscription.customer as string;\r\n  \r\n  // Find tenant by Stripe customer ID\r\n  const tenant = await db.tenants.findOne({ stripeCustomerId: customerId });\r\n  if (!tenant) {\r\n    logger.error({ event: 'TENANT_NOT_FOUND', customerId });\r\n    return;\r\n  }\r\n  \r\n  // Map Stripe status to app status\r\n  const planStatus = mapSubscriptionStatus(subscription.status);\r\n  const planName = getPlanFromPriceId(subscription.items.data[0]?.price.id);\r\n  \r\n  await db.tenants.update(\r\n    { id: tenant.id },\r\n    {\r\n      plan: planName,\r\n      planStatus,\r\n      subscriptionId: subscription.id,\r\n      currentPeriodEnd: new Date(subscription.current_period_end * 1000),\r\n    }\r\n  );\r\n  \r\n  logger.info({\r\n    event: 'SUBSCRIPTION_UPDATED',\r\n    tenantId: tenant.id,\r\n    plan: planName,\r\n    status: planStatus,\r\n  });\r\n}\r\n\r\nfunction mapSubscriptionStatus(stripeStatus: string): string {\r\n  const mapping: Record<string, string> = {\r\n    'active': 'active',\r\n    'trialing': 'trial',\r\n    'past_due': 'past_due',\r\n    'canceled': 'canceled',\r\n    'unpaid': 'suspended',\r\n  };\r\n  return mapping[stripeStatus] || 'unknown';\r\n}\r\n```\r\n\r\n### 4. Creating Checkout Sessions\r\n\r\n```typescript\r\nasync function createCheckoutSession(\r\n  tenantId: string,\r\n  priceId: string,\r\n  successUrl: string,\r\n  cancelUrl: string\r\n): Promise<string> {\r\n  const tenant = await db.tenants.findById(tenantId);\r\n  \r\n  // Create or reuse Stripe customer\r\n  let customerId = tenant.stripeCustomerId;\r\n  if (!customerId) {\r\n    const customer = await stripe.customers.create({\r\n      email: tenant.billingEmail,\r\n      metadata: { tenantId },\r\n    });\r\n    customerId = customer.id;\r\n    await db.tenants.update({ id: tenantId }, { stripeCustomerId: customerId });\r\n  }\r\n  \r\n  const session = await stripe.checkout.sessions.create({\r\n    customer: customerId,\r\n    line_items: [{ price: priceId, quantity: 1 }],\r\n    mode: 'subscription',\r\n    success_url: successUrl,\r\n    cancel_url: cancelUrl,\r\n    metadata: { tenantId }, // For webhook processing\r\n  });\r\n  \r\n  return session.url!;\r\n}\r\n```\r\n\r\n### 5. Handling Payment Failures\r\n\r\n```typescript\r\nasync function handlePaymentFailed(invoice: Stripe.Invoice) {\r\n  const customerId = invoice.customer as string;\r\n  const tenant = await db.tenants.findOne({ stripeCustomerId: customerId });\r\n  \r\n  if (!tenant) return;\r\n  \r\n  // Update tenant status\r\n  await db.tenants.update(\r\n    { id: tenant.id },\r\n    { planStatus: 'payment_failed' }\r\n  );\r\n  \r\n  // Notify tenant admins\r\n  const admins = await db.users.find({ \r\n    tenantId: tenant.id, \r\n    role: 'admin' \r\n  });\r\n  \r\n  for (const admin of admins) {\r\n    await emailService.send({\r\n      to: admin.email,\r\n      template: 'payment-failed',\r\n      data: {\r\n        tenantName: tenant.name,\r\n        amount: invoice.amount_due / 100,\r\n        currency: invoice.currency.toUpperCase(),\r\n        updatePaymentUrl: `/billing/update-payment`,\r\n      }\r\n    });\r\n  }\r\n}\r\n```\r\n\r\n## Pitfalls\r\n\r\n1. **Not verifying signatures**: Critical security vulnerability\r\n2. **Parsing body as JSON before Stripe**: Need raw body for verification\r\n3. **Not handling retries**: Events may be sent multiple times\r\n4. **Blocking webhook response**: Process async, respond quickly\r\n\r\n## Checklist\r\n\r\n- [ ] Webhook signature verification in place\r\n- [ ] Idempotency using event ID tracking\r\n- [ ] All relevant event types handled\r\n- [ ] Subscription state synced to database\r\n- [ ] Payment failure notifications sent\r\n- [ ] Webhook endpoint returns quickly (< 5s)\r\n- [ ] Failed events are retried or logged\r\n"
  },
  {
    "id": "sec.data-isolation",
    "title": "Data Isolation Strategies",
    "tags": [
      "security",
      "multitenancy",
      "database",
      "rls"
    ],
    "level": "advanced",
    "stacks": [
      "nodejs",
      "python",
      "go",
      "all"
    ],
    "scope": "security",
    "maturity": "stable",
    "works_with": [
      "nodejs",
      "python",
      "go",
      "all"
    ],
    "path": "D:/Code/backend-engineering-kit/.shared/production-backend-kit/patterns/sec.data-isolation.md",
    "type": "pattern",
    "sections": [],
    "checklist": [],
    "sources": [],
    "rawContent": "\r\n# Data Isolation Strategies\r\n\r\n## Problem\r\n\r\nIn shared-database multi-tenancy, preventing data leaks between tenants is critical. A single missing WHERE clause can expose customer data.\r\n\r\n## When to use\r\n\r\n- Shared database with tenant_id column approach\r\n- Need defense-in-depth for data isolation\r\n- Want ORM-level or database-level enforcement\r\n- Compliance requirements for data separation\r\n\r\n## Solution\r\n\r\n### 1. Application-Level: ORM Middleware\r\n\r\n```typescript\r\n// Prisma middleware example\r\nprisma.$use(async (params, next) => {\r\n  const tenantId = getTenantId();\r\n  \r\n  // Tables that require tenant scoping\r\n  const tenantTables = ['User', 'Order', 'Invoice', 'Project'];\r\n  \r\n  if (tenantTables.includes(params.model!)) {\r\n    // Inject tenant_id into all queries\r\n    if (params.action === 'findMany' || params.action === 'findFirst') {\r\n      params.args.where = { ...params.args.where, tenant_id: tenantId };\r\n    }\r\n    if (params.action === 'create') {\r\n      params.args.data.tenant_id = tenantId;\r\n    }\r\n    if (params.action === 'update' || params.action === 'delete') {\r\n      params.args.where = { ...params.args.where, tenant_id: tenantId };\r\n    }\r\n  }\r\n  \r\n  return next(params);\r\n});\r\n```\r\n\r\n### 2. Database-Level: Row-Level Security (PostgreSQL)\r\n\r\n```sql\r\n-- Enable RLS on table\r\nALTER TABLE orders ENABLE ROW LEVEL SECURITY;\r\n\r\n-- Create policy\r\nCREATE POLICY tenant_isolation ON orders\r\n  USING (tenant_id = current_setting('app.current_tenant')::uuid);\r\n\r\n-- Set tenant context before queries\r\nSET app.current_tenant = 'tenant-uuid-here';\r\n\r\n-- Now all queries are automatically filtered\r\nSELECT * FROM orders; -- Only returns rows for current tenant\r\n```\r\n\r\n### 3. Connection-Level Tenant Setting\r\n\r\n```typescript\r\n// Set tenant on each request\r\nasync function executeWithTenant<T>(\r\n  tenantId: string,\r\n  fn: () => Promise<T>\r\n): Promise<T> {\r\n  // Set PostgreSQL session variable\r\n  await db.query(`SET app.current_tenant = $1`, [tenantId]);\r\n  \r\n  try {\r\n    return await fn();\r\n  } finally {\r\n    // Reset for connection pool safety\r\n    await db.query(`RESET app.current_tenant`);\r\n  }\r\n}\r\n\r\n// Usage in middleware\r\napp.use(async (req, res, next) => {\r\n  await executeWithTenant(req.tenant.id, () => {\r\n    return next();\r\n  });\r\n});\r\n```\r\n\r\n### 4. Foreign Key Constraints\r\n\r\n```sql\r\n-- Ensure referential integrity includes tenant\r\nCREATE TABLE orders (\r\n  id UUID PRIMARY KEY,\r\n  tenant_id UUID NOT NULL REFERENCES tenants(id),\r\n  user_id UUID NOT NULL,\r\n  -- Composite foreign key ensures user belongs to same tenant\r\n  FOREIGN KEY (tenant_id, user_id) \r\n    REFERENCES users(tenant_id, id)\r\n);\r\n\r\n-- Composite index for performance\r\nCREATE INDEX idx_orders_tenant_user ON orders(tenant_id, user_id);\r\n```\r\n\r\n### 5. Audit Trail\r\n\r\n```typescript\r\n// Log all cross-tenant access attempts\r\nfunction logAccessAttempt(\r\n  action: string,\r\n  requestedTenantId: string,\r\n  actualTenantId: string\r\n) {\r\n  if (requestedTenantId !== actualTenantId) {\r\n    logger.warn({\r\n      event: 'CROSS_TENANT_ACCESS_ATTEMPT',\r\n      action,\r\n      requestedTenantId,\r\n      actualTenantId,\r\n      userId: getContext()?.userId,\r\n    });\r\n    \r\n    // Alert security team for repeated attempts\r\n    alertService.checkAndNotify('cross-tenant-access', actualTenantId);\r\n  }\r\n}\r\n```\r\n\r\n## Migration Strategy\r\n\r\nFor existing single-tenant apps moving to multi-tenant:\r\n\r\n```sql\r\n-- 1. Add tenant_id column\r\nALTER TABLE users ADD COLUMN tenant_id UUID;\r\n\r\n-- 2. Backfill with default tenant\r\nUPDATE users SET tenant_id = 'default-tenant-uuid' WHERE tenant_id IS NULL;\r\n\r\n-- 3. Make NOT NULL\r\nALTER TABLE users ALTER COLUMN tenant_id SET NOT NULL;\r\n\r\n-- 4. Add index\r\nCREATE INDEX idx_users_tenant ON users(tenant_id);\r\n\r\n-- 5. Enable RLS\r\nALTER TABLE users ENABLE ROW LEVEL SECURITY;\r\nCREATE POLICY tenant_isolation ON users USING (tenant_id = current_setting('app.current_tenant')::uuid);\r\n```\r\n\r\n## Pitfalls\r\n\r\n1. **RLS bypassed by superusers**: Use non-superuser roles for app\r\n2. **Missing indexes**: Every table with tenant_id needs index\r\n3. **JOIN without tenant filter**: Cross-table queries can leak\r\n4. **Connection pool contamination**: Reset tenant settings\r\n\r\n## Checklist\r\n\r\n- [ ] Every tenant-scoped table has tenant_id column\r\n- [ ] All tenant_id columns have indexes\r\n- [ ] ORM middleware or RLS policies in place\r\n- [ ] Composite foreign keys where applicable\r\n- [ ] Audit logging for access attempts\r\n- [ ] Tests verify cross-tenant queries fail\r\n"
  },
  {
    "id": "api-graphql-security",
    "title": "GraphQL Security Best Practices",
    "tags": [
      "security",
      "graphql",
      "api",
      "query-complexity",
      "depth-limit"
    ],
    "level": "intermediate",
    "stacks": [
      "graphql",
      "apollo",
      "nodejs"
    ],
    "scope": "api",
    "maturity": "stable",
    "works_with": [
      "graphql",
      "apollo",
      "nodejs"
    ],
    "path": "D:/Code/backend-engineering-kit/.shared/production-backend-kit/patterns/sec.graphql-security.md",
    "type": "pattern",
    "sections": [],
    "checklist": [],
    "sources": [],
    "rawContent": "\r\n# GraphQL Security Best Practices\r\n\r\n## Problem\r\n\r\nGraphQL introduces unique security challenges:\r\n- **Denial of Service**: Deeply nested or complex queries can overwhelm servers\r\n- **Information Disclosure**: Introspection exposes entire API schema\r\n- **Batching Attacks**: Multiple operations in single request\r\n- **Injection**: Query parameters can be exploited\r\n- **Authorization Complexity**: Field-level permissions are harder\r\n\r\nREST comparison:\r\n```\r\nREST:  GET /users/123         → 1 endpoint, predictable load\r\nGraphQL: query { users { posts { comments { ... } } } } → Unbounded complexity\r\n```\r\n\r\n## When to use\r\n\r\n- Any production GraphQL API\r\n- Public-facing GraphQL endpoints\r\n- APIs with sensitive data\r\n- Multi-tenant GraphQL services\r\n\r\n## Solution\r\n\r\n### 1. Query Depth Limiting\r\n\r\n```typescript\r\nimport { ApolloServer } from '@apollo/server';\r\nimport depthLimit from 'graphql-depth-limit';\r\n\r\nconst server = new ApolloServer({\r\n  typeDefs,\r\n  resolvers,\r\n  validationRules: [\r\n    depthLimit(5), // Max depth of 5 levels\r\n  ],\r\n});\r\n\r\n// What gets blocked:\r\n// query {\r\n//   user {           // 1\r\n//     posts {        // 2\r\n//       comments {   // 3\r\n//         author {   // 4\r\n//           posts {  // 5\r\n//             comments { // 6 → BLOCKED\r\n//             }\r\n//           }\r\n//         }\r\n//       }\r\n//     }\r\n//   }\r\n// }\r\n```\r\n\r\n### 2. Query Complexity Analysis\r\n\r\n```typescript\r\nimport { createComplexityRule, simpleEstimator, fieldExtensionsEstimator } from 'graphql-query-complexity';\r\n\r\nconst complexityRule = createComplexityRule({\r\n  maximumComplexity: 1000,\r\n  \r\n  estimators: [\r\n    // Use @complexity directive from schema\r\n    fieldExtensionsEstimator(),\r\n    \r\n    // Default: 1 point per field, multiplied by list size\r\n    simpleEstimator({ defaultComplexity: 1 }),\r\n  ],\r\n  \r\n  onComplete: (complexity: number) => {\r\n    console.log('Query Complexity:', complexity);\r\n  },\r\n  \r\n  createError: (max: number, actual: number) => {\r\n    return new GraphQLError(\r\n      `Query too complex: ${actual} > ${max} maximum allowed complexity`\r\n    );\r\n  },\r\n});\r\n\r\n// Schema with complexity hints\r\nconst typeDefs = gql`\r\n  type Query {\r\n    user(id: ID!): User @complexity(value: 1)\r\n    \r\n    users(\r\n      first: Int = 10\r\n      after: String\r\n    ): UserConnection @complexity(\r\n      value: 1\r\n      multipliers: [\"first\"]  # 1 * first\r\n    )\r\n    \r\n    searchUsers(query: String!): [User!]! @complexity(\r\n      value: 10  # Expensive operation\r\n      multipliers: [\"first\"]\r\n    )\r\n  }\r\n  \r\n  type User {\r\n    id: ID!\r\n    name: String!\r\n    email: String! @complexity(value: 1)\r\n    \r\n    # N+1 potential - higher cost\r\n    posts(first: Int = 10): [Post!]! @complexity(\r\n      value: 5\r\n      multipliers: [\"first\"]\r\n    )\r\n  }\r\n`;\r\n```\r\n\r\n### 3. Disable Introspection in Production\r\n\r\n```typescript\r\nimport { ApolloServer } from '@apollo/server';\r\nimport { ApolloServerPluginInlineTraceDisabled } from '@apollo/server/plugin/disabled';\r\n\r\nconst server = new ApolloServer({\r\n  typeDefs,\r\n  resolvers,\r\n  \r\n  // Disable introspection in production\r\n  introspection: process.env.NODE_ENV !== 'production',\r\n  \r\n  plugins: [\r\n    // Also disable inline tracing\r\n    ApolloServerPluginInlineTraceDisabled(),\r\n    \r\n    // Custom plugin to block introspection\r\n    {\r\n      async requestDidStart() {\r\n        return {\r\n          async didResolveOperation({ request, document }) {\r\n            // Check if query contains __schema or __type\r\n            const isIntrospection = request.operationName === 'IntrospectionQuery';\r\n            \r\n            if (isIntrospection && process.env.NODE_ENV === 'production') {\r\n              throw new GraphQLError('Introspection disabled in production');\r\n            }\r\n          },\r\n        };\r\n      },\r\n    },\r\n  ],\r\n});\r\n```\r\n\r\n### 4. Rate Limiting for GraphQL\r\n\r\n```typescript\r\nimport { GraphQLError } from 'graphql';\r\n\r\ninterface RateLimitContext {\r\n  ip: string;\r\n  userId?: string;\r\n}\r\n\r\n// Complexity-based rate limiting\r\nclass GraphQLRateLimiter {\r\n  private limits = new Map<string, { points: number; resetAt: number }>();\r\n  \r\n  private config = {\r\n    maxComplexityPerMinute: 10000,\r\n    maxQueriesPerMinute: 100,\r\n    windowMs: 60000,\r\n  };\r\n\r\n  async checkLimit(\r\n    context: RateLimitContext,\r\n    complexity: number\r\n  ): Promise<void> {\r\n    const key = context.userId || context.ip;\r\n    const now = Date.now();\r\n    \r\n    let bucket = this.limits.get(key);\r\n    \r\n    if (!bucket || bucket.resetAt < now) {\r\n      bucket = { points: 0, resetAt: now + this.config.windowMs };\r\n      this.limits.set(key, bucket);\r\n    }\r\n    \r\n    bucket.points += complexity;\r\n    \r\n    if (bucket.points > this.config.maxComplexityPerMinute) {\r\n      throw new GraphQLError('Rate limit exceeded', {\r\n        extensions: {\r\n          code: 'RATE_LIMITED',\r\n          retryAfter: Math.ceil((bucket.resetAt - now) / 1000),\r\n        },\r\n      });\r\n    }\r\n  }\r\n}\r\n\r\n// Apollo Server plugin\r\nconst rateLimitPlugin = {\r\n  async requestDidStart({ contextValue }) {\r\n    return {\r\n      async didResolveOperation({ request, document, contextValue }) {\r\n        const complexity = calculateComplexity(document);\r\n        await rateLimiter.checkLimit(\r\n          { ip: contextValue.ip, userId: contextValue.userId },\r\n          complexity\r\n        );\r\n      },\r\n    };\r\n  },\r\n};\r\n```\r\n\r\n### 5. Field-Level Authorization\r\n\r\n```typescript\r\nimport { mapSchema, getDirective, MapperKind } from '@graphql-tools/utils';\r\nimport { defaultFieldResolver, GraphQLSchema } from 'graphql';\r\n\r\n// Schema with auth directives\r\nconst typeDefs = gql`\r\n  directive @auth(requires: Role!) on FIELD_DEFINITION\r\n  directive @owner on FIELD_DEFINITION\r\n  \r\n  enum Role {\r\n    ADMIN\r\n    USER\r\n    GUEST\r\n  }\r\n  \r\n  type User {\r\n    id: ID!\r\n    name: String!\r\n    email: String! @auth(requires: USER) @owner\r\n    role: Role! @auth(requires: ADMIN)\r\n    \r\n    privateData: PrivateData @auth(requires: ADMIN)\r\n  }\r\n  \r\n  type Query {\r\n    me: User @auth(requires: USER)\r\n    users: [User!]! @auth(requires: ADMIN)\r\n  }\r\n`;\r\n\r\n// Transform schema to add authorization\r\nfunction authDirectiveTransformer(schema: GraphQLSchema): GraphQLSchema {\r\n  return mapSchema(schema, {\r\n    [MapperKind.OBJECT_FIELD]: (fieldConfig) => {\r\n      const authDirective = getDirective(schema, fieldConfig, 'auth')?.[0];\r\n      const ownerDirective = getDirective(schema, fieldConfig, 'owner')?.[0];\r\n      \r\n      if (authDirective || ownerDirective) {\r\n        const { resolve = defaultFieldResolver } = fieldConfig;\r\n        \r\n        fieldConfig.resolve = async (source, args, context, info) => {\r\n          // Check role-based auth\r\n          if (authDirective) {\r\n            const requiredRole = authDirective.requires;\r\n            if (!hasRole(context.user, requiredRole)) {\r\n              throw new GraphQLError('Not authorized', {\r\n                extensions: { code: 'FORBIDDEN' },\r\n              });\r\n            }\r\n          }\r\n          \r\n          // Check ownership\r\n          if (ownerDirective && source) {\r\n            const resourceOwnerId = source.userId || source.id;\r\n            if (resourceOwnerId !== context.user?.id) {\r\n              // Return null instead of error for non-owners\r\n              return null;\r\n            }\r\n          }\r\n          \r\n          return resolve(source, args, context, info);\r\n        };\r\n      }\r\n      \r\n      return fieldConfig;\r\n    },\r\n  });\r\n}\r\n\r\nfunction hasRole(user: User | null, requiredRole: string): boolean {\r\n  if (!user) return false;\r\n  \r\n  const roleHierarchy = { ADMIN: 3, USER: 2, GUEST: 1 };\r\n  return roleHierarchy[user.role] >= roleHierarchy[requiredRole];\r\n}\r\n```\r\n\r\n### 6. Input Validation\r\n\r\n```typescript\r\nimport { GraphQLError } from 'graphql';\r\nimport { z } from 'zod';\r\n\r\n// Zod schemas for validation\r\nconst CreateUserInput = z.object({\r\n  email: z.string().email().max(255),\r\n  name: z.string().min(1).max(100).regex(/^[a-zA-Z\\s]+$/),\r\n  password: z.string().min(8).max(72),\r\n});\r\n\r\nconst SearchInput = z.object({\r\n  query: z.string().min(1).max(100),\r\n  first: z.number().int().min(1).max(100).default(10),\r\n  after: z.string().optional(),\r\n});\r\n\r\n// Resolver with validation\r\nconst resolvers = {\r\n  Mutation: {\r\n    createUser: async (_, { input }, context) => {\r\n      // Validate input\r\n      const result = CreateUserInput.safeParse(input);\r\n      \r\n      if (!result.success) {\r\n        throw new GraphQLError('Invalid input', {\r\n          extensions: {\r\n            code: 'BAD_USER_INPUT',\r\n            validationErrors: result.error.flatten().fieldErrors,\r\n          },\r\n        });\r\n      }\r\n      \r\n      // Sanitize - remove potential SQL/NoSQL injection\r\n      const sanitizedInput = {\r\n        ...result.data,\r\n        email: result.data.email.toLowerCase().trim(),\r\n        name: result.data.name.trim(),\r\n      };\r\n      \r\n      return userService.create(sanitizedInput);\r\n    },\r\n  },\r\n  \r\n  Query: {\r\n    searchUsers: async (_, args, context) => {\r\n      const result = SearchInput.safeParse(args);\r\n      \r\n      if (!result.success) {\r\n        throw new GraphQLError('Invalid search parameters', {\r\n          extensions: { code: 'BAD_USER_INPUT' },\r\n        });\r\n      }\r\n      \r\n      // Escape special characters for search\r\n      const safeQuery = escapeSearchQuery(result.data.query);\r\n      \r\n      return userService.search(safeQuery, result.data.first, result.data.after);\r\n    },\r\n  },\r\n};\r\n\r\n// Escape regex/search special chars\r\nfunction escapeSearchQuery(query: string): string {\r\n  return query.replace(/[.*+?^${}()|[\\]\\\\]/g, '\\\\$&');\r\n}\r\n```\r\n\r\n### 7. Batching Attack Prevention\r\n\r\n```typescript\r\nimport { ApolloServer } from '@apollo/server';\r\n\r\n// Limit batched operations\r\nconst server = new ApolloServer({\r\n  typeDefs,\r\n  resolvers,\r\n  \r\n  // Disable batching entirely\r\n  allowBatchedHttpRequests: false,\r\n  \r\n  // Or limit batch size via plugin\r\n  plugins: [{\r\n    async requestDidStart() {\r\n      return {\r\n        async parsingDidStart({ request }) {\r\n          // Check for batched queries\r\n          if (Array.isArray(request.http?.body)) {\r\n            const batchSize = request.http.body.length;\r\n            if (batchSize > 5) {\r\n              throw new GraphQLError(\r\n                `Batch size ${batchSize} exceeds maximum of 5`\r\n              );\r\n            }\r\n          }\r\n        },\r\n      };\r\n    },\r\n  }],\r\n});\r\n\r\n// Prevent alias-based batching attacks\r\n// Attacker could do: query { a1: user(id: 1) a2: user(id: 2) ... a1000: user(id: 1000) }\r\nimport { parse, visit } from 'graphql';\r\n\r\nfunction limitAliases(query: string, maxAliases: number = 10): void {\r\n  const document = parse(query);\r\n  let aliasCount = 0;\r\n  \r\n  visit(document, {\r\n    Field(node) {\r\n      if (node.alias) {\r\n        aliasCount++;\r\n        if (aliasCount > maxAliases) {\r\n          throw new GraphQLError(`Too many aliases: max ${maxAliases} allowed`);\r\n        }\r\n      }\r\n    },\r\n  });\r\n}\r\n```\r\n\r\n### 8. Persisted Queries (APQ)\r\n\r\n```typescript\r\nimport { ApolloServer } from '@apollo/server';\r\nimport { \r\n  ApolloServerPluginCacheControl,\r\n  ApolloServerPluginLandingPageDisabled,\r\n} from '@apollo/server/plugin/disabled';\r\nimport { createHash } from 'crypto';\r\n\r\n// Persisted query store\r\nconst queryStore = new Map<string, string>();\r\n\r\n// Pre-register allowed queries\r\nconst allowedQueries = [\r\n  `query GetUser($id: ID!) { user(id: $id) { id name email } }`,\r\n  `query ListUsers { users { id name } }`,\r\n  `mutation CreateUser($input: CreateUserInput!) { createUser(input: $input) { id } }`,\r\n];\r\n\r\n// Hash and store queries\r\nallowedQueries.forEach(query => {\r\n  const hash = createHash('sha256').update(query).digest('hex');\r\n  queryStore.set(hash, query);\r\n});\r\n\r\n// Plugin to enforce persisted queries\r\nconst persistedQueriesPlugin = {\r\n  async requestDidStart({ request }) {\r\n    const extensions = request.extensions as { persistedQuery?: { sha256Hash: string } };\r\n    \r\n    // In production, ONLY allow persisted queries\r\n    if (process.env.NODE_ENV === 'production') {\r\n      if (!extensions?.persistedQuery) {\r\n        throw new GraphQLError('Only persisted queries allowed in production');\r\n      }\r\n      \r\n      const hash = extensions.persistedQuery.sha256Hash;\r\n      const query = queryStore.get(hash);\r\n      \r\n      if (!query) {\r\n        throw new GraphQLError('Unknown persisted query', {\r\n          extensions: { code: 'PERSISTED_QUERY_NOT_FOUND' },\r\n        });\r\n      }\r\n      \r\n      // Replace request query with stored query\r\n      request.query = query;\r\n    }\r\n    \r\n    return {};\r\n  },\r\n};\r\n\r\n// Apollo Client with APQ\r\n// import { createPersistedQueryLink } from '@apollo/client/link/persisted-queries';\r\n// import { sha256 } from 'crypto-hash';\r\n//\r\n// const link = createPersistedQueryLink({ sha256 }).concat(httpLink);\r\n```\r\n\r\n### 9. Query Cost Analysis with DataLoader\r\n\r\n```typescript\r\nimport DataLoader from 'dataloader';\r\n\r\n// Prevent N+1 AND track costs\r\nclass CostAwareDataLoader<K, V> {\r\n  private loader: DataLoader<K, V>;\r\n  private loadCount = 0;\r\n  private maxLoads: number;\r\n\r\n  constructor(\r\n    batchFn: (keys: readonly K[]) => Promise<(V | Error)[]>,\r\n    maxLoads: number = 100\r\n  ) {\r\n    this.maxLoads = maxLoads;\r\n    this.loader = new DataLoader(async (keys) => {\r\n      this.loadCount += keys.length;\r\n      \r\n      if (this.loadCount > this.maxLoads) {\r\n        throw new GraphQLError(\r\n          `Too many database loads: ${this.loadCount} > ${this.maxLoads}`,\r\n          { extensions: { code: 'QUERY_TOO_EXPENSIVE' } }\r\n        );\r\n      }\r\n      \r\n      return batchFn(keys);\r\n    });\r\n  }\r\n\r\n  load(key: K): Promise<V> {\r\n    return this.loader.load(key);\r\n  }\r\n\r\n  loadMany(keys: K[]): Promise<(V | Error)[]> {\r\n    return this.loader.loadMany(keys);\r\n  }\r\n}\r\n\r\n// Context factory\r\nfunction createContext({ req }): GraphQLContext {\r\n  return {\r\n    user: req.user,\r\n    loaders: {\r\n      user: new CostAwareDataLoader(\r\n        async (ids) => userService.findByIds(ids),\r\n        100\r\n      ),\r\n      post: new CostAwareDataLoader(\r\n        async (ids) => postService.findByIds(ids),\r\n        200\r\n      ),\r\n    },\r\n  };\r\n}\r\n```\r\n\r\n### 10. Logging and Monitoring\r\n\r\n```typescript\r\nconst auditPlugin = {\r\n  async requestDidStart({ request, contextValue }) {\r\n    const startTime = Date.now();\r\n    \r\n    return {\r\n      async didResolveOperation({ request, document, operationName }) {\r\n        // Log operation details\r\n        logger.info({\r\n          type: 'graphql_operation',\r\n          operationName,\r\n          query: request.query,\r\n          variables: sanitizeVariables(request.variables),\r\n          userId: contextValue.user?.id,\r\n          ip: contextValue.ip,\r\n        });\r\n      },\r\n      \r\n      async willSendResponse({ response }) {\r\n        const duration = Date.now() - startTime;\r\n        \r\n        metrics.histogram('graphql.request.duration', duration, {\r\n          operationName: request.operationName,\r\n          hasErrors: response.body.singleResult?.errors ? 'true' : 'false',\r\n        });\r\n        \r\n        // Alert on slow queries\r\n        if (duration > 5000) {\r\n          logger.warn({\r\n            type: 'slow_graphql_query',\r\n            duration,\r\n            operationName: request.operationName,\r\n            query: request.query,\r\n          });\r\n        }\r\n      },\r\n      \r\n      async didEncounterErrors({ errors }) {\r\n        for (const error of errors) {\r\n          logger.error({\r\n            type: 'graphql_error',\r\n            message: error.message,\r\n            path: error.path,\r\n            code: error.extensions?.code,\r\n            userId: contextValue.user?.id,\r\n          });\r\n          \r\n          metrics.increment('graphql.errors', {\r\n            code: error.extensions?.code || 'UNKNOWN',\r\n          });\r\n        }\r\n      },\r\n    };\r\n  },\r\n};\r\n\r\n// Sanitize variables to remove sensitive data\r\nfunction sanitizeVariables(variables: Record<string, any> | undefined) {\r\n  if (!variables) return undefined;\r\n  \r\n  const sensitiveFields = ['password', 'token', 'secret', 'apiKey'];\r\n  const sanitized = { ...variables };\r\n  \r\n  for (const field of sensitiveFields) {\r\n    if (field in sanitized) {\r\n      sanitized[field] = '[REDACTED]';\r\n    }\r\n  }\r\n  \r\n  return sanitized;\r\n}\r\n```\r\n\r\n## Complete Server Configuration\r\n\r\n```typescript\r\nimport { ApolloServer } from '@apollo/server';\r\nimport { expressMiddleware } from '@apollo/server/express4';\r\nimport express from 'express';\r\nimport depthLimit from 'graphql-depth-limit';\r\nimport { createComplexityRule } from 'graphql-query-complexity';\r\n\r\nconst server = new ApolloServer({\r\n  schema: authDirectiveTransformer(schema),\r\n  \r\n  // Validation rules\r\n  validationRules: [\r\n    depthLimit(7),\r\n    createComplexityRule({\r\n      maximumComplexity: 1000,\r\n      estimators: [fieldExtensionsEstimator(), simpleEstimator()],\r\n    }),\r\n  ],\r\n  \r\n  // Disable introspection in production\r\n  introspection: process.env.NODE_ENV !== 'production',\r\n  \r\n  // Disable batching\r\n  allowBatchedHttpRequests: false,\r\n  \r\n  // Plugins\r\n  plugins: [\r\n    rateLimitPlugin,\r\n    persistedQueriesPlugin,\r\n    auditPlugin,\r\n  ],\r\n  \r\n  // Format errors\r\n  formatError: (formattedError, error) => {\r\n    // Don't leak internal errors\r\n    if (process.env.NODE_ENV === 'production') {\r\n      if (!formattedError.extensions?.code) {\r\n        return {\r\n          message: 'Internal server error',\r\n          extensions: { code: 'INTERNAL_SERVER_ERROR' },\r\n        };\r\n      }\r\n    }\r\n    return formattedError;\r\n  },\r\n});\r\n\r\nconst app = express();\r\n\r\napp.use(\r\n  '/graphql',\r\n  express.json({ limit: '100kb' }), // Limit body size\r\n  expressMiddleware(server, {\r\n    context: async ({ req }) => ({\r\n      user: await authenticateRequest(req),\r\n      ip: req.ip,\r\n      loaders: createLoaders(),\r\n    }),\r\n  })\r\n);\r\n```\r\n\r\n## Pitfalls\r\n\r\n| Pitfall | Impact | How to Avoid |\r\n|---------|--------|--------------|\r\n| No depth limit | DoS via nested queries | Use graphql-depth-limit |\r\n| No complexity limit | Resource exhaustion | Implement query cost analysis |\r\n| Introspection in prod | Schema disclosure | Disable for production |\r\n| No rate limiting | Abuse, scraping | Complexity-aware rate limits |\r\n| Missing field auth | Data leakage | Use auth directives |\r\n| Unbounded lists | Memory exhaustion | Require pagination, max limits |\r\n| Verbose errors | Information disclosure | Sanitize in production |\r\n| No query logging | Audit failures | Log all operations |\r\n\r\n## Checklist\r\n\r\n- [ ] Query depth limit configured (max 5-7)\r\n- [ ] Query complexity analysis enabled\r\n- [ ] Complexity budget per client/IP\r\n- [ ] Introspection disabled in production\r\n- [ ] Batching disabled or limited\r\n- [ ] Field-level authorization implemented\r\n- [ ] Input validation on all mutations\r\n- [ ] Rate limiting based on complexity\r\n- [ ] DataLoader used to prevent N+1\r\n- [ ] Persisted queries for production\r\n- [ ] Request body size limited\r\n- [ ] Audit logging for all operations\r\n- [ ] Error messages sanitized\r\n- [ ] Timeout configured for resolvers\r\n\r\n## References\r\n\r\n- [OWASP GraphQL Cheat Sheet](https://cheatsheetseries.owasp.org/cheatsheets/GraphQL_Cheat_Sheet.html)\r\n- [Apollo Server Security](https://www.apollographql.com/docs/apollo-server/security/)\r\n- [How to GraphQL Security](https://www.howtographql.com/advanced/4-security/)\r\n- [graphql-query-complexity](https://github.com/slicknode/graphql-query-complexity)\r\n"
  },
  {
    "id": "sec-input-validation",
    "title": "Input Validation and Sanitization",
    "tags": [
      "security",
      "validation",
      "sanitization",
      "xss",
      "injection"
    ],
    "level": "intermediate",
    "stacks": [
      "nodejs",
      "python",
      "go"
    ],
    "scope": "security",
    "maturity": "stable",
    "works_with": [
      "nodejs",
      "python",
      "go"
    ],
    "path": "D:/Code/backend-engineering-kit/.shared/production-backend-kit/patterns/sec.input-validation.md",
    "type": "pattern",
    "sections": [],
    "checklist": [],
    "sources": [],
    "rawContent": "\r\n# Input Validation and Sanitization\r\n\r\n## Problem\r\n\r\nUnvalidated input causes:\r\n- **SQL Injection**: `'; DROP TABLE users; --`\r\n- **XSS**: `<script>document.cookie</script>`\r\n- **Command Injection**: `; rm -rf /`\r\n- **Path Traversal**: `../../etc/passwd`\r\n- **NoSQL Injection**: `{\"$gt\": \"\"}`\r\n- **Business Logic Bypass**: negative quantities, future dates\r\n\r\n## When to use\r\n\r\n- **Every** user input (request body, query params, headers)\r\n- File uploads\r\n- Webhook payloads\r\n- Data from external APIs\r\n- Configuration from environment\r\n\r\n## Solution\r\n\r\n### 1. Validation with Zod (TypeScript)\r\n\r\n```typescript\r\nimport { z } from 'zod';\r\n\r\n// User registration schema\r\nconst UserRegistrationSchema = z.object({\r\n  email: z\r\n    .string()\r\n    .email('Invalid email format')\r\n    .max(255, 'Email too long')\r\n    .toLowerCase()\r\n    .trim(),\r\n  \r\n  password: z\r\n    .string()\r\n    .min(8, 'Password must be at least 8 characters')\r\n    .max(72, 'Password too long')  // bcrypt limit\r\n    .regex(/[a-z]/, 'Password must contain lowercase letter')\r\n    .regex(/[A-Z]/, 'Password must contain uppercase letter')\r\n    .regex(/[0-9]/, 'Password must contain number')\r\n    .regex(/[^a-zA-Z0-9]/, 'Password must contain special character'),\r\n  \r\n  username: z\r\n    .string()\r\n    .min(3, 'Username must be at least 3 characters')\r\n    .max(30, 'Username too long')\r\n    .regex(/^[a-zA-Z0-9_-]+$/, 'Username can only contain letters, numbers, underscores, hyphens'),\r\n  \r\n  age: z\r\n    .number()\r\n    .int('Age must be a whole number')\r\n    .min(13, 'Must be at least 13 years old')\r\n    .max(120, 'Invalid age'),\r\n  \r\n  website: z\r\n    .string()\r\n    .url('Invalid URL')\r\n    .optional()\r\n    .refine(\r\n      (url) => !url || url.startsWith('https://'),\r\n      'URL must use HTTPS'\r\n    ),\r\n});\r\n\r\n// Order creation schema\r\nconst CreateOrderSchema = z.object({\r\n  items: z\r\n    .array(z.object({\r\n      productId: z.string().uuid('Invalid product ID'),\r\n      quantity: z\r\n        .number()\r\n        .int()\r\n        .positive('Quantity must be positive')\r\n        .max(100, 'Maximum 100 items per product'),\r\n    }))\r\n    .min(1, 'Order must have at least one item')\r\n    .max(50, 'Maximum 50 different products'),\r\n  \r\n  shippingAddress: z.object({\r\n    street: z.string().min(5).max(200),\r\n    city: z.string().min(2).max(100),\r\n    state: z.string().length(2),\r\n    zip: z.string().regex(/^\\d{5}(-\\d{4})?$/, 'Invalid ZIP code'),\r\n    country: z.enum(['US', 'CA', 'MX']),\r\n  }),\r\n  \r\n  paymentMethod: z.discriminatedUnion('type', [\r\n    z.object({\r\n      type: z.literal('credit_card'),\r\n      cardId: z.string().uuid(),\r\n    }),\r\n    z.object({\r\n      type: z.literal('paypal'),\r\n      paypalEmail: z.string().email(),\r\n    }),\r\n    z.object({\r\n      type: z.literal('bank_transfer'),\r\n      accountNumber: z.string().regex(/^\\d{10,17}$/),\r\n    }),\r\n  ]),\r\n  \r\n  couponCode: z\r\n    .string()\r\n    .regex(/^[A-Z0-9]{4,20}$/, 'Invalid coupon format')\r\n    .optional(),\r\n});\r\n\r\n// Validate and transform\r\nfunction validateInput<T>(schema: z.ZodSchema<T>, input: unknown): T {\r\n  const result = schema.safeParse(input);\r\n  \r\n  if (!result.success) {\r\n    const errors = result.error.errors.map(e => ({\r\n      field: e.path.join('.'),\r\n      message: e.message,\r\n    }));\r\n    \r\n    throw new ValidationError('Validation failed', errors);\r\n  }\r\n  \r\n  return result.data;\r\n}\r\n\r\n// Express middleware\r\nfunction validate<T>(schema: z.ZodSchema<T>) {\r\n  return (req: Request, res: Response, next: NextFunction) => {\r\n    try {\r\n      req.body = validateInput(schema, req.body);\r\n      next();\r\n    } catch (error) {\r\n      if (error instanceof ValidationError) {\r\n        return res.status(400).json({\r\n          error: 'Validation failed',\r\n          code: 'VALIDATION_ERROR',\r\n          details: error.errors,\r\n        });\r\n      }\r\n      next(error);\r\n    }\r\n  };\r\n}\r\n\r\n// Usage\r\napp.post('/api/users', validate(UserRegistrationSchema), async (req, res) => {\r\n  // req.body is validated and typed\r\n  const user = await userService.create(req.body);\r\n  res.status(201).json(user);\r\n});\r\n```\r\n\r\n### 2. Sanitization Utilities\r\n\r\n```typescript\r\nimport DOMPurify from 'isomorphic-dompurify';\r\nimport validator from 'validator';\r\n\r\nconst sanitizers = {\r\n  // HTML content sanitization (for rich text)\r\n  html(input: string): string {\r\n    return DOMPurify.sanitize(input, {\r\n      ALLOWED_TAGS: ['b', 'i', 'em', 'strong', 'a', 'p', 'br', 'ul', 'ol', 'li'],\r\n      ALLOWED_ATTR: ['href', 'title'],\r\n      ALLOW_DATA_ATTR: false,\r\n    });\r\n  },\r\n  \r\n  // Plain text (strip all HTML)\r\n  plainText(input: string): string {\r\n    return validator.escape(validator.stripLow(input.trim()));\r\n  },\r\n  \r\n  // Filename sanitization\r\n  filename(input: string): string {\r\n    // Remove path components and dangerous characters\r\n    return input\r\n      .replace(/[/\\\\?%*:|\"<>]/g, '')\r\n      .replace(/^\\.+/, '')  // No leading dots\r\n      .replace(/\\s+/g, '_')\r\n      .substring(0, 255);\r\n  },\r\n  \r\n  // SQL-safe string (use parameterized queries instead!)\r\n  // This is a LAST RESORT - always use parameterized queries\r\n  sqlString(input: string): string {\r\n    return input.replace(/['\";\\\\]/g, '');\r\n  },\r\n  \r\n  // URL parameter\r\n  urlParam(input: string): string {\r\n    return encodeURIComponent(input);\r\n  },\r\n  \r\n  // Shell argument (for subprocess)\r\n  // WARNING: Avoid shell commands with user input!\r\n  shellArg(input: string): string {\r\n    return `'${input.replace(/'/g, \"'\\\\''\")}'`;\r\n  },\r\n  \r\n  // JSON key (prevent prototype pollution)\r\n  jsonKey(input: string): string {\r\n    const dangerous = ['__proto__', 'constructor', 'prototype'];\r\n    if (dangerous.includes(input)) {\r\n      throw new Error('Invalid JSON key');\r\n    }\r\n    return input;\r\n  },\r\n  \r\n  // Phone number\r\n  phone(input: string): string {\r\n    return input.replace(/[^\\d+\\-\\(\\)\\s]/g, '').trim();\r\n  },\r\n  \r\n  // Credit card (mask for display)\r\n  creditCardMask(input: string): string {\r\n    const digits = input.replace(/\\D/g, '');\r\n    return `****-****-****-${digits.slice(-4)}`;\r\n  },\r\n};\r\n\r\n// Usage\r\nconst userComment = sanitizers.plainText(req.body.comment);\r\nconst richContent = sanitizers.html(req.body.content);\r\nconst uploadedFilename = sanitizers.filename(file.originalname);\r\n```\r\n\r\n### 3. SQL Injection Prevention\r\n\r\n```typescript\r\n// ❌ NEVER do this - SQL Injection vulnerable\r\nconst bad = `SELECT * FROM users WHERE email = '${email}'`;\r\n\r\n// ✅ ALWAYS use parameterized queries\r\n\r\n// node-postgres\r\nconst result = await pool.query(\r\n  'SELECT * FROM users WHERE email = $1 AND status = $2',\r\n  [email, 'active']\r\n);\r\n\r\n// Prisma (ORM)\r\nconst user = await prisma.user.findFirst({\r\n  where: {\r\n    email: email,  // Automatically parameterized\r\n    status: 'active',\r\n  },\r\n});\r\n\r\n// Raw queries with Prisma (still safe)\r\nconst users = await prisma.$queryRaw`\r\n  SELECT * FROM users WHERE email = ${email}\r\n`;\r\n\r\n// TypeORM\r\nconst user = await userRepository\r\n  .createQueryBuilder('user')\r\n  .where('user.email = :email', { email })\r\n  .andWhere('user.status = :status', { status: 'active' })\r\n  .getOne();\r\n\r\n// Dynamic column names (still dangerous!)\r\n// If you MUST use dynamic columns:\r\nconst allowedColumns = ['name', 'email', 'created_at'];\r\nif (!allowedColumns.includes(sortColumn)) {\r\n  throw new Error('Invalid sort column');\r\n}\r\nconst query = `SELECT * FROM users ORDER BY ${sortColumn}`;\r\n```\r\n\r\n### 4. NoSQL Injection Prevention\r\n\r\n```typescript\r\n// MongoDB injection vulnerabilities\r\n\r\n// ❌ Vulnerable to operator injection\r\n// Input: { \"$gt\": \"\" } bypasses password check\r\nconst bad = await collection.findOne({\r\n  username: req.body.username,\r\n  password: req.body.password,  // Could be { \"$gt\": \"\" }\r\n});\r\n\r\n// ✅ Type check and sanitize\r\nfunction sanitizeMongoQuery(input: unknown): string {\r\n  if (typeof input !== 'string') {\r\n    throw new ValidationError('Expected string input');\r\n  }\r\n  \r\n  // Remove MongoDB operators\r\n  if (input.startsWith('$')) {\r\n    throw new ValidationError('Invalid input');\r\n  }\r\n  \r\n  return input;\r\n}\r\n\r\nconst username = sanitizeMongoQuery(req.body.username);\r\nconst password = sanitizeMongoQuery(req.body.password);\r\n\r\nconst user = await collection.findOne({\r\n  username,\r\n  password: await hash(password),\r\n});\r\n\r\n// ✅ Better: Use schema validation\r\nconst LoginSchema = z.object({\r\n  username: z.string().min(1).max(50),\r\n  password: z.string().min(1).max(72),\r\n});\r\n\r\n// Mongoose with schema validation\r\nconst userSchema = new mongoose.Schema({\r\n  username: {\r\n    type: String,\r\n    required: true,\r\n    validate: {\r\n      validator: (v: string) => /^[a-zA-Z0-9_]+$/.test(v),\r\n      message: 'Invalid username format',\r\n    },\r\n  },\r\n});\r\n```\r\n\r\n### 5. Path Traversal Prevention\r\n\r\n```typescript\r\nimport path from 'path';\r\nimport fs from 'fs/promises';\r\n\r\n// ❌ Vulnerable to path traversal\r\n// Input: \"../../etc/passwd\"\r\nconst bad = path.join('/uploads', req.params.filename);\r\n\r\n// ✅ Safe file path handling\r\nconst UPLOAD_DIR = '/var/app/uploads';\r\n\r\nasync function getSecureFilePath(userInput: string): Promise<string> {\r\n  // Normalize and resolve the path\r\n  const safeName = path.basename(userInput);  // Remove directory components\r\n  const fullPath = path.resolve(UPLOAD_DIR, safeName);\r\n  \r\n  // Verify it's within allowed directory\r\n  if (!fullPath.startsWith(UPLOAD_DIR)) {\r\n    throw new SecurityError('Invalid file path');\r\n  }\r\n  \r\n  // Check file exists\r\n  try {\r\n    await fs.access(fullPath);\r\n  } catch {\r\n    throw new NotFoundError('File not found');\r\n  }\r\n  \r\n  return fullPath;\r\n}\r\n\r\n// Usage\r\napp.get('/files/:filename', async (req, res) => {\r\n  const filePath = await getSecureFilePath(req.params.filename);\r\n  res.sendFile(filePath);\r\n});\r\n\r\n// For file uploads\r\nconst uploadConfig = multer({\r\n  storage: multer.diskStorage({\r\n    destination: UPLOAD_DIR,\r\n    filename: (req, file, cb) => {\r\n      // Generate safe filename\r\n      const ext = path.extname(file.originalname).toLowerCase();\r\n      const allowedExts = ['.jpg', '.jpeg', '.png', '.gif', '.pdf'];\r\n      \r\n      if (!allowedExts.includes(ext)) {\r\n        return cb(new Error('Invalid file type'), '');\r\n      }\r\n      \r\n      const safeName = `${crypto.randomUUID()}${ext}`;\r\n      cb(null, safeName);\r\n    },\r\n  }),\r\n  limits: {\r\n    fileSize: 5 * 1024 * 1024,  // 5MB\r\n  },\r\n});\r\n```\r\n\r\n### 6. Command Injection Prevention\r\n\r\n```typescript\r\nimport { execFile, spawn } from 'child_process';\r\nimport { promisify } from 'util';\r\n\r\nconst execFileAsync = promisify(execFile);\r\n\r\n// ❌ NEVER do this - Command injection\r\nconst bad = exec(`convert ${inputFile} ${outputFile}`);\r\n\r\n// ✅ Use execFile with arguments array\r\nasync function convertImage(inputPath: string, outputPath: string) {\r\n  // Validate paths first\r\n  if (!inputPath.match(/^[a-zA-Z0-9_\\-./]+$/)) {\r\n    throw new Error('Invalid input path');\r\n  }\r\n  \r\n  await execFileAsync('convert', [\r\n    inputPath,\r\n    '-resize', '800x600',\r\n    '-quality', '85',\r\n    outputPath,\r\n  ]);\r\n}\r\n\r\n// ✅ For complex commands, use spawn with shell: false\r\nasync function runFFmpeg(input: string, output: string) {\r\n  return new Promise((resolve, reject) => {\r\n    const proc = spawn('ffmpeg', [\r\n      '-i', input,\r\n      '-vcodec', 'h264',\r\n      '-acodec', 'aac',\r\n      output,\r\n    ], {\r\n      shell: false,  // IMPORTANT: Don't use shell\r\n    });\r\n    \r\n    proc.on('close', (code) => {\r\n      if (code === 0) resolve(true);\r\n      else reject(new Error(`FFmpeg exited with code ${code}`));\r\n    });\r\n    \r\n    proc.on('error', reject);\r\n  });\r\n}\r\n\r\n// ✅ If you absolutely need shell, use allowlist\r\nconst ALLOWED_COMMANDS = ['ls', 'cat', 'head', 'tail'];\r\n\r\nfunction runShellCommand(command: string, args: string[]) {\r\n  if (!ALLOWED_COMMANDS.includes(command)) {\r\n    throw new Error('Command not allowed');\r\n  }\r\n  \r\n  // Still use execFile, not exec\r\n  return execFileAsync(command, args);\r\n}\r\n```\r\n\r\n### 7. Request Size and Rate Limiting\r\n\r\n```typescript\r\nimport express from 'express';\r\nimport rateLimit from 'express-rate-limit';\r\n\r\nconst app = express();\r\n\r\n// Limit request body size\r\napp.use(express.json({ limit: '100kb' }));\r\napp.use(express.urlencoded({ limit: '100kb', extended: true }));\r\n\r\n// Global rate limit\r\napp.use(rateLimit({\r\n  windowMs: 15 * 60 * 1000,  // 15 minutes\r\n  max: 100,\r\n  message: { error: 'Too many requests', retryAfter: 900 },\r\n}));\r\n\r\n// Stricter limits for sensitive endpoints\r\nconst authLimiter = rateLimit({\r\n  windowMs: 60 * 60 * 1000,  // 1 hour\r\n  max: 5,\r\n  message: { error: 'Too many login attempts' },\r\n});\r\n\r\napp.post('/api/auth/login', authLimiter, loginHandler);\r\napp.post('/api/auth/forgot-password', authLimiter, forgotPasswordHandler);\r\n\r\n// Array length limits\r\nconst ItemsSchema = z.object({\r\n  items: z.array(z.object({\r\n    id: z.string(),\r\n    value: z.number(),\r\n  })).max(1000, 'Maximum 1000 items'),\r\n});\r\n\r\n// String length limits everywhere\r\nconst CommentSchema = z.object({\r\n  content: z.string().max(10000, 'Comment too long'),\r\n  tags: z.array(z.string().max(50)).max(10),\r\n});\r\n\r\n// Pagination limits\r\nconst PaginationSchema = z.object({\r\n  page: z.number().int().min(1).default(1),\r\n  limit: z.number().int().min(1).max(100).default(20),\r\n});\r\n```\r\n\r\n### 8. Content-Type Validation\r\n\r\n```typescript\r\n// Validate content type matches actual content\r\n\r\nimport fileType from 'file-type';\r\nimport { Request, Response, NextFunction } from 'express';\r\n\r\nconst ALLOWED_TYPES = {\r\n  'image/jpeg': ['.jpg', '.jpeg'],\r\n  'image/png': ['.png'],\r\n  'image/gif': ['.gif'],\r\n  'application/pdf': ['.pdf'],\r\n};\r\n\r\nasync function validateFileType(\r\n  buffer: Buffer,\r\n  declaredMimeType: string\r\n): Promise<void> {\r\n  // Detect actual file type from magic bytes\r\n  const detected = await fileType.fromBuffer(buffer);\r\n  \r\n  if (!detected) {\r\n    throw new ValidationError('Could not detect file type');\r\n  }\r\n  \r\n  // Check against allowlist\r\n  if (!ALLOWED_TYPES[detected.mime]) {\r\n    throw new ValidationError(`File type ${detected.mime} not allowed`);\r\n  }\r\n  \r\n  // Verify declared type matches detected\r\n  if (detected.mime !== declaredMimeType) {\r\n    throw new ValidationError(\r\n      `Declared type ${declaredMimeType} does not match actual type ${detected.mime}`\r\n    );\r\n  }\r\n}\r\n\r\n// Usage with multer\r\nconst upload = multer({\r\n  storage: multer.memoryStorage(),\r\n  limits: { fileSize: 5 * 1024 * 1024 },\r\n});\r\n\r\napp.post('/upload', upload.single('file'), async (req, res, next) => {\r\n  try {\r\n    await validateFileType(req.file.buffer, req.file.mimetype);\r\n    \r\n    // Safe to process file\r\n    const filename = `${crypto.randomUUID()}${path.extname(req.file.originalname)}`;\r\n    await fs.writeFile(path.join(UPLOAD_DIR, filename), req.file.buffer);\r\n    \r\n    res.json({ filename });\r\n  } catch (error) {\r\n    next(error);\r\n  }\r\n});\r\n```\r\n\r\n### 9. Prototype Pollution Prevention\r\n\r\n```typescript\r\n// Prevent prototype pollution attacks\r\n\r\n// ❌ Vulnerable to prototype pollution\r\nfunction merge(target: any, source: any) {\r\n  for (const key in source) {\r\n    target[key] = source[key];  // Can pollute __proto__\r\n  }\r\n  return target;\r\n}\r\n\r\n// ✅ Safe merge\r\nfunction safeMerge<T extends object>(target: T, source: Partial<T>): T {\r\n  const dangerous = ['__proto__', 'constructor', 'prototype'];\r\n  \r\n  for (const key of Object.keys(source)) {\r\n    if (dangerous.includes(key)) {\r\n      continue;  // Skip dangerous keys\r\n    }\r\n    \r\n    if (Object.prototype.hasOwnProperty.call(source, key)) {\r\n      (target as any)[key] = (source as any)[key];\r\n    }\r\n  }\r\n  \r\n  return target;\r\n}\r\n\r\n// ✅ Use Object.create(null) for dictionaries\r\nconst safeDict = Object.create(null);\r\nsafeDict['user-input'] = 'value';  // No prototype chain\r\n\r\n// ✅ Freeze prototypes (do early in app startup)\r\nObject.freeze(Object.prototype);\r\nObject.freeze(Array.prototype);\r\n\r\n// ✅ Use Map instead of objects for user-keyed data\r\nconst userPreferences = new Map<string, unknown>();\r\nuserPreferences.set(userInputKey, value);\r\n```\r\n\r\n## Pitfalls\r\n\r\n| Pitfall | Impact | How to Avoid |\r\n|---------|--------|--------------|\r\n| Validation only on client | Bypass trivial | Always validate on server |\r\n| Blacklist approach | Easy to bypass | Use allowlists |\r\n| Missing encoding | XSS | Context-aware encoding |\r\n| String concat in SQL | SQL injection | Parameterized queries |\r\n| Shell commands with input | Command injection | Use execFile, not exec |\r\n| Trust content-type header | File upload attacks | Validate magic bytes |\r\n| No size limits | DoS | Limit all inputs |\r\n\r\n## Checklist\r\n\r\n- [ ] All inputs validated with schema\r\n- [ ] Validation happens server-side\r\n- [ ] Parameterized queries for all database ops\r\n- [ ] HTML sanitized before rendering\r\n- [ ] File paths validated against traversal\r\n- [ ] File types verified by magic bytes\r\n- [ ] Request body size limited\r\n- [ ] Rate limiting on sensitive endpoints\r\n- [ ] No shell commands with user input\r\n- [ ] Prototype pollution prevented\r\n- [ ] Error messages don't leak info\r\n\r\n## References\r\n\r\n- [OWASP Input Validation Cheat Sheet](https://cheatsheetseries.owasp.org/cheatsheets/Input_Validation_Cheat_Sheet.html)\r\n- [OWASP XSS Prevention](https://cheatsheetseries.owasp.org/cheatsheets/Cross_Site_Scripting_Prevention_Cheat_Sheet.html)\r\n- [Zod Documentation](https://zod.dev/)\r\n- [validator.js](https://github.com/validatorjs/validator.js)\r\n"
  },
  {
    "id": "sec-jwt-best-practices",
    "title": "JWT Security Best Practices",
    "tags": [
      "security",
      "jwt",
      "authentication",
      "tokens",
      "oauth"
    ],
    "level": "intermediate",
    "stacks": [
      "all"
    ],
    "scope": "security",
    "maturity": "stable",
    "works_with": [
      "all"
    ],
    "path": "D:/Code/backend-engineering-kit/.shared/production-backend-kit/patterns/sec.jwt-best-practices.md",
    "type": "pattern",
    "sections": [],
    "checklist": [],
    "sources": [],
    "rawContent": "\r\n# JWT Security Best Practices\r\n\r\n## Problem\r\n\r\nJWTs are widely used but frequently misconfigured, leading to:\r\n- Token forgery via algorithm confusion attacks\r\n- Sensitive data exposure in payloads\r\n- Stolen tokens used indefinitely\r\n- No way to revoke compromised tokens\r\n- XSS attacks stealing tokens from localStorage\r\n\r\n**JWTs are not session tokens. Treat them as signed assertions.**\r\n\r\n## When to use\r\n\r\n- Stateless authentication for APIs\r\n- Service-to-service authentication\r\n- OAuth 2.0 / OpenID Connect implementations\r\n- Short-lived authorization grants\r\n- Passing claims between trusted systems\r\n\r\n## When NOT to use\r\n\r\n- When you need instant revocation (use sessions)\r\n- Long-lived tokens (>1 hour) without refresh mechanism\r\n- Storing sensitive data (use opaque tokens + backend lookup)\r\n- Simple web apps where sessions work fine\r\n\r\n## Solution\r\n\r\n### 1. JWT Structure & Security\r\n\r\n```\r\n┌─────────────────────────────────────────────────────────────────────────────┐\r\n│                              JWT STRUCTURE                                  │\r\n├─────────────────────────────────────────────────────────────────────────────┤\r\n│                                                                             │\r\n│  eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6ImtleS0xIn0                   │\r\n│  .eyJzdWIiOiJ1c2VyXzEyMyIsImlhdCI6MTcwNTY0MDAwMCwiZXhwIjoxNzA1NjQzNjAwfQ   │\r\n│  .signature_here                                                            │\r\n│                                                                             │\r\n│  ┌──────────────┐    ┌──────────────────┐    ┌────────────────┐            │\r\n│  │   HEADER     │    │     PAYLOAD      │    │   SIGNATURE    │            │\r\n│  │  (Base64)    │    │    (Base64)      │    │   (Base64)     │            │\r\n│  ├──────────────┤    ├──────────────────┤    ├────────────────┤            │\r\n│  │ {            │    │ {                │    │ HMAC/RSA/ECDSA │            │\r\n│  │   \"alg\":     │    │   \"sub\": \"123\",  │    │ of header +    │            │\r\n│  │     \"RS256\", │    │   \"iat\": ...,    │    │ payload using  │            │\r\n│  │   \"typ\":     │    │   \"exp\": ...,    │    │ secret/key     │            │\r\n│  │     \"JWT\",   │    │   \"roles\": [...] │    │                │            │\r\n│  │   \"kid\":     │    │ }                │    │                │            │\r\n│  │     \"key-1\"  │    │                  │    │                │            │\r\n│  │ }            │    │                  │    │                │            │\r\n│  └──────────────┘    └──────────────────┘    └────────────────┘            │\r\n│                                                                             │\r\n│  ⚠️  PAYLOAD IS NOT ENCRYPTED - Anyone can decode it!                      │\r\n│                                                                             │\r\n└─────────────────────────────────────────────────────────────────────────────┘\r\n```\r\n\r\n### 2. Algorithm Selection\r\n\r\n```typescript\r\n// ✅ RECOMMENDED: Asymmetric algorithms\r\nconst SECURE_ALGORITHMS = {\r\n  RS256: 'RSA with SHA-256',      // Most common, widely supported\r\n  RS384: 'RSA with SHA-384',      // Higher security\r\n  RS512: 'RSA with SHA-512',      // Highest security, slower\r\n  ES256: 'ECDSA with P-256',      // Smaller keys, fast\r\n  ES384: 'ECDSA with P-384',      // Higher security\r\n  ES512: 'ECDSA with P-512',      // Highest ECDSA security\r\n  PS256: 'RSA-PSS with SHA-256',  // RSA with PSS padding\r\n  EdDSA: 'Edwards-curve DSA',     // Modern, fast (Ed25519)\r\n};\r\n\r\n// ⚠️ USE WITH CAUTION: Symmetric algorithms\r\nconst SYMMETRIC_ALGORITHMS = {\r\n  HS256: 'HMAC-SHA256', // Only if secret is truly secret\r\n  HS384: 'HMAC-SHA384', // and >256 bits random\r\n  HS512: 'HMAC-SHA512',\r\n};\r\n\r\n// ❌ NEVER USE\r\nconst DANGEROUS = {\r\n  none: 'No signature',  // NEVER! Algorithm confusion attack\r\n  RS256_with_HS256: 'Public key as HMAC secret', // Attack vector\r\n};\r\n```\r\n\r\n### 3. Secure JWT Creation\r\n\r\n```typescript\r\nimport * as jose from 'jose';\r\n\r\n// Key management\r\nconst privateKey = await jose.importPKCS8(process.env.JWT_PRIVATE_KEY!, 'RS256');\r\nconst publicKey = await jose.importSPKI(process.env.JWT_PUBLIC_KEY!, 'RS256');\r\n\r\ninterface TokenPayload {\r\n  sub: string;          // Subject (user ID)\r\n  email: string;\r\n  roles: string[];\r\n  tenantId: string;\r\n}\r\n\r\nasync function createAccessToken(payload: TokenPayload): Promise<string> {\r\n  const now = Math.floor(Date.now() / 1000);\r\n  \r\n  const jwt = await new jose.SignJWT({\r\n    // Custom claims\r\n    email: payload.email,\r\n    roles: payload.roles,\r\n    tenant_id: payload.tenantId,\r\n  })\r\n    .setProtectedHeader({ \r\n      alg: 'RS256', \r\n      typ: 'JWT',\r\n      kid: 'key-2024-01',  // Key ID for rotation\r\n    })\r\n    .setSubject(payload.sub)\r\n    .setIssuer('https://auth.example.com')\r\n    .setAudience('https://api.example.com')\r\n    .setIssuedAt(now)\r\n    .setExpirationTime(now + 15 * 60)  // 15 minutes\r\n    .setNotBefore(now)\r\n    .setJti(crypto.randomUUID())  // Unique token ID\r\n    .sign(privateKey);\r\n  \r\n  return jwt;\r\n}\r\n\r\nasync function createRefreshToken(userId: string): Promise<string> {\r\n  const jwt = await new jose.SignJWT({\r\n    type: 'refresh',\r\n  })\r\n    .setProtectedHeader({ alg: 'RS256', typ: 'JWT' })\r\n    .setSubject(userId)\r\n    .setIssuer('https://auth.example.com')\r\n    .setIssuedAt()\r\n    .setExpirationTime('7d')  // 7 days\r\n    .setJti(crypto.randomUUID())\r\n    .sign(privateKey);\r\n  \r\n  // Store refresh token ID in database for revocation\r\n  await db.refreshTokens.create({\r\n    jti: jose.decodeJwt(jwt).jti,\r\n    userId,\r\n    expiresAt: new Date(Date.now() + 7 * 24 * 60 * 60 * 1000),\r\n  });\r\n  \r\n  return jwt;\r\n}\r\n```\r\n\r\n### 4. Secure JWT Verification\r\n\r\n```typescript\r\n// CRITICAL: Always validate these!\r\nconst verifyOptions = {\r\n  algorithms: ['RS256'],         // Explicit allowlist - NEVER accept 'none'\r\n  issuer: 'https://auth.example.com',\r\n  audience: 'https://api.example.com',\r\n  clockTolerance: 30,            // 30 seconds leeway for clock skew\r\n  requiredClaims: ['sub', 'exp', 'iat', 'iss', 'aud'],\r\n};\r\n\r\nasync function verifyAccessToken(token: string): Promise<TokenPayload> {\r\n  try {\r\n    const { payload, protectedHeader } = await jose.jwtVerify(\r\n      token,\r\n      publicKey,\r\n      verifyOptions\r\n    );\r\n    \r\n    // Additional business validations\r\n    if (!payload.sub) {\r\n      throw new Error('Missing subject');\r\n    }\r\n    \r\n    if (!payload.tenant_id) {\r\n      throw new Error('Missing tenant');\r\n    }\r\n    \r\n    return {\r\n      sub: payload.sub as string,\r\n      email: payload.email as string,\r\n      roles: payload.roles as string[],\r\n      tenantId: payload.tenant_id as string,\r\n    };\r\n  } catch (error) {\r\n    if (error instanceof jose.errors.JWTExpired) {\r\n      throw new AuthError('TOKEN_EXPIRED', 'Token has expired');\r\n    }\r\n    if (error instanceof jose.errors.JWTClaimValidationFailed) {\r\n      throw new AuthError('INVALID_CLAIMS', 'Token claims validation failed');\r\n    }\r\n    throw new AuthError('INVALID_TOKEN', 'Token verification failed');\r\n  }\r\n}\r\n\r\n// Middleware\r\nasync function authMiddleware(req: Request, res: Response, next: NextFunction) {\r\n  const authHeader = req.headers.authorization;\r\n  \r\n  if (!authHeader?.startsWith('Bearer ')) {\r\n    return res.status(401).json({ error: 'Missing authorization header' });\r\n  }\r\n  \r\n  const token = authHeader.slice(7);\r\n  \r\n  try {\r\n    req.user = await verifyAccessToken(token);\r\n    next();\r\n  } catch (error) {\r\n    if (error instanceof AuthError) {\r\n      return res.status(401).json({ \r\n        error: error.code,\r\n        message: error.message,\r\n      });\r\n    }\r\n    return res.status(401).json({ error: 'Authentication failed' });\r\n  }\r\n}\r\n```\r\n\r\n### 5. Token Refresh Pattern\r\n\r\n```typescript\r\n// Token refresh endpoint\r\napp.post('/auth/refresh', async (req, res) => {\r\n  const { refreshToken } = req.body;\r\n  \r\n  try {\r\n    // 1. Verify refresh token\r\n    const { payload } = await jose.jwtVerify(refreshToken, publicKey, {\r\n      algorithms: ['RS256'],\r\n      issuer: 'https://auth.example.com',\r\n    });\r\n    \r\n    // 2. Check if refresh token is in database (not revoked)\r\n    const storedToken = await db.refreshTokens.findOne({\r\n      jti: payload.jti,\r\n      revokedAt: null,\r\n    });\r\n    \r\n    if (!storedToken) {\r\n      // Token was revoked or doesn't exist\r\n      throw new AuthError('REFRESH_TOKEN_REVOKED', 'Refresh token is invalid');\r\n    }\r\n    \r\n    // 3. Check for token reuse (replay attack detection)\r\n    if (storedToken.usedAt) {\r\n      // Token was already used! Possible token theft\r\n      // Revoke ALL refresh tokens for this user\r\n      await db.refreshTokens.updateMany(\r\n        { userId: storedToken.userId },\r\n        { revokedAt: new Date() }\r\n      );\r\n      \r\n      logger.security({\r\n        event: 'REFRESH_TOKEN_REUSE',\r\n        userId: storedToken.userId,\r\n        jti: payload.jti,\r\n      });\r\n      \r\n      throw new AuthError('REFRESH_TOKEN_REUSED', 'Security violation detected');\r\n    }\r\n    \r\n    // 4. Mark old refresh token as used\r\n    await db.refreshTokens.update(\r\n      { jti: payload.jti },\r\n      { usedAt: new Date() }\r\n    );\r\n    \r\n    // 5. Issue new tokens (rotation)\r\n    const user = await db.users.findById(storedToken.userId);\r\n    \r\n    const accessToken = await createAccessToken({\r\n      sub: user.id,\r\n      email: user.email,\r\n      roles: user.roles,\r\n      tenantId: user.tenantId,\r\n    });\r\n    \r\n    const newRefreshToken = await createRefreshToken(user.id);\r\n    \r\n    res.json({\r\n      accessToken,\r\n      refreshToken: newRefreshToken,\r\n      expiresIn: 900, // 15 minutes\r\n    });\r\n    \r\n  } catch (error) {\r\n    if (error instanceof jose.errors.JWTExpired) {\r\n      return res.status(401).json({ error: 'Refresh token expired' });\r\n    }\r\n    return res.status(401).json({ error: error.message });\r\n  }\r\n});\r\n```\r\n\r\n### 6. Token Revocation Strategies\r\n\r\n```typescript\r\n// Strategy 1: Short-lived access tokens + refresh token rotation\r\n// - Access token: 15 minutes, not revocable\r\n// - Refresh token: 7 days, stored in DB, revocable\r\n// Pros: Fast verification, minimal DB lookups\r\n// Cons: Up to 15 min window before revocation takes effect\r\n\r\n// Strategy 2: Token blacklist\r\nclass TokenBlacklist {\r\n  private redis: Redis;\r\n  \r\n  async revoke(jti: string, expiresAt: Date) {\r\n    const ttl = Math.ceil((expiresAt.getTime() - Date.now()) / 1000);\r\n    if (ttl > 0) {\r\n      await this.redis.setex(`blacklist:${jti}`, ttl, '1');\r\n    }\r\n  }\r\n  \r\n  async isRevoked(jti: string): Promise<boolean> {\r\n    return (await this.redis.exists(`blacklist:${jti}`)) === 1;\r\n  }\r\n}\r\n\r\n// Check blacklist during verification\r\nasync function verifyWithBlacklist(token: string) {\r\n  const { payload } = await jose.jwtVerify(token, publicKey, verifyOptions);\r\n  \r\n  if (payload.jti && await blacklist.isRevoked(payload.jti as string)) {\r\n    throw new AuthError('TOKEN_REVOKED', 'Token has been revoked');\r\n  }\r\n  \r\n  return payload;\r\n}\r\n\r\n// Strategy 3: Token versioning\r\n// Store a version number per user, include in token\r\n// Increment version to invalidate all tokens\r\nasync function verifyWithVersion(token: string) {\r\n  const { payload } = await jose.jwtVerify(token, publicKey, verifyOptions);\r\n  \r\n  const user = await cache.get(`user:${payload.sub}:tokenVersion`);\r\n  if (user && payload.v !== user.tokenVersion) {\r\n    throw new AuthError('TOKEN_VERSION_MISMATCH', 'Token is outdated');\r\n  }\r\n  \r\n  return payload;\r\n}\r\n\r\n// Logout - revoke all tokens\r\nasync function logout(userId: string) {\r\n  // Increment token version\r\n  await db.users.update({ id: userId }, { tokenVersion: { increment: 1 } });\r\n  await cache.del(`user:${userId}:tokenVersion`);\r\n  \r\n  // Revoke all refresh tokens\r\n  await db.refreshTokens.updateMany(\r\n    { userId, revokedAt: null },\r\n    { revokedAt: new Date() }\r\n  );\r\n}\r\n```\r\n\r\n### 7. Secure Token Storage (Client-Side)\r\n\r\n```typescript\r\n// ❌ NEVER: localStorage (XSS vulnerable)\r\nlocalStorage.setItem('token', jwt); // Attacker's JS can read this!\r\n\r\n// ❌ AVOID: sessionStorage (still XSS vulnerable)\r\nsessionStorage.setItem('token', jwt);\r\n\r\n// ✅ RECOMMENDED: HttpOnly cookies\r\n// Server sets this header:\r\nres.cookie('accessToken', jwt, {\r\n  httpOnly: true,      // JavaScript cannot access\r\n  secure: true,        // HTTPS only\r\n  sameSite: 'strict',  // CSRF protection\r\n  maxAge: 15 * 60 * 1000, // 15 minutes\r\n  path: '/api',        // Only sent to API routes\r\n});\r\n\r\n// For SPAs that need to read token expiry:\r\n// Send a non-sensitive \"tokenMeta\" cookie alongside\r\nres.cookie('tokenMeta', JSON.stringify({ exp: payload.exp }), {\r\n  httpOnly: false,  // JS can read this\r\n  secure: true,\r\n  sameSite: 'strict',\r\n  maxAge: 15 * 60 * 1000,\r\n});\r\n\r\n// ✅ ALTERNATIVE: In-memory + refresh\r\n// Store token only in memory (JS variable)\r\n// Use refresh token in HttpOnly cookie\r\nclass TokenManager {\r\n  private accessToken: string | null = null;\r\n  \r\n  async getAccessToken(): Promise<string> {\r\n    if (this.accessToken && !this.isExpired()) {\r\n      return this.accessToken;\r\n    }\r\n    \r\n    // Refresh token is in HttpOnly cookie, sent automatically\r\n    const response = await fetch('/auth/refresh', {\r\n      method: 'POST',\r\n      credentials: 'include',\r\n    });\r\n    \r\n    const { accessToken } = await response.json();\r\n    this.accessToken = accessToken;\r\n    return accessToken;\r\n  }\r\n}\r\n```\r\n\r\n### 8. Key Rotation\r\n\r\n```typescript\r\n// JWKS (JSON Web Key Set) for key rotation\r\n// Publish at: https://auth.example.com/.well-known/jwks.json\r\n\r\nconst keys = [\r\n  {\r\n    kty: 'RSA',\r\n    kid: 'key-2024-01',  // Current key\r\n    use: 'sig',\r\n    alg: 'RS256',\r\n    n: '...',  // Public key modulus\r\n    e: 'AQAB',\r\n  },\r\n  {\r\n    kty: 'RSA',\r\n    kid: 'key-2023-12',  // Previous key (for tokens still valid)\r\n    use: 'sig',\r\n    alg: 'RS256',\r\n    n: '...',\r\n    e: 'AQAB',\r\n  },\r\n];\r\n\r\n// Verification uses JWKS\r\nconst JWKS = jose.createRemoteJWKSet(\r\n  new URL('https://auth.example.com/.well-known/jwks.json'),\r\n  {\r\n    cacheMaxAge: 600000,  // Cache for 10 minutes\r\n    cooldownDuration: 30000,  // Wait 30s between refreshes\r\n  }\r\n);\r\n\r\nasync function verifyWithJWKS(token: string) {\r\n  // jose automatically selects key based on 'kid' header\r\n  const { payload } = await jose.jwtVerify(token, JWKS, verifyOptions);\r\n  return payload;\r\n}\r\n\r\n// Key rotation process:\r\n// 1. Generate new key pair\r\n// 2. Add new public key to JWKS (kid: key-2024-02)\r\n// 3. Wait for JWKS cache to refresh everywhere\r\n// 4. Start signing new tokens with new key\r\n// 5. After old tokens expire, remove old key from JWKS\r\n```\r\n\r\n## Pitfalls\r\n\r\n| Pitfall | Impact | How to Avoid |\r\n|---------|--------|--------------|\r\n| Accepting 'none' algorithm | Token forgery | Explicit algorithm allowlist |\r\n| Secret in code | Key compromise | Use environment variables, vault |\r\n| Long-lived access tokens | Extended compromise window | Short TTL (15 min) + refresh |\r\n| Sensitive data in payload | Data exposure | Payload is not encrypted! |\r\n| No audience validation | Token misuse across services | Always validate 'aud' claim |\r\n| localStorage storage | XSS token theft | Use HttpOnly cookies |\r\n| No refresh token rotation | Replay attacks | Rotate on each refresh |\r\n| Missing 'kid' header | Key rotation issues | Always include key ID |\r\n\r\n## Checklist\r\n\r\n- [ ] Algorithm explicitly whitelisted (no 'none')\r\n- [ ] Asymmetric algorithm used (RS256/ES256)\r\n- [ ] Short expiration on access tokens (≤15 min)\r\n- [ ] Refresh tokens stored server-side\r\n- [ ] Refresh token rotation implemented\r\n- [ ] All claims validated (iss, aud, exp, iat)\r\n- [ ] Key ID (kid) included in header\r\n- [ ] JWKS endpoint for key distribution\r\n- [ ] Key rotation process documented\r\n- [ ] Tokens stored in HttpOnly cookies (not localStorage)\r\n- [ ] Token revocation mechanism implemented\r\n- [ ] No sensitive data in payload\r\n- [ ] Clock skew tolerance configured\r\n\r\n## References\r\n\r\n- [RFC 7519 - JWT](https://datatracker.ietf.org/doc/html/rfc7519)\r\n- [OWASP JWT Cheat Sheet](https://cheatsheetseries.owasp.org/cheatsheets/JSON_Web_Token_for_Java_Cheat_Sheet.html)\r\n- [JWT.io Debugger](https://jwt.io/)\r\n- [Auth0 JWT Vulnerabilities](https://auth0.com/blog/critical-vulnerabilities-in-json-web-token-libraries/)\r\n- [jose Library](https://github.com/panva/jose)\r\n"
  },
  {
    "id": "sec.multitenancy-basics",
    "title": "Multi-Tenancy Basics",
    "tags": [
      "security",
      "multitenancy",
      "saas",
      "tenant"
    ],
    "level": "intermediate",
    "stacks": [
      "nodejs",
      "python",
      "go",
      "all"
    ],
    "scope": "security",
    "maturity": "stable",
    "works_with": [
      "nodejs",
      "python",
      "go",
      "all"
    ],
    "path": "D:/Code/backend-engineering-kit/.shared/production-backend-kit/patterns/sec.multitenancy-basics.md",
    "type": "pattern",
    "sections": [],
    "checklist": [],
    "sources": [],
    "rawContent": "\r\n# Multi-Tenancy Basics\r\n\r\n## Problem\r\n\r\nSaaS applications need to serve multiple tenants (customers) from a single codebase while ensuring complete data isolation and proper tenant identification.\r\n\r\n## When to use\r\n\r\n- Building SaaS products with multiple customers\r\n- Need to isolate data between organizations\r\n- Want shared infrastructure with logical separation\r\n- Starting with a monolith but need multi-tenant from day 1\r\n\r\n## Solution\r\n\r\n### 1. Tenant Resolution Strategies\r\n\r\nChoose one or combine multiple strategies:\r\n\r\n```typescript\r\n// Strategy 1: Subdomain-based\r\n// customer1.yourapp.com → tenant_id = \"customer1\"\r\nfunction resolveFromSubdomain(host: string): string | null {\r\n  const parts = host.split('.');\r\n  if (parts.length >= 3) {\r\n    return parts[0]; // First subdomain is tenant\r\n  }\r\n  return null;\r\n}\r\n\r\n// Strategy 2: Header-based\r\n// X-Tenant-ID: customer1\r\nfunction resolveFromHeader(req: Request): string | null {\r\n  return req.headers['x-tenant-id'] as string || null;\r\n}\r\n\r\n// Strategy 3: JWT claim\r\n// { sub: \"user123\", tenant_id: \"customer1\" }\r\nfunction resolveFromJWT(token: DecodedToken): string | null {\r\n  return token.tenant_id || token.org_id || null;\r\n}\r\n\r\n// Strategy 4: Path-based (less common)\r\n// /api/v1/tenants/customer1/resources\r\nfunction resolveFromPath(path: string): string | null {\r\n  const match = path.match(/^\\/api\\/v\\d+\\/tenants\\/([^\\/]+)/);\r\n  return match ? match[1] : null;\r\n}\r\n```\r\n\r\n### 2. Tenant Context Middleware\r\n\r\n```typescript\r\ninterface TenantContext {\r\n  tenantId: string;\r\n  tenantName: string;\r\n  plan: 'free' | 'pro' | 'enterprise';\r\n  features: string[];\r\n}\r\n\r\n// Express middleware\r\nasync function tenantMiddleware(\r\n  req: Request,\r\n  res: Response,\r\n  next: NextFunction\r\n) {\r\n  // 1. Resolve tenant ID\r\n  const tenantId = resolveFromJWT(req.user) \r\n    || resolveFromHeader(req)\r\n    || resolveFromSubdomain(req.hostname);\r\n\r\n  if (!tenantId) {\r\n    return res.status(400).json({\r\n      error: { code: 'TENANT_REQUIRED', message: 'Tenant identification required' }\r\n    });\r\n  }\r\n\r\n  // 2. Load tenant config\r\n  const tenant = await tenantService.getById(tenantId);\r\n  if (!tenant) {\r\n    return res.status(404).json({\r\n      error: { code: 'TENANT_NOT_FOUND', message: 'Tenant not found' }\r\n    });\r\n  }\r\n\r\n  // 3. Attach to request\r\n  req.tenant = tenant;\r\n  \r\n  // 4. Set for async context (logging, DB queries)\r\n  asyncLocalStorage.run({ tenantId }, () => next());\r\n}\r\n```\r\n\r\n### 3. Database Approach Decision\r\n\r\n| Approach | Pros | Cons | Best For |\r\n|----------|------|------|----------|\r\n| **Shared DB + tenant_id column** | Simple, low cost | Query discipline needed | Startups, < 1000 tenants |\r\n| **Shared DB + schema per tenant** | Better isolation | Schema migrations harder | Medium scale |\r\n| **Database per tenant** | Full isolation | Ops complexity | Enterprise/compliance |\r\n\r\n**Recommendation for lite SaaS**: Start with shared DB + `tenant_id` column.\r\n\r\n## Pitfalls\r\n\r\n1. **Forgetting tenant filter**: Every query MUST include tenant_id\r\n2. **Cross-tenant data leaks**: Test with multiple tenant users\r\n3. **Tenant in URL but not validated**: Always verify against authenticated user\r\n4. **Missing tenant in async operations**: Jobs, webhooks need tenant context\r\n\r\n## Checklist\r\n\r\n- [ ] Tenant resolution strategy chosen and documented\r\n- [ ] Middleware validates tenant on every request\r\n- [ ] Tenant context available in logs\r\n- [ ] Database queries scoped to tenant\r\n- [ ] Background jobs preserve tenant context\r\n- [ ] Webhooks validate tenant ownership\r\n"
  },
  {
    "id": "sec-password-storage",
    "title": "Password Storage",
    "tags": [
      "security",
      "passwords",
      "hashing",
      "authentication",
      "argon2",
      "bcrypt"
    ],
    "level": "intermediate",
    "stacks": [
      "all"
    ],
    "scope": "security",
    "maturity": "stable",
    "works_with": [
      "all"
    ],
    "path": "D:/Code/backend-engineering-kit/.shared/production-backend-kit/patterns/sec.password-storage.md",
    "type": "pattern",
    "sections": [],
    "checklist": [],
    "sources": [],
    "rawContent": "\r\n# Password Storage\r\n\r\n## Problem\r\n\r\nWeak password storage leads to credential theft and account compromise. Plaintext or poorly hashed passwords, once leaked, expose users across multiple sites due to password reuse. This is consistently in OWASP Top 10.\r\n\r\n## When to use\r\n\r\n- User registration systems\r\n- Any password-based authentication\r\n- Migrating legacy password systems\r\n- Internal admin accounts\r\n- API key/secret storage (hash if one-way needed)\r\n\r\n## Solution\r\n\r\n### 1. Algorithm Selection (in order of preference)\r\n\r\n| Algorithm | Recommendation | Notes |\r\n|-----------|----------------|-------|\r\n| **Argon2id** | ✅ Best | PHC winner, memory-hard, resists GPU/ASIC |\r\n| **bcrypt** | ✅ Good | Proven, wide support, 72-byte limit |\r\n| **scrypt** | ✅ Good | Memory-hard, more complex to tune |\r\n| PBKDF2-SHA256 | ⚠️ Legacy | Only if required (FIPS), needs high iterations |\r\n| SHA-256/512 | ❌ Never | Too fast, even with salt |\r\n| MD5/SHA1 | ❌ Never | Cryptographically broken |\r\n\r\n### 2. Argon2id Configuration (OWASP Recommended)\r\n\r\n```typescript\r\n// Recommended: tune to take ~250-500ms on your hardware\r\nconst argon2Config = {\r\n  type: argon2.argon2id,      // Hybrid mode (recommended)\r\n  memoryCost: 65536,          // 64 MB (minimum 47104 KB per OWASP)\r\n  timeCost: 3,                // 3 iterations (minimum 1)\r\n  parallelism: 4,             // 4 parallel threads\r\n  hashLength: 32,             // 32 bytes output\r\n  saltLength: 16,             // 16 bytes salt (auto-generated)\r\n};\r\n\r\n// Node.js with argon2\r\nimport * as argon2 from 'argon2';\r\n\r\nasync function hashPassword(password: string): Promise<string> {\r\n  return argon2.hash(password, argon2Config);\r\n}\r\n\r\nasync function verifyPassword(password: string, hash: string): Promise<boolean> {\r\n  return argon2.verify(hash, password);\r\n}\r\n```\r\n\r\n### 3. bcrypt Configuration\r\n\r\n```typescript\r\nimport * as bcrypt from 'bcrypt';\r\n\r\n// Cost factor: 2^cost iterations. Target 250ms+\r\n// 10 = ~100ms, 12 = ~300ms, 14 = ~1s (tune for your hardware)\r\nconst BCRYPT_COST = 12;\r\n\r\nasync function hashPassword(password: string): Promise<string> {\r\n  // Salt is auto-generated and embedded in hash\r\n  return bcrypt.hash(password, BCRYPT_COST);\r\n}\r\n\r\nasync function verifyPassword(password: string, hash: string): Promise<boolean> {\r\n  return bcrypt.compare(password, hash);\r\n}\r\n\r\n// Note: bcrypt truncates passwords > 72 bytes\r\n// Pre-hash with SHA-256 if longer passwords needed:\r\nasync function hashLongPassword(password: string): Promise<string> {\r\n  const preHash = crypto.createHash('sha256').update(password).digest('base64');\r\n  return bcrypt.hash(preHash, BCRYPT_COST);\r\n}\r\n```\r\n\r\n### 4. Password Strength & Breach Detection\r\n\r\n```typescript\r\nimport { pwnedPassword } from 'hibp';\r\n\r\nasync function validatePassword(password: string): Promise<void> {\r\n  // Length requirement (NIST recommends 8 min, we prefer 12)\r\n  if (password.length < 12) {\r\n    throw new ValidationError('Password must be at least 12 characters');\r\n  }\r\n  \r\n  // Maximum length (prevent DoS via very long passwords)\r\n  if (password.length > 128) {\r\n    throw new ValidationError('Password must be at most 128 characters');\r\n  }\r\n  \r\n  // Check against breached passwords (HaveIBeenPwned)\r\n  const breachCount = await pwnedPassword(password);\r\n  if (breachCount > 0) {\r\n    throw new ValidationError(\r\n      'This password has appeared in data breaches. Please choose a different password.'\r\n    );\r\n  }\r\n}\r\n```\r\n\r\n### 5. Hash Upgrade on Login\r\n\r\n```typescript\r\nasync function login(email: string, password: string): Promise<User> {\r\n  const user = await db.users.findByEmail(email);\r\n  if (!user) throw new AuthError('Invalid credentials');\r\n  \r\n  const valid = await verifyPassword(password, user.passwordHash);\r\n  if (!valid) {\r\n    await recordFailedLogin(user.id);\r\n    throw new AuthError('Invalid credentials');\r\n  }\r\n  \r\n  // Upgrade legacy hash if needed\r\n  if (needsRehash(user.passwordHash)) {\r\n    const newHash = await hashPassword(password);\r\n    await db.users.update(user.id, { passwordHash: newHash });\r\n    logger.info({ event: 'PASSWORD_HASH_UPGRADED', userId: user.id });\r\n  }\r\n  \r\n  return user;\r\n}\r\n\r\nfunction needsRehash(hash: string): boolean {\r\n  // Check if using old algorithm or low cost factor\r\n  if (hash.startsWith('$2a$') || hash.startsWith('$2b$')) {\r\n    const cost = parseInt(hash.split('$')[2], 10);\r\n    return cost < BCRYPT_COST; // Upgrade if cost too low\r\n  }\r\n  if (!hash.startsWith('$argon2id$')) {\r\n    return true; // Not Argon2id, needs upgrade\r\n  }\r\n  return false;\r\n}\r\n```\r\n\r\n## Pitfalls\r\n\r\n| Pitfall | How to Avoid |\r\n|---------|--------------|\r\n| Using fast hashes (MD5/SHA) | Use bcrypt, Argon2, or scrypt |\r\n| Global salt for all users | Unique salt per password |\r\n| Low work factor | Target 250ms+ hash time |\r\n| Not upgrading old hashes | Re-hash on successful login |\r\n| Timing attacks | Use constant-time comparison |\r\n\r\n## Checklist\r\n\r\n- [ ] Argon2id or bcrypt used\r\n- [ ] Unique salt per password\r\n- [ ] Work factor targets 250ms+ hash time\r\n- [ ] Password requirements enforced (length, complexity)\r\n- [ ] Compromised password check integrated\r\n- [ ] Rate limiting on login attempts\r\n- [ ] Legacy hashes upgraded on login\r\n- [ ] Password stored in separate secure store\r\n- [ ] Constant-time comparison used\r\n- [ ] MFA supported/encouraged\r\n\r\n## Snippets (Generic)\r\n\r\n```\r\nArgon2id Configuration:\r\n- Memory: 64 MB (65536 KB)\r\n- Iterations: 3\r\n- Parallelism: 4\r\n- Hash length: 32 bytes\r\n- Salt length: 16 bytes\r\n\r\nbcrypt Configuration:\r\n- Work factor (cost): 12-14 (adjust per hardware)\r\n- Auto-generates salt\r\n\r\nHash Storage Format:\r\n$argon2id$v=19$m=65536,t=3,p=4$salt_base64$hash_base64\r\n\r\nPassword Hashing Steps:\r\n1. Receive plaintext password\r\n2. Validate strength requirements\r\n3. Generate cryptographically random salt\r\n4. Hash with configured algorithm\r\n5. Store full hash string (includes params)\r\n\r\nVerification Steps:\r\n1. Retrieve stored hash\r\n2. Parse algorithm and parameters\r\n3. Hash input with same params + salt\r\n4. Constant-time compare result\r\n5. If match and old algorithm → upgrade hash\r\n\r\nPassword Requirements:\r\n- Minimum length: 12 characters\r\n- No maximum (up to 128)\r\n- Check against breached password list\r\n- No composition rules (allow any chars)\r\n\r\nHash Upgrade on Login:\r\nif verify_password(input, stored_hash):\r\n  if is_legacy_algorithm(stored_hash):\r\n    new_hash = hash_password(input)  # New algorithm\r\n    update_user_password_hash(user, new_hash)\r\n  return login_success()\r\n```\r\n\r\n## Sources\r\n\r\n- OWASP Password Storage Cheat Sheet: https://cheatsheetseries.owasp.org/cheatsheets/Password_Storage_Cheat_Sheet.html\r\n- Password Hashing Competition (Argon2): https://www.password-hashing.net/\r\n- Have I Been Pwned API: https://haveibeenpwned.com/API/v3\r\n- NIST Digital Identity Guidelines: https://pages.nist.gov/800-63-3/sp800-63b.html\r\n"
  },
  {
    "id": "sec-rate-limiting",
    "title": "Rate Limiting",
    "tags": [
      "security",
      "rate-limiting",
      "ddos",
      "availability",
      "redis"
    ],
    "level": "intermediate",
    "stacks": [
      "all"
    ],
    "scope": "security",
    "maturity": "stable",
    "works_with": [
      "all"
    ],
    "path": "D:/Code/backend-engineering-kit/.shared/production-backend-kit/patterns/sec.rate-limiting.md",
    "type": "pattern",
    "sections": [],
    "checklist": [],
    "sources": [],
    "rawContent": "\r\n# Rate Limiting\r\n\r\n## Problem\r\n\r\nWithout rate limiting, malicious users or buggy clients can overwhelm your API with requests, causing denial of service, increased costs, and degraded experience for legitimate users. It's also a key defense against credential stuffing and brute force attacks.\r\n\r\n## When to use\r\n\r\n- All public APIs\r\n- Authentication endpoints (login, password reset, MFA)\r\n- Resource-intensive operations (search, export)\r\n- Paid API tiers with quotas\r\n- Protecting against scraping and abuse\r\n\r\n## Solution\r\n\r\n### 1. Rate Limiting Algorithms\r\n\r\n| Algorithm | Pros | Cons | Best For |\r\n|-----------|------|------|----------|\r\n| **Token Bucket** | Allows bursts, smooth | Slightly complex | APIs with burst traffic |\r\n| **Sliding Window** | Fair distribution | Memory overhead | General use |\r\n| **Fixed Window** | Simple | Boundary burst problem | Simple use cases |\r\n| **Leaky Bucket** | Smooth output rate | No burst handling | Strict rate control |\r\n\r\n### 2. Token Bucket Implementation\r\n\r\n```typescript\r\nimport Redis from 'ioredis';\r\n\r\nconst redis = new Redis();\r\n\r\ninterface RateLimitResult {\r\n  allowed: boolean;\r\n  remaining: number;\r\n  resetAt: number;\r\n}\r\n\r\nasync function tokenBucketRateLimit(\r\n  key: string,\r\n  capacity: number,      // Max tokens (burst size)\r\n  refillRate: number,    // Tokens per second\r\n  tokensRequired: number = 1\r\n): Promise<RateLimitResult> {\r\n  const now = Date.now();\r\n  const bucketKey = `ratelimit:${key}`;\r\n  \r\n  // Lua script for atomic operation\r\n  const result = await redis.eval(`\r\n    local bucket = redis.call('HMGET', KEYS[1], 'tokens', 'lastRefill')\r\n    local tokens = tonumber(bucket[1]) or ARGV[1]\r\n    local lastRefill = tonumber(bucket[2]) or ARGV[4]\r\n    \r\n    -- Refill tokens based on time passed\r\n    local elapsed = (ARGV[4] - lastRefill) / 1000\r\n    local refill = elapsed * ARGV[2]\r\n    tokens = math.min(ARGV[1], tokens + refill)\r\n    \r\n    -- Try to consume tokens\r\n    local allowed = 0\r\n    if tokens >= ARGV[3] then\r\n      tokens = tokens - ARGV[3]\r\n      allowed = 1\r\n    end\r\n    \r\n    -- Update bucket\r\n    redis.call('HMSET', KEYS[1], 'tokens', tokens, 'lastRefill', ARGV[4])\r\n    redis.call('EXPIRE', KEYS[1], ARGV[5])\r\n    \r\n    return {allowed, tokens}\r\n  `, 1, bucketKey, capacity, refillRate, tokensRequired, now, 3600) as [number, number];\r\n  \r\n  return {\r\n    allowed: result[0] === 1,\r\n    remaining: Math.floor(result[1]),\r\n    resetAt: now + Math.ceil((capacity - result[1]) / refillRate) * 1000,\r\n  };\r\n}\r\n```\r\n\r\n### 3. Sliding Window Counter (Simpler)\r\n\r\n```typescript\r\nasync function slidingWindowRateLimit(\r\n  key: string,\r\n  limit: number,\r\n  windowSeconds: number\r\n): Promise<RateLimitResult> {\r\n  const now = Date.now();\r\n  const windowKey = `ratelimit:${key}:${Math.floor(now / 1000 / windowSeconds)}`;\r\n  const prevWindowKey = `ratelimit:${key}:${Math.floor(now / 1000 / windowSeconds) - 1}`;\r\n  \r\n  const [current, previous] = await redis.mget(windowKey, prevWindowKey);\r\n  \r\n  // Weight previous window by remaining time\r\n  const windowStart = Math.floor(now / 1000 / windowSeconds) * windowSeconds * 1000;\r\n  const elapsed = (now - windowStart) / (windowSeconds * 1000);\r\n  const weightedCount = (parseInt(current || '0') + \r\n    parseInt(previous || '0') * (1 - elapsed));\r\n  \r\n  if (weightedCount >= limit) {\r\n    return {\r\n      allowed: false,\r\n      remaining: 0,\r\n      resetAt: windowStart + windowSeconds * 1000,\r\n    };\r\n  }\r\n  \r\n  await redis.incr(windowKey);\r\n  await redis.expire(windowKey, windowSeconds * 2);\r\n  \r\n  return {\r\n    allowed: true,\r\n    remaining: Math.floor(limit - weightedCount - 1),\r\n    resetAt: windowStart + windowSeconds * 1000,\r\n  };\r\n}\r\n```\r\n\r\n### 4. Rate Limit Middleware\r\n\r\n```typescript\r\nimport { RateLimiterRedis } from 'rate-limiter-flexible';\r\n\r\nconst rateLimiter = new RateLimiterRedis({\r\n  storeClient: redis,\r\n  keyPrefix: 'rl',\r\n  points: 100,        // 100 requests\r\n  duration: 60,       // per 60 seconds\r\n  blockDuration: 60,  // Block for 60s if exceeded\r\n});\r\n\r\nconst strictLimiter = new RateLimiterRedis({\r\n  storeClient: redis,\r\n  keyPrefix: 'rl:auth',\r\n  points: 5,          // 5 attempts\r\n  duration: 60,       // per minute\r\n  blockDuration: 300, // Block for 5 minutes\r\n});\r\n\r\nfunction rateLimitMiddleware(limiter: RateLimiterRedis) {\r\n  return async (req: Request, res: Response, next: NextFunction) => {\r\n    const key = req.user?.id || req.ip; // User ID or IP\r\n    \r\n    try {\r\n      const result = await limiter.consume(key);\r\n      \r\n      res.set({\r\n        'X-RateLimit-Limit': limiter.points,\r\n        'X-RateLimit-Remaining': result.remainingPoints,\r\n        'X-RateLimit-Reset': new Date(Date.now() + result.msBeforeNext).toISOString(),\r\n      });\r\n      \r\n      next();\r\n    } catch (rateLimitRes) {\r\n      res.set({\r\n        'X-RateLimit-Limit': limiter.points,\r\n        'X-RateLimit-Remaining': 0,\r\n        'X-RateLimit-Reset': new Date(Date.now() + rateLimitRes.msBeforeNext).toISOString(),\r\n        'Retry-After': Math.ceil(rateLimitRes.msBeforeNext / 1000),\r\n      });\r\n      \r\n      res.status(429).json({\r\n        error: 'Too Many Requests',\r\n        retryAfter: Math.ceil(rateLimitRes.msBeforeNext / 1000),\r\n      });\r\n    }\r\n  };\r\n}\r\n\r\n// Usage\r\napp.use('/api', rateLimitMiddleware(rateLimiter));\r\napp.post('/api/login', rateLimitMiddleware(strictLimiter), loginHandler);\r\n```\r\n\r\n## Pitfalls\r\n\r\n| Pitfall | How to Avoid |\r\n|---------|--------------|\r\n| Only IP-based limiting | Combine with API key/user limits |\r\n| Not handling authenticated users | Different limits for auth vs anon |\r\n| Blocking on Redis failure | Fail open with logging, or local fallback |\r\n| Harsh limits on first deploy | Start lenient, tighten based on data |\r\n| No burst allowance | Token bucket allows reasonable bursts |\r\n\r\n## Checklist\r\n\r\n- [ ] Rate limit algorithm chosen\r\n- [ ] Limits defined per scope (global, user, IP)\r\n- [ ] Login/auth endpoints have stricter limits\r\n- [ ] 429 response returned with Retry-After\r\n- [ ] Rate limit headers included in responses\r\n- [ ] Distributed storage (Redis) configured\r\n- [ ] Failure mode defined (open/closed)\r\n- [ ] Limits documented for consumers\r\n- [ ] Monitoring alerts on rate limit hits\r\n- [ ] Different limits for API tiers\r\n\r\n## Snippets (Generic)\r\n\r\n```\r\nResponse Headers:\r\nHTTP/1.1 429 Too Many Requests\r\nX-RateLimit-Limit: 1000\r\nX-RateLimit-Remaining: 0\r\nX-RateLimit-Reset: 1642000000\r\nRetry-After: 60\r\n\r\nToken Bucket Algorithm:\r\n1. Bucket holds N tokens (capacity)\r\n2. Tokens added at rate R per second\r\n3. Each request consumes 1 token\r\n4. If no tokens → reject with 429\r\n5. Allows bursts up to capacity\r\n\r\nRedis Rate Limiter (pseudo):\r\nkey = \"rate_limit:{user_id}:{window}\"\r\ncurrent = redis.incr(key)\r\nif current == 1:\r\n  redis.expire(key, window_seconds)\r\nif current > limit:\r\n  return 429\r\n\r\nSliding Window:\r\n1. Count requests in last N seconds\r\n2. Use sorted set with request timestamps\r\n3. Remove expired entries\r\n4. Check count against limit\r\n\r\nTypical Limits:\r\n- Anonymous: 100/minute per IP\r\n- Authenticated: 1000/minute per user\r\n- Login attempts: 5/minute per IP\r\n- Password reset: 3/hour per email\r\n- Expensive operations: 10/minute per user\r\n```\r\n\r\n## Sources\r\n\r\n- Stripe Rate Limiting: https://stripe.com/blog/rate-limiters\r\n- Cloudflare Rate Limiting: https://developers.cloudflare.com/waf/rate-limiting-rules/\r\n- Redis Rate Limiting Patterns: https://redis.io/commands/incr/#pattern-rate-limiter\r\n- Token Bucket Algorithm: https://en.wikipedia.org/wiki/Token_bucket\r\n"
  },
  {
    "id": "sec.rbac-boundaries",
    "title": "RBAC & Authorization Boundaries",
    "tags": [
      "security",
      "rbac",
      "authorization",
      "permissions"
    ],
    "level": "intermediate",
    "stacks": [
      "nodejs",
      "python",
      "go",
      "all"
    ],
    "scope": "security",
    "maturity": "stable",
    "works_with": [
      "nodejs",
      "python",
      "go",
      "all"
    ],
    "path": "D:/Code/backend-engineering-kit/.shared/production-backend-kit/patterns/sec.rbac-boundaries.md",
    "type": "pattern",
    "sections": [],
    "checklist": [],
    "sources": [],
    "rawContent": "\r\n# RBAC & Authorization Boundaries\r\n\r\n## Problem\r\n\r\nMulti-tenant SaaS needs fine-grained access control: users have roles within their tenant, and permissions must be checked on every operation.\r\n\r\n## When to use\r\n\r\n- SaaS with multiple users per tenant\r\n- Different permission levels (admin, member, viewer)\r\n- Need to control feature access by plan\r\n- Audit requirements for access decisions\r\n\r\n## Solution\r\n\r\n### 1. Role & Permission Model\r\n\r\n```typescript\r\n// Roles are tenant-scoped\r\ninterface Role {\r\n  id: string;\r\n  tenantId: string;\r\n  name: string;\r\n  permissions: Permission[];\r\n  isSystem: boolean; // Built-in vs custom\r\n}\r\n\r\n// Permissions are global definitions\r\ntype Permission = \r\n  | 'users:read' | 'users:write' | 'users:delete'\r\n  | 'orders:read' | 'orders:write' | 'orders:delete'\r\n  | 'billing:read' | 'billing:write'\r\n  | 'settings:read' | 'settings:write'\r\n  | 'admin:*';\r\n\r\n// Default roles\r\nconst DEFAULT_ROLES: Record<string, Permission[]> = {\r\n  owner: ['admin:*'],\r\n  admin: ['users:*', 'orders:*', 'settings:*', 'billing:read'],\r\n  member: ['orders:read', 'orders:write'],\r\n  viewer: ['orders:read'],\r\n};\r\n```\r\n\r\n### 2. Permission Check Middleware\r\n\r\n```typescript\r\nfunction requirePermission(...required: Permission[]) {\r\n  return async (req: Request, res: Response, next: NextFunction) => {\r\n    const user = req.user;\r\n    const tenant = req.tenant;\r\n    \r\n    // Get user's roles in this tenant\r\n    const userRoles = await roleService.getUserRoles(user.id, tenant.id);\r\n    const userPermissions = new Set<string>();\r\n    \r\n    for (const role of userRoles) {\r\n      for (const perm of role.permissions) {\r\n        userPermissions.add(perm);\r\n        // Handle wildcards\r\n        if (perm.endsWith(':*')) {\r\n          const resource = perm.replace(':*', '');\r\n          ['read', 'write', 'delete'].forEach(action => \r\n            userPermissions.add(`${resource}:${action}`)\r\n          );\r\n        }\r\n        // admin:* grants everything\r\n        if (perm === 'admin:*') {\r\n          return next();\r\n        }\r\n      }\r\n    }\r\n    \r\n    // Check all required permissions\r\n    const hasAll = required.every(p => userPermissions.has(p));\r\n    if (!hasAll) {\r\n      logger.warn({\r\n        event: 'PERMISSION_DENIED',\r\n        userId: user.id,\r\n        tenantId: tenant.id,\r\n        required,\r\n        had: [...userPermissions],\r\n      });\r\n      return res.status(403).json({\r\n        error: { code: 'FORBIDDEN', message: 'Insufficient permissions' }\r\n      });\r\n    }\r\n    \r\n    next();\r\n  };\r\n}\r\n\r\n// Usage\r\napp.get('/api/users', \r\n  requirePermission('users:read'),\r\n  userController.list\r\n);\r\n\r\napp.delete('/api/users/:id',\r\n  requirePermission('users:delete'),\r\n  userController.remove\r\n);\r\n```\r\n\r\n### 3. Resource-Level Authorization\r\n\r\n```typescript\r\n// Not just \"can user do X?\" but \"can user do X to THIS resource?\"\r\nasync function canAccessResource(\r\n  userId: string,\r\n  tenantId: string,\r\n  resourceType: string,\r\n  resourceId: string,\r\n  action: 'read' | 'write' | 'delete'\r\n): Promise<boolean> {\r\n  // 1. Check tenant ownership\r\n  const resource = await getResource(resourceType, resourceId);\r\n  if (resource.tenant_id !== tenantId) {\r\n    return false; // Cross-tenant access denied\r\n  }\r\n  \r\n  // 2. Check user permissions\r\n  const hasPermission = await hasUserPermission(\r\n    userId, \r\n    tenantId, \r\n    `${resourceType}:${action}`\r\n  );\r\n  if (!hasPermission) {\r\n    return false;\r\n  }\r\n  \r\n  // 3. Check resource-specific rules\r\n  if (resourceType === 'order' && action === 'write') {\r\n    // Only owner or assigned user can edit\r\n    if (resource.created_by !== userId && resource.assigned_to !== userId) {\r\n      const isAdmin = await hasUserPermission(userId, tenantId, 'admin:*');\r\n      if (!isAdmin) return false;\r\n    }\r\n  }\r\n  \r\n  return true;\r\n}\r\n```\r\n\r\n### 4. Feature Flags by Plan\r\n\r\n```typescript\r\ninterface TenantPlan {\r\n  name: 'free' | 'pro' | 'enterprise';\r\n  features: string[];\r\n  limits: Record<string, number>;\r\n}\r\n\r\nconst PLANS: Record<string, TenantPlan> = {\r\n  free: {\r\n    name: 'free',\r\n    features: ['basic_reports'],\r\n    limits: { users: 3, projects: 5 }\r\n  },\r\n  pro: {\r\n    name: 'pro', \r\n    features: ['basic_reports', 'advanced_reports', 'api_access'],\r\n    limits: { users: 25, projects: 50 }\r\n  },\r\n  enterprise: {\r\n    name: 'enterprise',\r\n    features: ['*'],\r\n    limits: { users: -1, projects: -1 } // Unlimited\r\n  }\r\n};\r\n\r\nfunction requireFeature(feature: string) {\r\n  return (req: Request, res: Response, next: NextFunction) => {\r\n    const tenant = req.tenant;\r\n    const plan = PLANS[tenant.plan];\r\n    \r\n    if (!plan.features.includes(feature) && !plan.features.includes('*')) {\r\n      return res.status(403).json({\r\n        error: { \r\n          code: 'FEATURE_NOT_AVAILABLE',\r\n          message: `Upgrade to access ${feature}`,\r\n          upgradeUrl: `/billing/upgrade`\r\n        }\r\n      });\r\n    }\r\n    next();\r\n  };\r\n}\r\n```\r\n\r\n## Pitfalls\r\n\r\n1. **Checking role instead of permission**: Roles change, permissions are stable\r\n2. **Client-only checks**: Always verify on server\r\n3. **Forgot tenant scope**: User in Tenant A shouldn't see Tenant B roles\r\n4. **Permission creep**: Regular audits of who has what\r\n\r\n## Checklist\r\n\r\n- [ ] Permission model defined (not just roles)\r\n- [ ] Middleware checks on all protected routes\r\n- [ ] Resource-level authorization implemented\r\n- [ ] Feature flags by plan working\r\n- [ ] Audit log for permission denials\r\n- [ ] Regular permission review process\r\n"
  },
  {
    "id": "sec-secrets-management",
    "title": "Secrets Management",
    "tags": [
      "security",
      "secrets",
      "configuration",
      "devops",
      "vault"
    ],
    "level": "intermediate",
    "stacks": [
      "all"
    ],
    "scope": "security",
    "maturity": "stable",
    "works_with": [
      "all"
    ],
    "path": "D:/Code/backend-engineering-kit/.shared/production-backend-kit/patterns/sec.secrets-management.md",
    "type": "pattern",
    "sections": [],
    "checklist": [],
    "sources": [],
    "rawContent": "\r\n# Secrets Management\r\n\r\n## Problem\r\n\r\nHardcoded secrets in code or config files get committed to version control, exposed in logs, and leaked through breaches. Poor secret management is a top cause of security incidents and is easy to prevent with proper tooling.\r\n\r\n## When to use\r\n\r\n- API keys, tokens, credentials\r\n- Database connection strings\r\n- Encryption keys\r\n- Third-party service credentials\r\n- Any sensitive configuration\r\n- Certificates and private keys\r\n\r\n## Solution\r\n\r\n### 1. Secret Storage Solutions\r\n\r\n| Solution | Best For | Features |\r\n|----------|----------|----------|\r\n| **HashiCorp Vault** | On-prem, multi-cloud | Dynamic secrets, encryption as service |\r\n| **AWS Secrets Manager** | AWS workloads | Auto-rotation, RDS integration |\r\n| **AWS Parameter Store** | AWS, simpler needs | Cheaper, hierarchical, versioned |\r\n| **Azure Key Vault** | Azure workloads | HSM-backed, managed identities |\r\n| **Google Secret Manager** | GCP workloads | IAM integration, versioning |\r\n| **1Password/Doppler** | Dev teams, CI/CD | Easy adoption, good DX |\r\n\r\n### 2. Runtime Secret Injection\r\n\r\n**AWS Secrets Manager (Node.js):**\r\n```typescript\r\nimport { SecretsManagerClient, GetSecretValueCommand } from '@aws-sdk/client-secrets-manager';\r\n\r\nconst client = new SecretsManagerClient({ region: 'us-east-1' });\r\n\r\ninterface DatabaseSecret {\r\n  username: string;\r\n  password: string;\r\n  host: string;\r\n  port: number;\r\n}\r\n\r\nasync function getSecret<T>(secretId: string): Promise<T> {\r\n  const command = new GetSecretValueCommand({ SecretId: secretId });\r\n  const response = await client.send(command);\r\n  return JSON.parse(response.SecretString!);\r\n}\r\n\r\n// Cache secrets with TTL\r\nconst secretCache = new Map<string, { value: any; expiresAt: number }>();\r\nconst CACHE_TTL = 5 * 60 * 1000; // 5 minutes\r\n\r\nasync function getCachedSecret<T>(secretId: string): Promise<T> {\r\n  const cached = secretCache.get(secretId);\r\n  if (cached && cached.expiresAt > Date.now()) {\r\n    return cached.value;\r\n  }\r\n  \r\n  const secret = await getSecret<T>(secretId);\r\n  secretCache.set(secretId, { value: secret, expiresAt: Date.now() + CACHE_TTL });\r\n  return secret;\r\n}\r\n\r\n// Usage\r\nconst dbSecret = await getCachedSecret<DatabaseSecret>('prod/database/credentials');\r\n```\r\n\r\n**HashiCorp Vault (Node.js):**\r\n```typescript\r\nimport vault from 'node-vault';\r\n\r\nconst vaultClient = vault({\r\n  endpoint: process.env.VAULT_ADDR,\r\n  token: process.env.VAULT_TOKEN, // Or use AppRole auth\r\n});\r\n\r\nasync function getVaultSecret(path: string): Promise<Record<string, string>> {\r\n  const result = await vaultClient.read(`secret/data/${path}`);\r\n  return result.data.data;\r\n}\r\n\r\n// AppRole authentication (recommended for apps)\r\nasync function authenticateAppRole(): Promise<string> {\r\n  const result = await vaultClient.approleLogin({\r\n    role_id: process.env.VAULT_ROLE_ID,\r\n    secret_id: process.env.VAULT_SECRET_ID,\r\n  });\r\n  return result.auth.client_token;\r\n}\r\n```\r\n\r\n### 3. Kubernetes Secrets (with External Secrets Operator)\r\n\r\n```yaml\r\n# ExternalSecret syncs from AWS Secrets Manager\r\napiVersion: external-secrets.io/v1beta1\r\nkind: ExternalSecret\r\nmetadata:\r\n  name: database-credentials\r\nspec:\r\n  refreshInterval: 1h\r\n  secretStoreRef:\r\n    name: aws-secrets-manager\r\n    kind: SecretStore\r\n  target:\r\n    name: db-secret\r\n  data:\r\n    - secretKey: username\r\n      remoteRef:\r\n        key: prod/database/credentials\r\n        property: username\r\n    - secretKey: password\r\n      remoteRef:\r\n        key: prod/database/credentials\r\n        property: password\r\n```\r\n\r\n### 4. Secret Rotation\r\n\r\n```typescript\r\n// Automated rotation with AWS Secrets Manager\r\n// This Lambda is triggered by Secrets Manager\r\nexport async function handler(event: RotationEvent): Promise<void> {\r\n  const { SecretId, ClientRequestToken, Step } = event;\r\n  \r\n  switch (Step) {\r\n    case 'createSecret':\r\n      // Generate new credentials\r\n      const newPassword = generateSecurePassword();\r\n      await secretsManager.putSecretValue({\r\n        SecretId,\r\n        ClientRequestToken,\r\n        SecretString: JSON.stringify({ ...currentSecret, password: newPassword }),\r\n        VersionStage: 'AWSPENDING',\r\n      });\r\n      break;\r\n      \r\n    case 'setSecret':\r\n      // Update the database with new credentials\r\n      await updateDatabasePassword(newPassword);\r\n      break;\r\n      \r\n    case 'testSecret':\r\n      // Verify new credentials work\r\n      await testDatabaseConnection(newPassword);\r\n      break;\r\n      \r\n    case 'finishSecret':\r\n      // Mark new version as current\r\n      await secretsManager.updateSecretVersionStage({\r\n        SecretId,\r\n        VersionStage: 'AWSCURRENT',\r\n        MoveToVersionId: ClientRequestToken,\r\n      });\r\n      break;\r\n  }\r\n}\r\n```\r\n\r\n### 5. Git Pre-Commit Hook (Secret Scanning)\r\n\r\n```yaml\r\n# .pre-commit-config.yaml\r\nrepos:\r\n  - repo: https://github.com/Yelp/detect-secrets\r\n    rev: v1.4.0\r\n    hooks:\r\n      - id: detect-secrets\r\n        args: ['--baseline', '.secrets.baseline']\r\n  \r\n  - repo: https://github.com/awslabs/git-secrets\r\n    rev: master\r\n    hooks:\r\n      - id: git-secrets\r\n```\r\n\r\n```bash\r\n# Initialize git-secrets\r\ngit secrets --install\r\ngit secrets --register-aws\r\n\r\n# Add custom patterns\r\ngit secrets --add 'private_key'\r\ngit secrets --add 'api[_-]?key'\r\ngit secrets --add 'password\\s*=\\s*[\"\\'][^\"\\']+[\"\\']'\r\n```\r\n\r\n## Pitfalls\r\n\r\n| Pitfall | How to Avoid |\r\n|---------|--------------|\r\n| Secrets in git history | Use git-secrets, scan before commit |\r\n| Logging credentials | Sanitize logs, mask patterns |\r\n| Same secrets across envs | Unique per environment |\r\n| Never rotating secrets | Schedule rotation quarterly |\r\n| Over-sharing access | Least privilege, need-to-know |\r\n\r\n## Checklist\r\n\r\n- [ ] No secrets in source code\r\n- [ ] No secrets in docker images\r\n- [ ] Secrets stored in dedicated tool\r\n- [ ] Environment-specific secrets\r\n- [ ] Secret access is audited\r\n- [ ] Rotation schedule defined\r\n- [ ] CI/CD secrets masked in logs\r\n- [ ] Git pre-commit hooks scan for secrets\r\n- [ ] Emergency rotation procedure documented\r\n- [ ] Access granted on need-to-know basis\r\n\r\n## Snippets (Generic)\r\n\r\n```\r\nEnvironment Variables:\r\n# .env (not committed)\r\nDATABASE_URL=postgres://user:pass@host/db\r\nAPI_KEY=secret_key_here\r\n\r\n# Application reads from env\r\ndb_url = os.environ.get('DATABASE_URL')\r\n\r\nSecret Manager Pattern:\r\n1. Store secret in vault/manager\r\n2. Application authenticates to secret service\r\n3. Fetch secret at runtime\r\n4. Cache with TTL (short-lived)\r\n5. Re-fetch on expiry or rotation\r\n\r\nAWS Secrets Manager:\r\naws secretsmanager get-secret-value --secret-id prod/db/password\r\n\r\nKubernetes Secret (encoded):\r\napiVersion: v1\r\nkind: Secret\r\nmetadata:\r\n  name: db-credentials\r\ntype: Opaque\r\ndata:\r\n  password: base64-encoded-value\r\n\r\nGit Pre-commit Hook:\r\n# .pre-commit-config.yaml\r\n- repo: https://github.com/awslabs/git-secrets\r\n  hooks:\r\n    - id: git-secrets\r\n\r\nRotation Steps:\r\n1. Generate new secret\r\n2. Add new secret (keep old active)\r\n3. Update applications to use new\r\n4. Verify all using new secret\r\n5. Revoke old secret\r\n```\r\n\r\n## Sources\r\n\r\n- OWASP Secrets Management Cheat Sheet: https://cheatsheetseries.owasp.org/cheatsheets/Secrets_Management_Cheat_Sheet.html\r\n- HashiCorp Vault: https://www.vaultproject.io/\r\n- AWS Secrets Manager: https://aws.amazon.com/secrets-manager/\r\n- 12 Factor App - Config: https://12factor.net/config\r\n"
  },
  {
    "id": "sec.tenant-context",
    "title": "Tenant Context Propagation",
    "tags": [
      "security",
      "multitenancy",
      "context",
      "async"
    ],
    "level": "intermediate",
    "stacks": [
      "nodejs",
      "python",
      "go",
      "all"
    ],
    "scope": "security",
    "maturity": "stable",
    "works_with": [
      "nodejs",
      "python",
      "go",
      "all"
    ],
    "path": "D:/Code/backend-engineering-kit/.shared/production-backend-kit/patterns/sec.tenant-context.md",
    "type": "pattern",
    "sections": [],
    "checklist": [],
    "sources": [],
    "rawContent": "\r\n# Tenant Context Propagation\r\n\r\n## Problem\r\n\r\nTenant context must flow seamlessly through the entire request lifecycle: from HTTP request → business logic → database queries → logs → background jobs.\r\n\r\n## When to use\r\n\r\n- Multi-tenant SaaS with shared database\r\n- Need consistent tenant scoping across layers\r\n- Want automatic tenant filtering in ORM/queries\r\n- Require tenant info in all log entries\r\n\r\n## Solution\r\n\r\n### 1. AsyncLocalStorage for Context (Node.js)\r\n\r\n```typescript\r\nimport { AsyncLocalStorage } from 'async_hooks';\r\n\r\ninterface RequestContext {\r\n  tenantId: string;\r\n  userId: string;\r\n  correlationId: string;\r\n  startTime: number;\r\n}\r\n\r\nexport const asyncContext = new AsyncLocalStorage<RequestContext>();\r\n\r\n// Get current context\r\nexport function getContext(): RequestContext | undefined {\r\n  return asyncContext.getStore();\r\n}\r\n\r\nexport function getTenantId(): string {\r\n  const ctx = getContext();\r\n  if (!ctx?.tenantId) {\r\n    throw new Error('No tenant context available');\r\n  }\r\n  return ctx.tenantId;\r\n}\r\n```\r\n\r\n### 2. Middleware Setup\r\n\r\n```typescript\r\napp.use((req, res, next) => {\r\n  const context: RequestContext = {\r\n    tenantId: req.tenant.id,\r\n    userId: req.user?.id || 'anonymous',\r\n    correlationId: req.headers['x-correlation-id'] || crypto.randomUUID(),\r\n    startTime: Date.now(),\r\n  };\r\n\r\n  asyncContext.run(context, () => {\r\n    // All downstream code has access to context\r\n    next();\r\n  });\r\n});\r\n```\r\n\r\n### 3. Database Query Scoping\r\n\r\n```typescript\r\n// Base repository with automatic tenant scoping\r\nclass TenantScopedRepository<T> {\r\n  constructor(private model: Model<T>) {}\r\n\r\n  async findAll(where: Partial<T> = {}): Promise<T[]> {\r\n    return this.model.find({\r\n      ...where,\r\n      tenant_id: getTenantId(), // Automatic!\r\n    });\r\n  }\r\n\r\n  async create(data: Omit<T, 'tenant_id'>): Promise<T> {\r\n    return this.model.create({\r\n      ...data,\r\n      tenant_id: getTenantId(), // Automatic!\r\n    });\r\n  }\r\n\r\n  async findById(id: string): Promise<T | null> {\r\n    return this.model.findOne({\r\n      id,\r\n      tenant_id: getTenantId(), // Prevents cross-tenant access\r\n    });\r\n  }\r\n}\r\n\r\n// Usage\r\nconst userRepo = new TenantScopedRepository(UserModel);\r\nconst users = await userRepo.findAll({ active: true });\r\n// SQL: SELECT * FROM users WHERE active = true AND tenant_id = 'current-tenant'\r\n```\r\n\r\n### 4. Structured Logging with Context\r\n\r\n```typescript\r\nimport pino from 'pino';\r\n\r\nfunction createLogger() {\r\n  return pino({\r\n    mixin() {\r\n      const ctx = getContext();\r\n      return ctx ? {\r\n        tenantId: ctx.tenantId,\r\n        userId: ctx.userId,\r\n        correlationId: ctx.correlationId,\r\n      } : {};\r\n    }\r\n  });\r\n}\r\n\r\nconst logger = createLogger();\r\n\r\n// All logs automatically include tenant context\r\nlogger.info({ action: 'order.created', orderId: '123' });\r\n// Output: { \"tenantId\": \"acme\", \"userId\": \"user1\", \"correlationId\": \"abc\", \"action\": \"order.created\" }\r\n```\r\n\r\n### 5. Background Job Context\r\n\r\n```typescript\r\n// When enqueuing jobs, include tenant context\r\nasync function enqueueJob(jobType: string, payload: any) {\r\n  const ctx = getContext();\r\n  \r\n  await jobQueue.add(jobType, {\r\n    ...payload,\r\n    _context: {\r\n      tenantId: ctx?.tenantId,\r\n      correlationId: ctx?.correlationId,\r\n    }\r\n  });\r\n}\r\n\r\n// Job processor restores context\r\njobQueue.process('*', async (job) => {\r\n  const { _context, ...payload } = job.data;\r\n  \r\n  // Restore context for the job\r\n  await asyncContext.run(_context, async () => {\r\n    await processJob(job.name, payload);\r\n  });\r\n});\r\n```\r\n\r\n## Pitfalls\r\n\r\n1. **Context lost in callbacks**: Use `asyncContext.run()` properly\r\n2. **Missing context in error handlers**: Log errors may lack tenant info\r\n3. **Third-party libraries**: Some bypass AsyncLocalStorage\r\n4. **Tests without context**: Mock context in unit tests\r\n\r\n## Checklist\r\n\r\n- [ ] AsyncLocalStorage (or equivalent) configured\r\n- [ ] Middleware sets context on every request\r\n- [ ] All repositories use tenant-scoped queries\r\n- [ ] Logs include tenant context automatically\r\n- [ ] Background jobs preserve and restore context\r\n- [ ] Error handlers have access to context\r\n"
  },
  {
    "id": "sec-threat-checklist",
    "title": "Security Threat Checklist",
    "tags": [
      "security",
      "owasp",
      "threats",
      "assessment"
    ],
    "level": "advanced",
    "stacks": [
      "all"
    ],
    "scope": "security",
    "maturity": "stable",
    "works_with": [
      "all"
    ],
    "path": "D:/Code/backend-engineering-kit/.shared/production-backend-kit/patterns/sec.threat-checklist.md",
    "type": "pattern",
    "sections": [],
    "checklist": [],
    "sources": [],
    "rawContent": "\r\n# Security Threat Checklist\r\n\r\n## Problem\r\n\r\nSecurity is often an afterthought, leading to vulnerabilities that could have been prevented with systematic review. Teams lack structured approach to identify and address common threats.\r\n\r\n## When to use\r\n\r\n- New application security review\r\n- Feature security assessment\r\n- Pre-launch security audit\r\n- Regular security check-ins\r\n- After security incident\r\n\r\n## Solution\r\n\r\n1. **Use OWASP Top 10 as baseline**\r\n   - Review against known vulnerability classes\r\n   - Systematic threat identification\r\n   - Prioritize by risk\r\n\r\n2. **Apply threat modeling**\r\n   - STRIDE: Spoofing, Tampering, Repudiation, Info Disclosure, DoS, Elevation\r\n   - Data flow diagrams\r\n   - Trust boundary analysis\r\n\r\n3. **Automate where possible**\r\n   - SAST (static analysis)\r\n   - DAST (dynamic analysis)\r\n   - Dependency scanning\r\n   - Secret scanning\r\n\r\n4. **Create security requirements**\r\n   - Convert threats to testable requirements\r\n   - Include in definition of done\r\n   - Security regression tests\r\n\r\n## Pitfalls\r\n\r\n| Pitfall | How to Avoid |\r\n|---------|--------------|\r\n| One-time security review | Make security continuous |\r\n| Only external threats | Consider insider threats too |\r\n| Ignoring dependencies | Scan all third-party packages |\r\n| No remediation tracking | File tickets, track resolution |\r\n| Security theater | Focus on real risks, not checkboxes |\r\n\r\n## Checklist\r\n\r\n### Injection\r\n- [ ] SQL injection prevented (parameterized queries)\r\n- [ ] NoSQL injection prevented\r\n- [ ] Command injection prevented\r\n- [ ] LDAP injection prevented\r\n- [ ] XSS (Cross-Site Scripting) prevented\r\n\r\n### Authentication\r\n- [ ] Passwords hashed with bcrypt/Argon2\r\n- [ ] Multi-factor authentication supported\r\n- [ ] Session management secure\r\n- [ ] Password reset flow secure\r\n- [ ] Account lockout after failed attempts\r\n\r\n### Authorization\r\n- [ ] Access controls enforced server-side\r\n- [ ] Principle of least privilege applied\r\n- [ ] IDOR (Insecure Direct Object Reference) prevented\r\n- [ ] Admin functions properly protected\r\n- [ ] API endpoints have authorization checks\r\n\r\n### Data Protection\r\n- [ ] Sensitive data encrypted at rest\r\n- [ ] TLS enforced for data in transit\r\n- [ ] Secrets not in source code\r\n- [ ] PII properly handled\r\n- [ ] Logging doesn't contain sensitive data\r\n\r\n### Configuration\r\n- [ ] Security headers configured (CSP, HSTS, etc.)\r\n- [ ] Debug mode disabled in production\r\n- [ ] Default credentials changed\r\n- [ ] Unnecessary features disabled\r\n- [ ] Error messages don't leak info\r\n\r\n### Dependencies\r\n- [ ] Dependencies regularly updated\r\n- [ ] Vulnerability scanning in CI/CD\r\n- [ ] No known vulnerabilities in deps\r\n- [ ] Software bill of materials maintained\r\n\r\n## Snippets (Generic)\r\n\r\n```\r\nSTRIDE Threat Categories:\r\nS - Spoofing: Can attacker pretend to be someone else?\r\nT - Tampering: Can attacker modify data?\r\nR - Repudiation: Can attacker deny actions?\r\nI - Info Disclosure: Can attacker access private data?\r\nD - Denial of Service: Can attacker crash/slow service?\r\nE - Elevation of Privilege: Can attacker gain more access?\r\n\r\nSecurity Review Process:\r\n1. Draw data flow diagram\r\n2. Identify trust boundaries\r\n3. Apply STRIDE to each component\r\n4. Rate risk (likelihood × impact)\r\n5. Define mitigations\r\n6. Track remediation\r\n\r\nMinimum Security Headers:\r\nContent-Security-Policy: default-src 'self'\r\nX-Content-Type-Options: nosniff\r\nX-Frame-Options: DENY\r\nStrict-Transport-Security: max-age=31536000\r\nX-XSS-Protection: 0 (rely on CSP instead)\r\n\r\nRegular Security Activities:\r\n- Weekly: Review dependency alerts\r\n- Monthly: Security awareness reminder\r\n- Quarterly: Penetration testing\r\n- Per release: Security review\r\n- Annually: Full security audit\r\n```\r\n\r\n## Sources\r\n\r\n- OWASP Top 10: https://owasp.org/www-project-top-ten/\r\n- OWASP Cheat Sheet Series: https://cheatsheetseries.owasp.org/\r\n- STRIDE Threat Modeling: https://learn.microsoft.com/en-us/azure/security/develop/threat-modeling-tool-threats\r\n- NIST Cybersecurity Framework: https://www.nist.gov/cyberframework\r\n"
  }
]